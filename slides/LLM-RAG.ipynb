{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48332385",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLM RAG\n",
    "\n",
    "<https://arxiv.org/abs/2312.10997>\n",
    "\n",
    "《Retrieval-Augmented Generation for Large Language Models: A Survey》这篇论文介绍了 RAG 技术的发展。\n",
    "\n",
    "本文尝试对其关键要点进行一些简单的介绍。\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/qrcode_s3.laisky.com.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b88fd7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Need RAG, LLM 面临的问题\n",
    "\n",
    "在介绍 RAG 是什么以前，先介绍了 LLM 目前所面临的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20424a9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 训练成本\n",
    "\n",
    "神经网络类型的 AI 有一大特点，就是预训练的成本远远高于推理。\n",
    "\n",
    "OpenAI 在 Dev Day 2023 宣称 GPT 的预训练成本在 2～3 百万美元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc862009",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 数据集落后\n",
    "\n",
    "高昂的预训练成本带来一个最直接的问题就是：模型更新缓慢。 GPT-3.5 的数据集时间为 September 2021。\n",
    "\n",
    "而且数据集的更新成本不仅仅是训练，还有数据集的收集和清洗，这都进一步降低了模型更新的频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6481c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### hallucination 幻觉\n",
    "\n",
    "有一种观点认为 LLM 是一种高效的有损信息压缩算法，它的信息解压缩过程依赖于用户 prompt。\n",
    "\n",
    "这使得 LLM 的答复质量非常容易受到用户 prompt 的干扰。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfc951b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-hallucination.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725331aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transparency 透明性\n",
    "\n",
    "LLM 给出的回答完全是黑盒，根本不知道来自哪，自然也就难以查证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020ffaf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ee8353",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is RAG\n",
    "\n",
    "### Parameter\n",
    "\n",
    "我们将 LLM 的信息分为两个渠道：\n",
    "\n",
    "1. Parameteric knowledge：预训练 LLM 时使用的信息\n",
    "2. Non-parametric knowledge：LLM 推理时 context 内的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c9ffa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RAG\n",
    "\n",
    "根据 LLM 对 parameter 的依赖程度，可以再分为三类：\n",
    "\n",
    "1. fully parameterized model：只依赖预训练数据\n",
    "2. RAG：hybrid\n",
    "3. RCG：完全依赖推理时的外部信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8a74eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "所以可以对 RAG 进行定义：Retrieval-Augmented Generation，基于信息抓取的生成。\n",
    "\n",
    "或者更通俗的理解为：在直接将数据交给模型以前，先进行一轮信息检索，完善输入信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e23064",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RAG Vs. Fine-Tunning\n",
    "\n",
    "RAG 和 fine-tuning 都是可以提高 LLM 模型性能的方法，两者的应用场景存在一些差别：\n",
    "\n",
    "1. RAG 适合少量垂直领域的精确信息，不适合开放式的大量数据\n",
    "2. fine-tuning 适合大量数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c795dc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "需要注意的是，fine-tuning 和 pre-traning 的区别在于：\n",
    "\n",
    "fine-tuning 不适合让 LLM 学习新知识，而是适合让 LLM 强化某个已知知识。\n",
    "\n",
    "可以认为 fine-tuning 是复习，RAG 是考试时的小抄。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb798682",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What is fine-tuning\n",
    "\n",
    "其实就是组织大量结构化的问答数据喂给 LLM，增强其对特定语料的回答能力。\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-fine-tunning-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf3e55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "fine-tuning 和 RAG 并不矛盾，两者契合可以发挥出更大的作用\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3949cd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-vs-finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c83a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG's Evolution\n",
    "\n",
    "介绍 RAG 技术和工作流的进化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59a33af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Naive RAG\n",
    "\n",
    "最简单的 RAG，就是顾名思义的执行三个步骤：\n",
    "\n",
    "1. Retrieval: 根据 prompt 抓取外部数据\n",
    "2. Augmented: 使用外部数据增强 prompt \n",
    "3. Generation: 把增强后的 prompt 交给 LLM，生成 predict/answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097f9ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### embeddings\n",
    "\n",
    "Naive 处理资料集的方式也是单一的：\n",
    "\n",
    "1. 首先，全部转换为文字（text）\n",
    "2. 对文字进行切块（chunk）\n",
    "3. 把每一个 chunk 交给 embeddings-model，计算词向量（word vector）\n",
    "4. 将词向量和 chunk 存储向量数据库\n",
    "\n",
    "embeddings 就是基于 LLM 将一个语句转化为一个高维向量，切分资料的 `chunk_size` 是一个关键参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b77c0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-embaeedings-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598dfcb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Retrieval\n",
    "\n",
    "执行类似步骤：\n",
    "\n",
    "1. 将 prompt 交给 embeddings-model，计算词向量\n",
    "2. 在向量数据库中执行 ANN 查询，获取 K 个最近似结果\n",
    "\n",
    "注：Approximate Nearest Neighbours (ANN) 和精确计算距离的 KNN 不同，ANN 适用于高维空间的近似计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84350f53",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Augmented\n",
    "\n",
    "将获取到的资料按照某个模板，和 prompt 进行拼接，得到最终 prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a63fccf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Generation\n",
    "\n",
    "将最终 prompt 交给 LLM，得到回答。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc350e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-naive-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd381825",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "一个简单的结合向量数据库实现 prompt 增强的例子：\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-embeddings-code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f178e86",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 向量化的问题\n",
    "\n",
    "可以看出数据的检索完全依赖于向量化所产生的词向量。\n",
    "\n",
    "> when an embedding model calculates the vector representation of a sentence, it does so based on the similarity of the sentence to the pre-trained data\n",
    "\n",
    "而 embedding model 在计算一个语句的向量时，需要基于 pre-training 的数据来理解输入的语句。如果输入的语句和预训练数据集的差异特别大，会导致结果出现很大偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0fc53",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "基于词向量相似性搜索的方案，precision 和 recall 都很低。就是查询的数据不一定有用，有用的数据不一定被查询到。\n",
    "\n",
    "Ps. precision 度量结果数据中的阳性率。recall 度量所有阳性被找到的概率。都是越高越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f594c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advanced RAG\n",
    "\n",
    "Advanced RAG 在 Naive 的基础上增加了 pre-retrieval 和 post-retrieval 两个方法。\n",
    "\n",
    "工作流变为：\n",
    "\n",
    "1. pre-retrieval process\n",
    "2. embeddings\n",
    "3. post-retrieval process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf252e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-adv-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dccaa3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pre-retrieval Process\n",
    "\n",
    "在 embeddings 以前，对数据进行清洗和规整，可以分为 5 个步骤：\n",
    "\n",
    "1. Enhancing Data Granularity: 对数据内容进行修订和简化，确保数据源的正确性和可读性\n",
    "2. Optimizing Index Structures: 优化数据索引，引入图数据库、多级索引等结构\n",
    "  1. GraphDB: 可以用来建立知识图谱，进行关联查询\n",
    "  2. Hierarchical Indices: 索引和查询是两类不同的任务，可以使用不同的 index\n",
    "3. Adding Medadata Information: 为切块后的数据增加 metadata，标记数据来源\n",
    "4. Alignment Optimization: 可以为每一个 chunk 生成一个假设性提问，然后将这个问题本身也嵌合到 chunk 中\n",
    "  * Hypotherical Question 和用户 query 间具有更强的语意相关性，可以提高检索的关联度。\n",
    "5. Mixed Retrieval: 混合使用多种检索技术，而不仅仅是词向量搜索。\n",
    "  * 也称为 hybrid/fusion/mixed/ensemble retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b2f35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Embedding\n",
    "\n",
    "对 embeddings 过程中所使用的 embedding-model 也进行改进\n",
    "\n",
    "1. Fine-tunning Embedding: 可以将领域知识预先通过 fine-tuning 内嵌到模型中\n",
    "2. Dynamic Embedding: 在 embeddings 时，不要仅针对关键词（static），而是要联合上下文一起（dynamic）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233dc657",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Post-Retrieval Process\n",
    "\n",
    "在完成资料查询，提交给 LLM 前，继续对收集到的资料进行优化\n",
    "\n",
    "1. ReRank: 根据关联度进行打分和重排序\n",
    "2. Prompt Compression: 无关输入对 LLM 的性能有负面影响。压缩不相关信息，强调关键信息，减少总长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0aa06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RAG Pipeline Optimization\n",
    "\n",
    "一些通用的 RAG 检索资料优化办法\n",
    "\n",
    "1. Hybrid Search: 前面提到过的混合检索\n",
    "2. Recursive Retrieval And Query Engine: 多阶段检索，先检索一批小 chunk，再根据小 chunk 去检索大 chunk\n",
    "  1. Sentence Window Retrieval: 检索到小 chunk，然后返回 surrounding context 的大 chunk\n",
    "  2. Parent-Child Chunks Retrieval: 每个小 chunk 都指向一个或多个更详细的大 chunk，如果有多个大 chunk 被引用，则使用这个大 chunk 替换小 chunk。\n",
    "3. StepBack-prompt: 一种 prompt-engineering，可以显著提高推理密集型任务的性能。让 LLM 更关注抽象概念\n",
    "4. Subqueries: 根据语意拆分为多个小查询\n",
    "5. HyDE: 先让 LLM 回答一次，然后根据 LLM 的回答再去搜索相关资料。但如果 LLM 对相关话题不熟悉，反而会加重幻觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8789cce6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sentence Window Retrieval\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-sentence-window-retrieval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44632348",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Parent-Child Chunks Retrieval\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-parent-child-chunks-retrieval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61e9a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modular RAG\n",
    "\n",
    "这是作者提到的 RAG 的最终进化形态。不过其实所使用的技术都是前面 naive 和 advanced 里提到过的。\n",
    "\n",
    "最重要的改变更多是架构设计上的，将单一的命令式流水线（pipeline），变成了响应式的动态调度（adaptive/routing）。\n",
    "\n",
    "所有前面提到过的功能都被封装为了功能模块，根据任务类型进行动态组合和调度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe5f2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-modular-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f7d6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Retriever\n",
    "\n",
    "在 Modular RAG 中，Retriever 负责对外部数据源进行预处理和查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c24f841",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 增强 Embeddings 的语意准确度\n",
    "\n",
    "#### Chunk\n",
    "\n",
    "RAG 的最终生成物不能超出 LLM 的 context window，所以数据切片是必要的，而切片的 chunk size 的选择是优化的第一步。\n",
    "\n",
    "OpenAI 的 text-embedding-ada-002 的最优 chunk size 为 256～512 tokens。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a06aa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Fine-tuning Embedding Models\n",
    "\n",
    "embedding 是 RAG 的核心，为了让 embedding model 能够更好地理解垂直领域信息，可以对 embedding model 进行 fine-tuning。\n",
    "\n",
    "Ps. OpenAI 目前尚不支持该功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bb545",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-embedding-code-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bfa314",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 将用户请求和数据集进行对齐\n",
    "\n",
    "#### Query Rewrite/Transformation\n",
    "\n",
    "最简单直观的方法就是让 LLM 重写用户查询。\n",
    "* 利用 prompt engineering 优化查询\n",
    "* 将复杂问题拆分为多个子查询等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b9c60e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Prompt Engineering\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-prompt-engineering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4180ed6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multi Query Retriever / Sub Question Query Engine\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-multi-query-retriever.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383ade9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Query Embedding Transformation\n",
    "\n",
    "query rewrite 是粗粒度的，query 的 embedding 应该是细粒度的。\n",
    "\n",
    "query 最终也需要被 embedding 然后再去搜索外部资料，处理 query 的 embedding model 也可以被 fine-tuning，以使其可以更好的匹配特定任务，尤其是使其可以更好的关联到结构化的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04973cb0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aligning Retriever's Output and LLM's Preference\n",
    "\n",
    "单纯的计算 retriever 的 hit rate（正确性）是不够的，因为可能查找的资料并不是 LLM 所需要的。\n",
    "相比单纯的信息正确，LLM 更偏好于可读性更好的资料。\n",
    "\n",
    "所以 retriever 还需要对齐（alignment），才能真正提高 RAG 的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed75d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generator\n",
    "\n",
    "Generator 负责将 retriever 抓取的资料转化为更好的格式，喂给 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d005d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Post-Retrieval Processing\n",
    "\n",
    "retriever 抓取的资料可能过长或存在冗余。在 retriever 后，提交给 LLM 前，增加一步做数据清洗的 post-retrieval 步骤。\n",
    "\n",
    "post-retrieval 的核心目标：信息压缩和结果重排（rerank）。\n",
    "\n",
    "信息压缩的必要性：降低噪音，减少长度，增强 LLM 生成效率。（冗余信息会极大的干扰 LLM 的生成质量）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7547c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Rerank\n",
    "\n",
    "RAG 里的 context 不是越多越好，实际上添加的上下文越多，LLM 的性能指标反而可能会降低。\n",
    "\n",
    "* `Catastrophic Forgetting`: LLM 学习新知识后会遗忘旧知识，导致回归性能降级。\n",
    "* 此外 LLM 的 `In Context Learning (ICL)` 是位置相关的，越靠前效果越好。\n",
    "\n",
    "rerank 可以改善 CF，通过将最相关的信息放在最前面，然后限制总的信息量。\n",
    "我理解是，提供的新信息越少，对旧指标的干扰也就越少。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f3b11",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### GPT-4-128k ICL\n",
    "\n",
    "可以看出 LLM 的 ICL 对头尾效果最好\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-gpt-4-128k-icl.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8c58f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Augmentation in RAG\n",
    "\n",
    "RAG 对 LLM 的提升是全面的，可以从三个维度来理解 RAG：\n",
    "\n",
    "1. Stages: RAG 可以应用于 LLM 生命周期的全部三个阶段：pre-training、fine-tuning、inference\n",
    "2. Data: 可以对数据进行增强\n",
    "3. Process: RAG 的具体实操方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e354e14",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RAG 概念概览\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f7682",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stages\n",
    "\n",
    "RAG 并不仅仅是简单地用于增强用户 prompt，而是可以作用于 LLM 全生命周期\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b704d7b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pre-training Stage\n",
    "\n",
    "在 Pre-training 阶段就引入 RAG，对预训练数据集进行增强。\n",
    "\n",
    "优点：基于 RAG 的数据增强，比从头去重新准备预训练数据集要更简单。\n",
    "\n",
    "缺点：和预训练一样的缺点，更新缓慢，更新成本高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7444aab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Fine-tunning State\n",
    "\n",
    "对于垂直领域，针对 LLM 和 retriever 都进行 fine-tuning，可以提高性能。\n",
    "\n",
    "fine-tuning 的缺点：需要结构化的训练数据，需要的计算资源远大于推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44371f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Inference Stage\n",
    "\n",
    "推理阶段 RAG 是目前最流行的方式\n",
    "\n",
    "优点：轻量、便宜，实时\n",
    "\n",
    "缺点：需要占据 LLM 宝贵的 context 空间，需要针对底层 LLM 进行定制化优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceb90f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 对数据源进行增强\n",
    "\n",
    "基于知识图谱的结构化数据，可以在数据检索时提供更为关联的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0acf52c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### LLM Generated Content RAG\n",
    "\n",
    "外部输入的新知识有时候会对 LLM 的性能带来负影响，所以有一条 RAG 的研究道路是探索深入挖掘 LLM 的内生知识。\n",
    "\n",
    "SKR 让 LLM 区分已知和未知信息，只要求 retriever 获取未知信息，使用内置信息回答已知问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdebf7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Augmentation Process\n",
    "\n",
    "本节讨论 RAG 操作流程的优化。\n",
    "\n",
    "单步 RAG 可能会因为信息冗余使得 LLM 抓不住重点（lost in the middle）。\n",
    "此外，也让 LLM 无法进行深入的多步推理。\n",
    "\n",
    "可以设计一个逻辑 loop: retriever - generator。通过多轮循环，以取得更好的效果。\n",
    "（Recursive retrieval and multi-hop retrieval）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0aa2d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Iterative Retrieval\n",
    "\n",
    "利用外部数据增强推理 & 利用推理增强数据抓取，不断循环迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d889958",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Adaptive Retrieval\n",
    "\n",
    "也叫做 query routing。实际上就是利用 tools/agents/function call 的功能，让 LLM 可以自行根据 context 调用 retriever 抓取外部数据源。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c2aec7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### function calling / tools\n",
    "\n",
    "function calling 是一个多轮交互过程，简而言之就是让 LLM 可以主动问问题，然后你将答案融合到原始 query 后再重新交给 LLM。\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-function-calling.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65189e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multi-Document Agents\n",
    "\n",
    "结合 agent 实现了 hybrid retrieval\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-multi-document-agents.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4995e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66be808b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation Methods\n",
    "\n",
    "有两种评估方式，其实和我们惯常的测试方法论也是一样的\n",
    "\n",
    "1. Independent Evaluation: 对各个流程/模块进行分别测试，可以理解为单元测试\n",
    "2. End-to-End Evaluation: 顾名思义，模仿用户行为，直接就最终接口进行测试\n",
    "\n",
    "E2E 测试又可分为 unlabeled 和 labeled。我的理解是，labeled 应该是偏向于人工校验，需要提供标准答案，然后计算 EM 等指标。unlabeled 偏向于自动测试，可以使用一些标准方法计算得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82c30d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Retrieval Metrics\n",
    "\n",
    "* `Hit Rate (HR)`: 检索到相关资料的概率\n",
    "* `Recall`: 所有应该被检索的文档里，被正确检索出来的比例\n",
    "* `Precesion`: 检索出来的文档里，相关文档的比率\n",
    "* `Mean Reciprocal Rank (MRR)`: rerank 的指标，度量 retriever 返回的最优信息是否出现在了最前列\n",
    "* `Mean Average Precision (mAP)`: 也称为 `mAP@K`，度量第 K 个正确答案的位置。可以理解为 MRR 的复数版。\n",
    "* `Normalized Discounted Cumulative Gain (NDCG)`: 度量整体的 rerank 质量\n",
    "* `Exact Match (EM)`: 查询到的资料里，包含正确答案的概率\n",
    "* `F1 Score`: 就是 recall 和 precision 的调和平均数：\n",
    "* `Semantic Answer Similarity (SAS)`: 比较正确答案和 LLM 回复（predict）间的语意相似度。\n",
    "\n",
    "<https://laisky.notion.site/Metrics-for-Information-Retrieval-and-Question-Answering-Pipelines-d5d4e3beb820419ca494596e319befcf?pvs=4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff68c99",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Metrics and Abilities\n",
    "\n",
    "#### Key Metrics\n",
    "\n",
    "1. Faithfulness: retriever 取回的资料不应该违背原始问题\n",
    "2. Answer Relevance: RAG 生成 answer 应该忠于原始 query\n",
    "3. Context Relevance: RAG 生成的 context 和原始 query 一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b017efe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Key abilities\n",
    "\n",
    "能够支撑 RAG 的底层 LLM 所应该具备的四个基础能力：\n",
    "\n",
    "1. Noise Robustness: 能够分辨噪音\n",
    "2. Negative Rejection: 信息不足时应该拒绝回答\n",
    "3. Information Integration: 能够整合杂乱的信息\n",
    "4. Counterfactual Robustness:能够分辨 retrieval 提供的信息中存在的事实错误"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e22196",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "明显在做梦😂，现实是通用大模型的 hallucination 非常显著"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60da50c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Future Prospects\n",
    "\n",
    "RAG 未来的三个发展方向：\n",
    "\n",
    "1. vertical optimization\n",
    "2. horizontal expansion\n",
    "3. ecosystem of RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163921b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vertical Optimization\n",
    "\n",
    "垂直优化，和水平扩容相对，一般指增强自身的能力，而不是依靠外部扩展，此处指增强 LLM 自身的能力。\n",
    "\n",
    "包括更大的 context，更稳定的回答能力。目前的 LLM 极易受到参考信息中的错误和无关信息的干扰。\n",
    "\n",
    "RAG 和 Fine-tuning 更深入地互相协调也是一个重要的优化方向。\n",
    "\n",
    "最后就是 RAG 的工程实践，包括性能提升和安全性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062d7af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Horizontail Expansion\n",
    "\n",
    "多模态！\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-vision-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f492a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Thanks"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
