{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7ae9c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LLM RAG\n",
    "\n",
    "<https://arxiv.org/abs/2312.10997>\n",
    "\n",
    "《Retrieval-Augmented Generation for Large Language Models: A Survey》这篇论文介绍了 RAG 技术的发展。\n",
    "\n",
    "本文尝试对其关键要点进行一些简单的介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ca34b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Need RAG, LLM 面临的问题\n",
    "\n",
    "在介绍 RAG 是什么以前，先介绍了 LLM 目前所面临的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59d912",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 训练成本\n",
    "\n",
    "神经网络类型的 AI 有一大特点，就是预训练的成本远远高于推理。\n",
    "\n",
    "OpenAI 在 Dev Day 2023 宣称 GPT 的预训练成本在 2～3 百万美元。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf39fdc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 数据集落后\n",
    "\n",
    "高昂的预训练成本带来一个最直接的问题就是：模型更新缓慢。 GPT-3.5 的数据集时间为 September 2021。\n",
    "\n",
    "而且数据集的更新成本不仅仅是训练，还有数据集的收集和清洗，这都进一步降低了模型更新的频率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3647e9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### hallucination 幻觉\n",
    "\n",
    "有一种观点认为 LLM 是一种高效的有损信息压缩算法，它的信息解压缩过程依赖于用户 prompt。\n",
    "\n",
    "这使得 LLM 的答复质量非常容易受到用户 prompt 的干扰。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807da523",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-hallucination.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b80b866",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transparency 透明性\n",
    "\n",
    "LLM 给出的回答完全是黑盒，根本不知道来自哪，自然也就难以查证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5691c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is RAG\n",
    "\n",
    "### Parameter\n",
    "\n",
    "我们将 LLM 的信息分为两个渠道：\n",
    "\n",
    "1. Parameteric knowledge：预训练 LLM 时使用的信息\n",
    "2. Non-parametric knowledge：LLM 推理时 context 内的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace8e96",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RAG\n",
    "\n",
    "根据 LLM 对 parameter 的依赖程度，可以再分为三类：\n",
    "\n",
    "1. fully parameterized model：只依赖预训练数据\n",
    "2. RAG：hybrid\n",
    "3. RCG：完全依赖推理时的外部信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60727c62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "所以可以对 RAG 进行定义：Retrieval-Augmented Generation，基于信息抓取的生成。\n",
    "\n",
    "或者更通俗的理解为：在直接将数据交给模型以前，先进行一轮信息检索，完善输入信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae437926",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RAG Vs. Fine-Tunning\n",
    "\n",
    "RAG 和 fine-tuning 都是可以提高 LLM 模型性能的方法，两者的应用场景存在一些差别：\n",
    "\n",
    "1. RAG 适合少量垂直领域的精确信息，不适合开放式的大量数据\n",
    "2. fine-tuning 适合大量数据\n",
    "\n",
    "需要注意的是，fine-tuning 和 pre-traning 的区别在于，fine-tuning 不适合让 LLM 学习新知识，而是适合让 LLM 强化某个已知知识。\n",
    "\n",
    "可以认为 fine-tuning 是复习，RAG 是考试时的小抄。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed1eae2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What is fine-tuning\n",
    "\n",
    "其实就是组织大量结构化的问答数据喂给 LLM，增强其对特定语料的回答能力。\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-fine-tunning-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d68b488",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "fine-tuning 和 RAG 并不矛盾，两者契合可以发挥出更大的作用\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad4b31",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-rag-vs-finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96b152e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RAG's Evolution\n",
    "\n",
    "介绍 RAG 技术和工作流的进化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bed5b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive RAG\n",
    "\n",
    "最简单的 RAG，就是顾名思义的执行三个步骤：\n",
    "\n",
    "1. Retrieval: 根据 prompt 抓取外部数据\n",
    "2. Augmented: 使用外部数据增强 prompt \n",
    "3. Generation: 把增强后的 prompt 交给 LLM，生成 predict/answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bd692",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### embeddings\n",
    "\n",
    "Naive 处理资料集的方式也是单一的：\n",
    "\n",
    "1. 首先，全部转换为文字（text）\n",
    "2. 对文字进行切块（chunk）\n",
    "3. 把每一个 chunk 交给 embeddings-model，计算词向量（word vector）\n",
    "4. 将词向量和 chunk 存储向量数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530e997",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "embeddings 就是基于 LLM 将一个语句转化为一个高维向量，切分资料的 `chunk_size` 是一个关键参数。\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-embaeedings-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de872d13",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Retrieval\n",
    "\n",
    "执行类似步骤：\n",
    "\n",
    "1. 将 prompt 交给 embeddings-model，计算词向量\n",
    "2. 在向量数据库中执行 KNN 查询，获取 K 个最近似结果\n",
    "\n",
    "\n",
    "#### Augmented\n",
    "\n",
    "将获取到的资料按照某个模板，和 prompt 进行拼接，得到最终 prompt\n",
    "\n",
    "#### Generation\n",
    "\n",
    "将最终 prompt 交给 LLM，得到回答。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850ba52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "一个简单的结合向量数据库实现 prompt 增强的例子：\n",
    "\n",
    "![](https://s3.laisky.com/uploads/2023/12/llm-embeddings-code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f2db1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 向量化的问题\n",
    "\n",
    "可以看出数据的检索完全依赖于向量化所产生的词向量。\n",
    "\n",
    "> when an embedding model calculates the vector representation of a sentence, it does so based on the similarity of the sentence to the pre-trained data\n",
    "\n",
    "而 embedding model 在计算一个语句的向量时，需要基于 pre-training 的数据来理解输入的语句。如果输入的语句和预训练数据集的差异特别大，会导致结果出现很大偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cd1a7e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "基于词向量相似性搜索的方案，precision 和 recall 都很低。就是查询的数据不一定有用，有用的数据不一定被查询到。\n",
    "\n",
    "Ps. precision 度量结果数据中的阳性率。recall 度量所有阳性被找到的概率。都是越高越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98740986",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advanced RAG\n",
    "\n",
    "Advanced RAG 在 Naive 的基础上增加了 pre-retrieval 和 post-retrieval 两个方法。\n",
    "\n",
    "工作流变为：\n",
    "\n",
    "1. pre-retrieval process\n",
    "2. embeddings\n",
    "3. post-retrieval process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243748f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pre-retrieval Process\n",
    "\n",
    "在 embeddings 以前，对数据进行清洗和规整，可以分为 5 个步骤：\n",
    "\n",
    "1. Enhancing Data Granularity: 对数据内容进行修订和简化，确保数据源的正确性和可读性\n",
    "2. Optimizing Index Structures: 优化数据索引，引入图数据库等关联结构\n",
    "3. Adding Medadata Information: 为切块后的数据增加 metadata，标记数据来源\n",
    "4. Alignment Optimization: 可以为每一个 chunk 生成一个假设性提问，然后将这个问题本身也嵌合到 chunk 中，这样可以提高检索的关联度。\n",
    "5. Mixed Retrieval: 混合使用多种检索技术，而不仅仅是词向量搜索。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412217f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Embedding\n",
    "\n",
    "对 embeddings 过程中所使用的 embedding-model 也进行改进\n",
    "\n",
    "1. Fine-tunning Embedding: 可以将领域知识预先通过 fine-tuning 内嵌到模型中\n",
    "2. Dynamic Embedding: 在 embeddings 时，不要仅针对关键词（static），而是要联合上下文一起（dynamic）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bca97c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Post-Retrieval Process\n",
    "\n",
    "在完成资料查询，提交给 LLM 前，继续对收集到的资料进行优化\n",
    "\n",
    "1. ReRank: 根据关联度进行打分和重排序\n",
    "2. Prompt Compression: 无关输入对 LLM 的性能有负面影响。压缩不相关信息，强调关键信息，减少总长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f8146",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RAG Pipeline Optimization\n",
    "\n",
    "一些通用的 RAG 检索资料优化办法\n",
    "\n",
    "1. Hybrid Search: 前面提到过的混合检索\n",
    "2. Recursive Retrieval And Query Engine: 多阶段检索，先检索一批小 chunk，再根据小 chunk 去检索大 chunk\n",
    "3. StepBack-prompt: 一种 prompt-engineering，可以显著提高推理密集型任务的性能。让 LLM 更关注抽象概念\n",
    "4. Subqueries: 根据语意拆分为多个小查询\n",
    "5. HyDE: 先让 LLM 回答一次，然后根据 LLM 的回答再去搜索相关资料。但如果 LLM 对相关话题不熟悉，反而会加重幻觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2e00c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modular RAG\n",
    "\n",
    "这是作者提到的 RAG 的最终进化形态。不过其实所使用的技术都是前面 naive 和 advanced 里提到过的。\n",
    "\n",
    "最重要的改变更多是架构设计上的，将单一的命令式流水线（pipeline），变成了相应式的动态调度。\n",
    "\n",
    "所有前面提到过的功能都被封装为了功能模块，根据任务类型进行动态组合和调度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96697255",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-modular-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f6323a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Retriever\n",
    "\n",
    "在 Modular RAG 中，Retriever 负责对外部数据源进行预处理和查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdd1e45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 增强 Embeddings 的语意准确度\n",
    "\n",
    "#### Chunk\n",
    "\n",
    "RAG 的最终生成物不能超出 LLM 的 context window，所以数据切片是必要的，而切片的 chunk size 的选择是优化的第一步。\n",
    "\n",
    "OpenAI 的 text-embedding-ada-002 的最优 chunk size 为 256～512 tokens。\n",
    "\n",
    "\n",
    "#### Fine-tuning Embedding Models\n",
    "\n",
    "embedding 是 RAG 的核心，为了让 embedding model 能够更好地理解垂直领域信息，可以对 embedding model 进行 fine-tuning。\n",
    "\n",
    "Ps. OpenAI 目前尚不支持该功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218a462",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://s3.laisky.com/uploads/2023/12/llm-embedding-code-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5332b60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 将用户请求和数据集进行对齐\n",
    "\n",
    "#### Query Rewrite\n",
    "\n",
    "最简单直观的方法就是让 LLM 重写用户查询，比如将其拆分为多个子查询。\n",
    "\n",
    "#### Query Embedding Transformation\n",
    "\n",
    "query rewrite 是粗粒度的，query 的 embedding 应该是细粒度的。\n",
    "\n",
    "query 最终也需要被 embedding 然后再去搜索外部资料，处理 query 的 embedding model 也可以被 fine-tuning，以使其可以更好的匹配特定任务，尤其是使其可以更好的关联到结构化的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8251016",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aligning Retriever's Output and LLM's Preference\n",
    "\n",
    "单纯的计算 retriever 的 hit rate（正确性）是不够的，因为可能查找的资料并不是 LLM 所需要的。\n",
    "相比单纯的信息正确，LLM 更偏好于可读性更好的资料。\n",
    "\n",
    "所以 retriever 还需要对齐（alignment），才能真正提高 RAG 的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11a43e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Retrieval Metrics\n",
    "\n",
    "* `Hit Rate (HR)`: 检索到相关资料的概率\n",
    "* `Recall`: 所有应该被检索的文档里，被正确检索出来的比例\n",
    "* `Precesion`: 检索出来的文档里，相关文档的比率\n",
    "* `Mean Reciprocal Rank (MRR)`: rerank 的指标，度量 retriever 返回的最优信息是否出现在了最前列\n",
    "* `Mean Average Precision (mAP)`: 也称为 `mAP@K`，度量第 K 个正确答案的位置。可以理解为 MRR 的复数版。\n",
    "* `Normalized Discounted Cumulative Gain (NDCG)`: 度量整体的 rerank 质量\n",
    "* `Exact Match (EM)`: 查询到的资料里，包含正确答案的概率\n",
    "* `F1 Score`: 就是 recall 和 precision 的调和平均数：\n",
    "* `Semantic Answer Similarity (SAS)`: 比较正确答案和 LLM 回复（predict）间的语意相似度。\n",
    "\n",
    "<https://laisky.notion.site/Metrics-for-Information-Retrieval-and-Question-Answering-Pipelines-d5d4e3beb820419ca494596e319befcf?pvs=4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472ed704",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beca748",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09f903c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a522c302",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2998b93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d02490aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a347271",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31412715",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b7a8415",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22b45796",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
