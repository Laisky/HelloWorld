{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Web 爬虫基础概念及常见攻防方法\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 爬虫是什么\n",
    "\n",
    "爬虫是一类程序的总称，这类程序的作用就是漫游于互联网上，抓取一切有用的信息并保存起来。\n",
    "\n",
    "21世纪是“信息为王”的时代，而爬虫，可算是信息时代的基础。\n",
    "\n",
    "我们所访问的互联网服务很大程度上都依赖于爬虫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 先扯扯互联网\n",
    "\n",
    "我们知道互联网可以粗略的分为两个时代，web1.0 和 web2.0 时代。\n",
    "\n",
    "1.0 是网站发布信息，用户前来阅读。\n",
    "\n",
    "2.0 是网站提供平台，用户使用平台分享信息。\n",
    "\n",
    "也许我们还可以划一个 2.5 时代，由大数据引擎收集用户数据，给用户提供更好的定制化服务。\n",
    "\n",
    "而爬虫正是诞生于 1.0，直接推动了 2.5 时代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 举几个例子\n",
    "\n",
    "\n",
    "#### 1.0\n",
    "\n",
    "搜索引擎大概是最早的爬虫。正是由爬虫在广泛的抓取页面信息，才使得搜索引擎有足够的数据提供服务。\n",
    "\n",
    "比如常见的谷歌爬虫（Google Spider）\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/google%20spider.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 2.0\n",
    "\n",
    "然后 web 2.0 时代来了，随着用户产生越拉越多的数据，互联网上的数据经历了指数级的爆炸增长，这也让基于用户行为的大数据分析成为了可能。\n",
    "\n",
    "数据挖掘界有句脍炙人口的话，“再好的算法也不如更多更好的数据”，无论是精准营销还是商业分析，数据都是关键，而如果获取更全面更实时的数据，也成了现代爬虫的前沿话题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "举个数挖届经常举的例子，是关于我们都很熟悉的页面广告。\n",
    "\n",
    "加入网站的用户男女各占一半，那么如果我挂一个剃须刀的广告，就会浪费一半的流量，而我挂一个化妆品的广告，也会浪费一半的流量。\n",
    "\n",
    "大数据能给广告业带来的颠覆就是，一样的成本，却能将受众提升一倍。也就是给女性用户显示化妆品，而给男性用户显示剃须刀。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "要做到这一切，就需要精准的用户行为分析，而这又依赖于来源广泛的用户行为数据。\n",
    "\n",
    "比如我只要看看用户京东的购物记录，或者看看用户的微博，就可以猜到 ta 的性别，但是你又不是微博或京东，你如何取得数据？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 爬虫的技术原理\n",
    "\n",
    "前面讲完情怀，回来讲讲技术了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 基础原理\n",
    "\n",
    "现代应用，无论是客户端，还是浏览器，或是手机应用，都需要与远程主机通信获取或提交数据。\n",
    "\n",
    "所以理论上我们只需要**向远程主机伪造请求**，就可以获取到任何我们想要的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 然而，现实\n",
    "\n",
    "原理虽然简单，但是应用的开发者也不是傻子，他们当然只希望给真实的用户服务，而不希望被别有用心的人爬走数据。\n",
    "\n",
    "所以客户端程序内往往会内置复杂的加密算法，与服务器间使用五花八门的私有协议进行通信，让你无法通过劫持流量来分析数据包，而客户端往往又是编译过（甚至加壳）的二进制，也让你难以破解客户端查看内部机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 浏览器的天然开放性\n",
    "\n",
    "好在客户端已逐渐的淡出市场，而浏览器是天然开放的。\n",
    "\n",
    "和网页相关的任何文件，任何链接，都是开放可见的。\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/baidu.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "所以，Web 爬虫的基石就是：**用户在浏览器上的一切操作，都会转化为浏览器向后端服务器的网络数据，而这一切的过程，我们都看得到，既然看得到，就可以伪造**。\n",
    "\n",
    "伪造的意思就是，程序可以模仿为一个用户，请求网页、登陆、点击、浏览。\n",
    "\n",
    "而爬虫与反爬的核心斗争也在于此：如何判断对方是一个真实的用户？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### robots.txt\n",
    "\n",
    "在假设爬虫都是坏人前，我们说讲一下好的爬虫。\n",
    "\n",
    "比如搜索引擎，我们假设他们都是善意的，所以我们应该有一个渠道，可以告诉他们哪些页面可以抓取，哪些页面不应该抓取\n",
    "\n",
    "这个渠道就是 robots.txt\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/srceen_shot%202016-08-11%20at%2014.54.06.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 爬虫的技术架构\n",
    "\n",
    "爬虫是一个系统工程的总称，一般来讲，具有以下几个基础组件：\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/srceen_shot%202016-08-11%20at%2011.12.51.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "稍微复杂点的，会变成这样：\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/srceen_shot%202016-08-11%20at%2011.20.57.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 爬虫的技术难点\n",
    "\n",
    "上面讲了基础的技术原理和架构，下面就讲讲实战了。\n",
    "\n",
    "不过没时间 step by step，就非常简单的介绍一下了\n",
    "\n",
    "主要介绍爬虫工作中的各个技术要点，以及可能的攻防方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 并发量\n",
    "\n",
    "很多爬虫，会对数据的时效性有要求，而要保证时效性，就需要在较短的时间间隔里爬取全网数据，这就对并发量提高了需求。\n",
    "\n",
    "为了提高单机并发量，我们可以考虑采用多线程或携程，并且对 fetcher 采用多机分布式架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 访问限制\n",
    "\n",
    "既然爬虫需要超出正常范畴的高并发，这就会成为一个特征行为。反爬者就可以通过访问限制来遏制爬虫。\n",
    "\n",
    "比如可以限制单 IP 在某一时间段内的访问限制。\n",
    "\n",
    "而且手段可以很灵活，可以设置多个阈值，对应不同的应对行为，比如验证码、一定时间内禁止访问等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 分布式\n",
    "\n",
    "为了应对服务器对单 IP 的访问限制，所以发展出了分布式的爬虫，运动多台云主机，在服务器允许的频率范围下进行爬取。\n",
    "\n",
    "这种方法说实话没有很好的解决办法，不过至少增加爬取者的成本，降低对方的抓取效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 去重\n",
    "\n",
    "去重是爬虫中的一个技术重点。因为爬取页面时是根据链接来获取更多的页的，而同一个页面可能会被多个页面索引，那么该页就有可能被重复抓取。\n",
    "\n",
    "在访问频率受限的情况下，重复抓取是一件成本很大的行为。\n",
    "\n",
    "解决方法：标注页面特征值，使用 bloomfilter 等算法去重查验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 页面解析\n",
    "\n",
    "HTTP 请求拿到 body 后，需要把你需要的内容解析出来。\n",
    "\n",
    "不过这其实并不难，稍微熟悉正则表达式的都可以做到，何况我们还有 XPATH。\n",
    "\n",
    "如果你使用 Python 的话，还有一个更好的神器 pyquery 可供选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "荣威RX5 ¥186800\n",
      "原装进口速尊 ¥334800\n",
      "森林人 ¥115696\n",
      "上汽大通G10 0%\n",
      "力狮 ¥94361\n",
      "上汽大通G10 0%\n",
      "上汽大通G10 0%\n",
      "傲虎 ¥132738\n",
      "上汽大通G10 0%\n",
      "昕锐 0%\n",
      "君威 ¥79943\n",
      "全新晶锐 0%\n"
     ]
    }
   ],
   "source": [
    "from pyquery import PyQuery as pq\n",
    "\n",
    "doc = pq('http://car.chexiang.com/')\n",
    "# css select 的语法\n",
    "for car in doc('.clearfix > li > div.car-list-box'):\n",
    "    print(car.cssselect('.car-series')[0].text, car.cssselect('span')[-1].text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 数据清洗\n",
    "\n",
    "大量的数据信息抓取下来后，数据清洗也是个体力活。\n",
    "\n",
    "比如同品牌同型号的运动鞋，在淘宝上可能会有截然不同的描述，你怎么把它们规整到同一个类目下面呢？\n",
    "\n",
    "不过这是属于数据挖掘中 ETL（Extract, Transform, Load） 的范畴，今天主要介绍爬虫，就略过了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 动态加载\n",
    "\n",
    "越来越多的页面才用 js 动态的加载内容，为爬虫工作带来了相当的难度。（这也是一种反爬策略）\n",
    "\n",
    "不过现在大部分的爬虫框架都自带了动态加载功能，会模仿浏览器等待加载过程结束后再解析页面内容。\n",
    "\n",
    "缺点是会带来更大的带宽，降低效率。（可以指定加载特定的内容）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 签名算法\n",
    "\n",
    "前面都假设爬取者可以毫无顾虑的从接口获取到数据，是不是太简单了？\n",
    "\n",
    "前端可以给每一个请求加上一个签名字段（signature），通过某种自定义的摘要算法计算请求体而得出，服务端也通过同样的算法进行验证。\n",
    "\n",
    "前端的摘要算法通过代码丑化、拼接等方法进行伪装，并定期更新，让攻击者难以破解。\n",
    "\n",
    "而攻击者也就因为无法伪造出签名，而无法伪造请求，从而几乎杜绝自动化爬虫。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### phantom\n",
    "\n",
    "难道就没办法破签名了？不怕，我们还有大杀器 phantom。\n",
    "\n",
    "phantom.js 可以调用浏览器内核去打开网页，和真实用户的操作几乎完全一样。\n",
    "\n",
    "甚至可以模仿用户的鼠标操作，来破解行为验证。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 验证码\n",
    "\n",
    "这是我们车享在采用的办法，当用户需要提交敏感信息，或者访问行为触发阈值时，要求用户输入验证码。\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/444444.png)\n",
    "\n",
    "看上去蛮糊的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "10 毫秒后：\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/5555555.png)\n",
    "\n",
    "然后甚至都不需要建模，直接调用 tesseract-ocr 识别就能拿到正确的字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "验证码的标准处理步骤有：\n",
    "\n",
    "  - 二值化\n",
    "  - 去噪点\n",
    "  - 抠图\n",
    "  - 翻转\n",
    "  - 标准化\n",
    "  - OCR\n",
    "  \n",
    "只要是字符不重叠、前景和背景颜色不同的非中文验证码，都可以轻松的快速识别。所以你懂，反识别也就是反着来。\n",
    "  \n",
    "当然，只要你的样本程度不够，我就可以针对性的建模来突破（有钱的大佬还可以聘用 click farm）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "顺便介绍个国外的很有创意的验证码\n",
    "\n",
    "你们过去在上谷歌或亚马逊的时候，有时候能看到这样的验证码\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/Unknown-2.jpeg)\n",
    "\n",
    "这是第一代 reCAPTCHA 搞出来的验证码，你只需要简单的配置，就可以在你的服务上接入这套验证码系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 新时代验证码\n",
    "\n",
    "当然，你现在在谷歌上已经很难看到上面的验证码了，现在更常见的是下面这个：\n",
    "\n",
    "![null](http://laisky-blog.qiniudn.com/Unknown-3.jpeg)\n",
    "\n",
    "点一点就完成了验证，是不是很爽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 换个思路\n",
    "\n",
    "浏览器这条路被搞的这么复杂，就没有其他的方法吗？\n",
    "\n",
    "当然是有的：\n",
    "\n",
    "  - cdn\n",
    "  - mobile web\n",
    "  - app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 跑题\n",
    "\n",
    "讲完爬虫，跑题一点点其他的，比如我们上周遭受的攻击，我觉得这可能是一次撞库。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 撞库\n",
    "\n",
    "比如我从网易上偷到了 1000 万用户的账户密码。我知道有很多人喜欢用同样的用户名密码，所以我会用这 1000 万账户名密码去其他网站上尝试登录。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
