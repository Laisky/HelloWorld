{
 "metadata": {
  "name": "",
  "signature": "sha256:b34beb93be9ed03ed9e1582b4d7207bcfd41b41e1c3c377321155888f5f2074d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Everaldo Aguiar & Reid Johnson\n",
      "#\n",
      "# Modified from:\n",
      "# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)\n",
      "#\n",
      "# Functions to compute and extract association rules from a given frequent \n",
      "# itemset generated by the Apriori algorithm.\n",
      "\n",
      "def apriori(dataset, min_support=0.5, verbose=False):\n",
      "    \"\"\"Implements the Apriori algorithm.\n",
      "\n",
      "    The Apriori algorithm will iteratively generate new candidate \n",
      "    k-itemsets using the frequent (k-1)-itemsets found in the previous \n",
      "    iteration.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset : list\n",
      "        The dataset (a list of transactions) from which to generate \n",
      "        candidate itemsets.\n",
      "\n",
      "    min_support : float\n",
      "        The minimum support threshold. Defaults to 0.5.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    F : list\n",
      "        The list of frequent itemsets.\n",
      "\n",
      "    support_data : dict\n",
      "        The support data for all candidate itemsets.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] R. Agrawal, R. Srikant, \"Fast Algorithms for Mining Association \n",
      "           Rules\", 1994.\n",
      "\n",
      "    \"\"\"\n",
      "    C1 = create_candidates(dataset) # generate 1-item sets\n",
      "    D = map(set, dataset)\n",
      "    F1, support_data = support_prune(D, C1, min_support, verbose=False) # prune candidate 1-itemsets\n",
      "    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n",
      "    k = 2 # the itemset cardinality\n",
      "    while (len(F[k - 2]) > 0):\n",
      "        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n",
      "        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n",
      "        support_data.update(supK) # update the support counts to reflect pruning\n",
      "        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n",
      "        k += 1\n",
      "\n",
      "    if verbose:\n",
      "        # Print a list of all the frequent itemsets.\n",
      "        for kset in F:\n",
      "            for item in kset:\n",
      "                print(\"\" \\\n",
      "                    + \"{\" \\\n",
      "                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n",
      "                    + \"}\" \\\n",
      "                    + \":  sup = \" + str(round(support_data[item], 3)))\n",
      "\n",
      "    return F, support_data\n",
      "\n",
      "def create_candidates(dataset, verbose=False):\n",
      "    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset : list\n",
      "        The dataset (a list of transactions) from which to generate candidate \n",
      "        itemsets.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n",
      "    immutable and hashable).\n",
      "    \"\"\"\n",
      "    c1 = set([]) # list of all items in the database of transactions\n",
      "    for transaction in dataset:\n",
      "        c1.update(set(transaction))\n",
      "        \n",
      "    c1 = sorted(c1)\n",
      "\n",
      "    if verbose:\n",
      "        # Print a list of all the candidate items.\n",
      "        print(\"\" \\\n",
      "            + \"{\" \\\n",
      "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
      "            + \"}\")\n",
      "\n",
      "    # Map c1 to a frozenset because it will be the key of a dictionary.\n",
      "    return map(frozenset, c1)\n",
      "\n",
      "def support_prune(dataset, candidates, min_support, verbose=False):\n",
      "    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n",
      "\n",
      "    By the apriori principle, if an itemset is frequent, then all of its \n",
      "    subsets must also be frequent. As a result, we can perform support-based \n",
      "    pruning to systematically control the exponential growth of candidate \n",
      "    itemsets. Thus, itemsets that do not meet the minimum support level are \n",
      "    pruned from the input list of itemsets (dataset).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    dataset : list\n",
      "        The dataset (a list of transactions) from which to generate candidate \n",
      "        itemsets.\n",
      "\n",
      "    candidates : frozenset\n",
      "        The list of candidate itemsets.\n",
      "\n",
      "    min_support : float\n",
      "        The minimum support threshold.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    retlist : list\n",
      "        The list of frequent itemsets.\n",
      "\n",
      "    support_data : dict\n",
      "        The support data for all candidate itemsets.\n",
      "    \"\"\"\n",
      "    sscnt = {} # set for support counts\n",
      "    for tid in dataset:\n",
      "        for can in candidates:\n",
      "            if can.issubset(tid):\n",
      "                sscnt.setdefault(can, 0)\n",
      "                sscnt[can] += 1\n",
      "\n",
      "    num_items = float(len(dataset)) # total number of transactions in the dataset\n",
      "    retlist = [] # array for unpruned itemsets\n",
      "    support_data = {} # set for support data for corresponding itemsets\n",
      "    for key in sscnt:\n",
      "        # Calculate the support of itemset key.\n",
      "        support = sscnt[key] / num_items\n",
      "        if support >= min_support:\n",
      "            retlist.insert(0, key)\n",
      "        support_data[key] = support\n",
      "\n",
      "    # Print a list of the pruned itemsets.\n",
      "    if verbose:\n",
      "        for kset in retlist:\n",
      "            for item in kset:\n",
      "                print(\"{\" + str(item) + \"}\")\n",
      "        print(\"\")\n",
      "        for key in sscnt:\n",
      "            print(\"\" \\\n",
      "                + \"{\" \\\n",
      "                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n",
      "                + \"}\" \\\n",
      "                + \":  sup = \" + str(support_data[key]))\n",
      "\n",
      "    return retlist, support_data\n",
      "\n",
      "def apriori_gen(freq_sets, k):\n",
      "    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
      "\n",
      "    This operation generates new candidate k-itemsets based on the frequent \n",
      "    (k-1)-itemsets found in the previous iteration. The candidate generation \n",
      "    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n",
      "    items are identical.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    freq_sets : list\n",
      "        The list of frequent (k-1)-itemsets.\n",
      "\n",
      "    k : integer\n",
      "        The cardinality of the current itemsets being evaluated.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    retlist : list\n",
      "        The list of merged frequent itemsets.\n",
      "    \"\"\"\n",
      "    retList = [] # list of merged frequent itemsets\n",
      "    lenLk = len(freq_sets) # number of frequent itemsets\n",
      "    for i in range(lenLk):\n",
      "        for j in range(i+1, lenLk):\n",
      "            a=list(freq_sets[i])\n",
      "            b=list(freq_sets[j])\n",
      "            a.sort()\n",
      "            b.sort()\n",
      "            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n",
      "            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n",
      "\n",
      "            if F1 == F2: # if the first k-2 items are identical\n",
      "                # Merge the frequent itemsets.\n",
      "                retList.append(freq_sets[i] | freq_sets[j])\n",
      "\n",
      "    return retList\n",
      "\n",
      "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
      "    \"\"\"Generates a set of candidate rules.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    freq_set : frozenset\n",
      "        The complete list of frequent itemsets.\n",
      "\n",
      "    H : list\n",
      "        A list of frequent itemsets (of a particular length).\n",
      "\n",
      "    support_data : dict\n",
      "        The support data for all candidate itemsets.\n",
      "\n",
      "    rules : list\n",
      "        A potentially incomplete set of candidate rules above the minimum \n",
      "        confidence threshold.\n",
      "\n",
      "    min_confidence : float\n",
      "        The minimum confidence threshold. Defaults to 0.5.\n",
      "    \"\"\"\n",
      "    m = len(H[0])\n",
      "    if m == 1:\n",
      "        Hmp1 = calc_confidence(freq_set, H, support_data, rules, min_confidence, verbose)\n",
      "    if (len(freq_set) > (m+1)):\n",
      "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
      "        Hmp1 = calc_confidence(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
      "        if len(Hmp1) > 1:\n",
      "            # If there are candidate rules above the minimum confidence \n",
      "            # threshold, recurse on the list of these candidate rules.\n",
      "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
      "\n",
      "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
      "    \"\"\"Evaluates the generated rules.\n",
      "\n",
      "    One measurement for quantifying the goodness of association rules is \n",
      "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
      "    the support for P and H divided by the support for P \n",
      "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
      "    (thus P|H means all the items in set P or in set H).\n",
      "\n",
      "    To calculate the confidence, we iterate through the frequent itemsets and \n",
      "    associated support data. For each frequent itemset, we divide the support \n",
      "    of the itemset by the support of the antecedent (left-hand-side of the \n",
      "    rule).\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    freq_set : frozenset\n",
      "        The complete list of frequent itemsets.\n",
      "\n",
      "    H : list\n",
      "        A list of frequent itemsets (of a particular length).\n",
      "\n",
      "    min_support : float\n",
      "        The minimum support threshold.\n",
      "\n",
      "    rules : list\n",
      "        A potentially incomplete set of candidate rules above the minimum \n",
      "        confidence threshold.\n",
      "\n",
      "    min_confidence : float\n",
      "        The minimum confidence threshold. Defaults to 0.5.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    pruned_H : list\n",
      "        The list of candidate rules above the minimum confidence threshold.\n",
      "    \"\"\"\n",
      "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
      "    for conseq in H: # iterate over the frequent itemsets\n",
      "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
      "        if conf >= min_confidence:\n",
      "            rules.append((freq_set - conseq, conseq, conf))\n",
      "            pruned_H.append(conseq)\n",
      "\n",
      "            if verbose:\n",
      "                print(\"\" \\\n",
      "                    + \"{\" \\\n",
      "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
      "                    + \"}\" \\\n",
      "                    + \" ---> \" \\\n",
      "                    + \"{\" \\\n",
      "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
      "                    + \"}\" \\\n",
      "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
      "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
      "\n",
      "    return pruned_H\n",
      "\n",
      "def generate_rules(F, support_data, min_confidence=0.5, verbose=True):\n",
      "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
      "\n",
      "    For each frequent itemset, we calculate the confidence of using a\n",
      "    particular item as the rule consequent (right-hand-side of the rule). By \n",
      "    testing and merging the remaining rules, we recursively create a list of \n",
      "    pruned rules.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    F : list\n",
      "        A list of frequent itemsets.\n",
      "\n",
      "    support_data : dict\n",
      "        The corresponding support data for the frequent itemsets (L).\n",
      "\n",
      "    min_confidence : float\n",
      "        The minimum confidence threshold. Defaults to 0.5.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    rules : list\n",
      "        The list of candidate rules above the minimum confidence threshold.\n",
      "    \"\"\"\n",
      "    rules = []\n",
      "    for i in range(1, len(F)):\n",
      "        for freq_set in F[i]:\n",
      "            H1 = [frozenset([itemset]) for itemset in freq_set]\n",
      "            if (i > 1):\n",
      "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
      "            else:\n",
      "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
      "\n",
      "    return rules"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}