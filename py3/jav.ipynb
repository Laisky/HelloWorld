{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc70584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# fetch by actress list by onejav\n",
    "# ------------------------------------------------\n",
    "# fetch_onejav_by_actress_list()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# fetch by actress list by javland\n",
    "# ------------------------------------------------\n",
    "# fetch_javland_by_actress_list()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# fetch movies list by fetch_javtrailers\n",
    "# ------------------------------------------------\n",
    "JAVTRAILERS_START_PAGE = 1917\n",
    "# fetch_javtrailers()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# generate full text search metadata for each movie\n",
    "# ------------------------------------------------\n",
    "ENABLE_FULLTEXT_LLM = False\n",
    "# generate_fulltext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc97e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import ALL_COMPLETED, ThreadPoolExecutor, wait\n",
    "from datetime import datetime\n",
    "from threading import RLock\n",
    "from hashlib import sha256\n",
    "from io import BytesIO\n",
    "from queue import Queue\n",
    "from textwrap import dedent\n",
    "from typing import Generator, List, NamedTuple, Optional, Union, Dict, Any\n",
    "\n",
    "import bson.json_util\n",
    "import pymongo\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bson import ObjectId\n",
    "from kipp.decorator import timer\n",
    "from kipp.utils import setup_logger\n",
    "from minio import Minio\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "logger = setup_logger(\"jav\")\n",
    "logger.setLevel(logging.INFO)\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "os.environ[\"TZ\"] = \"UTC\"\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://100.122.41.16:17777\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://100.122.41.16:17777\"\n",
    "os.environ[\"http_proxy\"] = \"http://100.122.41.16:17777\"\n",
    "os.environ[\"https_proxy\"] = \"http://100.122.41.16:17777\"\n",
    "os.environ[\"NO_PROXY\"] = (\n",
    "    \"localhost,127.0.0.1,100.64.0.0/10,192.168.0.0/16,10.0.0.0/8,127.0.0.0/8,snake-carp.ts.net\"\n",
    ")\n",
    "\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=100)\n",
    "\n",
    "\n",
    "sys.path.append(r\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# mongodb\n",
    "mongo = MongoClient(\n",
    "    f\"mongodb://{prd.MONGO_ADMIN_USER}:{prd.MONGO_ADMIN_PASSWD}@{prd.MONGO_HOST}:{prd.MONGO_PORT}\",\n",
    ")\n",
    "col_actress = mongo[\"jav\"][\"actress\"]\n",
    "col_movies = mongo[\"jav\"][\"movies\"]\n",
    "col_fulltext = mongo[\"jav\"][\"fulltext\"]\n",
    "\n",
    "\n",
    "# add index to db\n",
    "col_movies.create_index([(\"name\", pymongo.ASCENDING)])\n",
    "col_actress.create_index([(\"name\", pymongo.ASCENDING)])\n",
    "col_fulltext.create_index([(\"word\", pymongo.ASCENDING)], unique=True)\n",
    "\n",
    "# minio\n",
    "s3cli: Minio = Minio(\n",
    "    endpoint=prd.S3_MINIO_ADDR,\n",
    "    access_key=prd.S3_KEY,\n",
    "    secret_key=prd.S3_SECRET,\n",
    "    secure=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e846e720",
   "metadata": {},
   "source": [
    "## Common Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ae8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Common Utils\n",
    "# =========================================\n",
    "\n",
    "\n",
    "class Actress(NamedTuple):\n",
    "    # id is the unique id of the actress\n",
    "    id: ObjectId\n",
    "    # name is the name of the actress\n",
    "    name: str\n",
    "    # other_names is the other names of the actress, also contains the name\n",
    "    other_names: List[str]\n",
    "    # url is the url of the actress\n",
    "    url: str\n",
    "\n",
    "\n",
    "class Movie(NamedTuple):\n",
    "    # actresses is a list of actresses' id\n",
    "    actresses: List[ObjectId]\n",
    "    # name is the code of the movie\n",
    "    name: str\n",
    "    # description is the title of the movie\n",
    "    description: Optional[str]\n",
    "    # img_urls is a list of image urls that describe the movie\n",
    "    img_urls: List[str]\n",
    "    # tags is a list of tags that describe the movie\n",
    "    tags: List[str]\n",
    "    # published_date is the date when the movie is published\n",
    "    published_date: Optional[datetime]\n",
    "    # url is the url of the movie\n",
    "    urls: List[str]\n",
    "\n",
    "\n",
    "_REGEX_SPLIT_ACTRESS_NAME = re.compile(r\"([\\w・]+)\")\n",
    "\n",
    "\n",
    "def split_actress_name(name: str) -> List[str]:\n",
    "    \"\"\"split actress name to list of names\"\"\"\n",
    "    names = [\n",
    "        v.strip().lower()\n",
    "        for v in _REGEX_SPLIT_ACTRESS_NAME.findall(name)\n",
    "        if v and v.strip()\n",
    "    ]\n",
    "\n",
    "    vals = []\n",
    "    for v in names:\n",
    "        vals.extend(v.split(\",\"))\n",
    "\n",
    "    names = [v.strip() for v in vals if v.strip()]\n",
    "    names = list(set(names))\n",
    "    return names\n",
    "\n",
    "\n",
    "def save_actress(name: str, url: str, other_names: List[str]) -> Actress:\n",
    "    \"\"\"save actress to db, return actress_id\"\"\"\n",
    "    logger.info(f\"save actress {name}\")\n",
    "\n",
    "    url = url.strip()\n",
    "\n",
    "    names = split_actress_name(name)\n",
    "    other_names.extend(names)\n",
    "    assert names, f\"can not find name for {name=}, {url=}\"\n",
    "\n",
    "    name = choose_kanji_name(names)\n",
    "    other_names.append(name)\n",
    "    parsed_other_names: List[str] = []\n",
    "    for v in other_names:\n",
    "        parsed_other_names.extend(split_actress_name(v))\n",
    "    parsed_other_names = list(set(parsed_other_names))\n",
    "\n",
    "    col_actress.update_one(\n",
    "        {\"other_names\": name},\n",
    "        {\n",
    "            \"$set\": {\n",
    "                \"name\": name,\n",
    "            },\n",
    "            \"$addToSet\": {\n",
    "                \"other_names\": {\"$each\": parsed_other_names},\n",
    "                \"urls\": {\"$each\": [url]},\n",
    "            },\n",
    "        },\n",
    "        upsert=True,\n",
    "    )\n",
    "\n",
    "    docu = col_actress.find_one({\"other_names\": name})\n",
    "    assert docu, f\"can not find actress {name}\"\n",
    "    return Actress(\n",
    "        name=name,\n",
    "        url=url,\n",
    "        id=docu[\"_id\"],\n",
    "        other_names=docu[\"other_names\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def replace_image_url(movie_name: str, img_url: str) -> str:\n",
    "    \"\"\"download image and upload to s3, return new url\"\"\"\n",
    "    docu = col_movies.find_one({\"name\": movie_name})\n",
    "    if docu and docu[\"img_urls\"]:\n",
    "        return docu[\"img_urls\"][0]\n",
    "\n",
    "    if not img_url:\n",
    "        return \"\"\n",
    "\n",
    "    new_img_url = \"\"\n",
    "    try:\n",
    "        resp = requests.get(img_url)\n",
    "        assert resp.status_code == 200, f\"get image for {movie_name=}, {img_url=}\"\n",
    "\n",
    "        img_content = resp.content\n",
    "        digest = sha256(img_content).hexdigest()\n",
    "        objkey = f\"jav/{digest[:2]}/{digest[2:4]}/{digest}.png\"\n",
    "\n",
    "        # check whether image exists\n",
    "        new_img_url = f\"{prd.S3_SERVER}/public/{objkey}\"\n",
    "        if requests.head(new_img_url).status_code == 200:\n",
    "            return new_img_url\n",
    "\n",
    "        logger.info(f\"upload image to s3: {objkey}\")\n",
    "        s3cli.put_object(\n",
    "            bucket_name=\"public\",\n",
    "            object_name=objkey,\n",
    "            data=BytesIO(img_content),\n",
    "            length=len(img_content),\n",
    "            content_type=\"image/png\",\n",
    "        )\n",
    "    except Exception:\n",
    "        logger.exception(\n",
    "            f\"download and upload image got error for {movie_name=}, {img_url=}\"\n",
    "        )\n",
    "\n",
    "    return new_img_url\n",
    "\n",
    "\n",
    "REGEX_MOVIE_NAME = re.compile(r\"^([A-Z]+)(\\d+)$\")\n",
    "\n",
    "\n",
    "def normalize_movie_name(name: str) -> str:\n",
    "    name = name.upper().strip()\n",
    "    matched = REGEX_MOVIE_NAME.findall(name)\n",
    "    if matched:\n",
    "        prefix, num = matched[0]\n",
    "        if prefix and num:\n",
    "            name = f\"{prefix}-{num}\"\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def save_movie(movie: Movie):\n",
    "    \"\"\"save movie to db by movie name\"\"\"\n",
    "    movie_name = normalize_movie_name(movie.name)\n",
    "    assert movie_name, f\"movie_name is empty {movie.urls=}\"\n",
    "\n",
    "    img_urls = [v.strip() for v in movie.img_urls if v.strip()]\n",
    "    tags = [v.strip() for v in movie.tags if v.strip()]\n",
    "    actresses = [v for v in movie.actresses if v]\n",
    "    movie_urls = [v.strip() for v in movie.urls if v.strip()]\n",
    "    descriptions = [movie.description.strip()] if movie.description else []\n",
    "\n",
    "    docu_to_set: Dict[str, Union[str, datetime]] = {\"name\": movie_name}\n",
    "    if movie.published_date:\n",
    "        docu_to_set[\"published_date\"] = movie.published_date\n",
    "\n",
    "    logger.info(f\"save movie {movie_name=}\")\n",
    "    col_movies.update_one(\n",
    "        {\"name\": movie_name},\n",
    "        {\n",
    "            \"$set\": docu_to_set,\n",
    "            \"$addToSet\": {\n",
    "                \"actresses\": {\"$each\": actresses},\n",
    "                \"tags\": {\"$each\": tags},\n",
    "                \"img_urls\": {\"$each\": img_urls},\n",
    "                \"urls\": {\"$each\": movie_urls},\n",
    "                \"descriptions\": {\"$each\": descriptions},\n",
    "            },\n",
    "        },\n",
    "        upsert=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# choose_kanji_name([\"abc\", \"ブレイク・ブロッサム\", \"田中瞳\", \"ブレイク\"])\n",
    "def choose_kanji_name(names: List[str]) -> str:\n",
    "    \"\"\"Choose the kanji name from a list of names.\"\"\"\n",
    "    assert names, \"names is empty\"\n",
    "\n",
    "    kanji_pattern = re.compile(r\"[\\u3040-\\u30efa-zA-Z・]\")\n",
    "    hiragana_pattern = re.compile(r\"[\\u3040-\\u309f]\")\n",
    "    english_pattern = re.compile(r\"[a-zA-Z]\")\n",
    "\n",
    "    for name in names:\n",
    "        if not kanji_pattern.search(name):\n",
    "            # First, choose the name which contains Kanji.\n",
    "            # not english nor hiragana\n",
    "            return name\n",
    "\n",
    "    for name in names:\n",
    "        if hiragana_pattern.search(name):\n",
    "            # Second, choose the name which contains Hiragana\n",
    "            return name\n",
    "\n",
    "    for name in names:\n",
    "        if not english_pattern.search(name):\n",
    "            # Last, choose the name without English alphabets\n",
    "            return name\n",
    "\n",
    "    # If none of the above conditions are met, return the first name\n",
    "    return names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e0ee2",
   "metadata": {},
   "source": [
    "## `https://onejav.com/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afa565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# https://onejav.com/\n",
    "# =========================================\n",
    "\n",
    "actress_index_url = \"https://onejav.com/actress/\"\n",
    "\n",
    "REG_ONEJAV_ACTRESS_NAME = re.compile(r\"([\\w ]+)(?:<[^>]+>(\\w+).*)?\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def _gen_onejav_actress(q: Queue[Optional[Actress]]):\n",
    "    n_page = 1\n",
    "    n_retry = 0\n",
    "    while True:\n",
    "        url = f\"https://onejav.com/actress/?page={n_page}\"\n",
    "        logger.info(f\"gen_onejav_actress get actress for {url=}\")\n",
    "        resp = requests.get(url)\n",
    "        if resp.status_code == 500:\n",
    "            if n_retry < 3:\n",
    "                time.sleep(3)\n",
    "                logger.warning(f\"gen_onejav_actress retry {url=}, {n_retry=}\")\n",
    "                n_retry += 1\n",
    "                continue\n",
    "            else:\n",
    "                n_retry = 0\n",
    "                n_page += 1\n",
    "                continue\n",
    "        elif resp.status_code == 404:\n",
    "            logger.info(f\"gen_onejav_actress exit since for {url=}\")\n",
    "            return\n",
    "        elif resp.status_code != 200:\n",
    "            logger.info(\n",
    "                f\"gen_onejav_actress exit for {url=} [{resp.status_code}]{resp.text}\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "        for card in soup.select(\".container .card\"):\n",
    "            name, url = \"\", \"\"\n",
    "\n",
    "            ele = card.select_one(\".card-header .card-header-title\")\n",
    "            assert ele, f\"cannot find actress name for {url=}\"\n",
    "            name = ele.decode_contents().strip()\n",
    "\n",
    "            ele = card.select_one(\"a\")\n",
    "            assert ele, f\"cannot find actress url for {url=}\"\n",
    "            url = f\"https://onejav.com{ele.attrs['href']}\"\n",
    "\n",
    "            # parse actress name\n",
    "            # name = 'Jun Suehiro <small class=\"text-muted ml-1\">末広純</small>'\n",
    "            [(alt_name, name)] = REG_ONEJAV_ACTRESS_NAME.findall(name)\n",
    "            name = name.strip()\n",
    "            alt_name = alt_name.strip()\n",
    "            name = name or alt_name\n",
    "\n",
    "            logger.debug(f\"yield actress {name}\")\n",
    "            actress = save_actress(name, url, list(set([name, alt_name])))\n",
    "            q.put(actress)\n",
    "\n",
    "        n_page += 1\n",
    "\n",
    "\n",
    "\n",
    "@timer\n",
    "def _gen_onejav_movies_by_actress(upstream_q: Queue[Optional[Actress]]):\n",
    "    while True:\n",
    "        actress = upstream_q.get()\n",
    "        if actress is None:\n",
    "            upstream_q.put(None)\n",
    "            logger.info(f\"gen_onejav_movies_by_actress exit\")\n",
    "            return\n",
    "\n",
    "        n_page = 1\n",
    "        n_retry = 0\n",
    "        while True:\n",
    "            url = f\"{actress.url}?page={n_page}\"\n",
    "            resp = requests.get(url)\n",
    "            if resp.status_code == 404:\n",
    "                logger.info(f\"gen_movies got 404 for {url=}\")\n",
    "                break\n",
    "            elif resp.status_code != 200:\n",
    "                if n_retry < 3:\n",
    "                    time.sleep(3)\n",
    "                    logger.warning(f\"gen_movies retry {url=}, {n_retry=}\")\n",
    "                    n_retry += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    n_retry = 0\n",
    "                    n_page += 1\n",
    "                    logger.warning(f\"gen_movies skip for {url=}, {n_retry=}\")\n",
    "                    continue\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            for card in soup.select(\".container .card\"):\n",
    "                try:\n",
    "                    ele = card.select_one(\".image\")\n",
    "                    assert ele, f\"can not find image {url=}\"\n",
    "                    img_url = ele.attrs[\"src\"]\n",
    "\n",
    "                    ele = card.select_one(\".title a\")\n",
    "                    assert ele, f\"can not find title {url=}\"\n",
    "                    name = normalize_movie_name(ele.decode_contents())\n",
    "\n",
    "                    description = None\n",
    "                    if card.select_one(\".has-text-grey-dark\"):\n",
    "                        ele = card.select_one(\".has-text-grey-dark\")\n",
    "                        assert ele, f\"can not find description {url=}\"\n",
    "                        description = ele.decode_contents().strip()\n",
    "                    tags = [\n",
    "                        ele.decode_contents().strip()\n",
    "                        for ele in card.select(\".tags .tag\")\n",
    "                    ]\n",
    "\n",
    "                    date: Optional[datetime] = None\n",
    "                    ele = card.select_one(\".card-content .title .subtitle a\")\n",
    "                    if ele and ele.attrs.get(\"href\"):\n",
    "                        # /2024/07/14\n",
    "                        date = datetime.strptime(ele.attrs[\"href\"], \"/%Y/%m/%d\")\n",
    "\n",
    "                    img_url = replace_image_url(name, img_url)\n",
    "                    movie = Movie(\n",
    "                        actresses=[actress.id],\n",
    "                        name=name,\n",
    "                        description=description,\n",
    "                        img_urls=[img_url] if img_url else [],\n",
    "                        tags=tags,\n",
    "                        published_date=date,\n",
    "                        urls=[],\n",
    "                    )\n",
    "                    save_movie(movie)\n",
    "                except Exception:\n",
    "                    logger.exception(f\"parse movie error {url=}\")\n",
    "                    continue\n",
    "\n",
    "            n_page += 1\n",
    "\n",
    "\n",
    "@timer\n",
    "def fetch_onejav_by_actress_list():\n",
    "    \"\"\"fetch actresses and movies by actress list and save to db\"\"\"\n",
    "    actress_queue = Queue(maxsize=10)\n",
    "\n",
    "    fs_load_actress = []\n",
    "    fs_load_movies = []\n",
    "\n",
    "    fs_load_actress.append(executor.submit(_gen_onejav_actress, actress_queue))\n",
    "    for _ in range(20):\n",
    "        fs_load_movies.append(\n",
    "            executor.submit(_gen_onejav_movies_by_actress, actress_queue)\n",
    "        )\n",
    "\n",
    "    wait(fs_load_actress, return_when=ALL_COMPLETED)\n",
    "    actress_queue.put(None)\n",
    "    wait(fs_load_movies, return_when=ALL_COMPLETED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab872f",
   "metadata": {},
   "source": [
    "## `https://jav.land/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f7610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# https://jav.land/\n",
    "# =========================================\n",
    "\n",
    "actress_index_url = \"https://jav.land/ja/star_list.php?prefix={alphabet}&page={page}\"\n",
    "\n",
    "REGEX_JAVLAND_ACTRESS_NAME = re.compile(r\"(\\w+)(?:（(\\w+)(?:、(\\w+){0,}）))?\")\n",
    "\n",
    "\n",
    "class JavlandMovieInfo(NamedTuple):\n",
    "    actress: Actress\n",
    "    url: str\n",
    "    description: Optional[str]\n",
    "\n",
    "\n",
    "@timer\n",
    "def gen_javland_actress(q: Queue[Optional[Actress]]):\n",
    "    n_page = 1\n",
    "    alphabet = \"A\"\n",
    "    n_retry = 0\n",
    "    while alphabet <= \"Z\":\n",
    "        logger.info(f\"gen_javland_actress for {alphabet=}\")\n",
    "        while True:\n",
    "            url = actress_index_url.format(alphabet=alphabet, page=n_page)\n",
    "            try:\n",
    "                logger.info(f\"request actress {url=}\")\n",
    "                resp = requests.get(url, timeout=10)\n",
    "                if resp.status_code % 100 == 5:\n",
    "                    if n_retry < 3:\n",
    "                        time.sleep(3)\n",
    "                        logger.warning(f\"gen_javland_actress retry {url=}, {n_retry=}\")\n",
    "                        n_retry += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        n_retry = 0\n",
    "                        n_page += 1\n",
    "                        continue\n",
    "                elif resp.status_code == 404:\n",
    "                    logger.info(f\"gen_javland_actress exit since for {url=}\")\n",
    "                    break\n",
    "                elif resp.status_code != 200:\n",
    "                    logger.info(\n",
    "                        f\"gen_javland_actress exit for {url=} [{resp.status_code}]{resp.text}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                cards = soup.select(\".container-fluid .genre_list a.text-center\")\n",
    "                if not cards:\n",
    "                    break\n",
    "\n",
    "                for card in cards:\n",
    "                    raw_name = card.decode_contents().strip()\n",
    "\n",
    "                    # parse actress name\n",
    "                    # name = \"馬場ふみか\" / ayami（赤西涼、まひる）\n",
    "                    matched = REGEX_JAVLAND_ACTRESS_NAME.findall(raw_name)\n",
    "                    assert matched, f\"can not parse actress name {raw_name=}\"\n",
    "                    assert matched[0], f\"can not parse actress name {raw_name=}\"\n",
    "                    other_names = list(\n",
    "                        set([v.strip() for v in matched[0] if v.strip()])\n",
    "                    )\n",
    "                    name = choose_kanji_name(other_names)\n",
    "                    assert name, f\"can not parse actress name {raw_name=}\"\n",
    "\n",
    "                    actress_url = f\"https://jav.land{card.attrs['href']}\"\n",
    "\n",
    "                    logger.debug(f\"yield actress {name}\")\n",
    "                    actress = save_actress(name, actress_url, other_names)\n",
    "                    q.put(actress)\n",
    "            except Exception:\n",
    "                time.sleep(1)\n",
    "                logger.exception(f\"gen_javland_actress error {url=}\")\n",
    "                continue\n",
    "\n",
    "            n_page += 1\n",
    "\n",
    "        n_page = 1\n",
    "        alphabet = chr(ord(alphabet) + 1)\n",
    "\n",
    "\n",
    "@timer\n",
    "def gen_javland_movies_by_actress(\n",
    "    upstream_q: Queue[Optional[Actress]],\n",
    "    downstream_q: Queue[Optional[JavlandMovieInfo]],\n",
    "):\n",
    "    while True:\n",
    "        actress = upstream_q.get()\n",
    "        if actress is None:\n",
    "            upstream_q.put(None)\n",
    "            logger.info(f\"gen_javland_movies_by_actress exit\")\n",
    "            return\n",
    "\n",
    "        n_page = 1\n",
    "        n_retry = 0\n",
    "        while True:\n",
    "            url = f\"{actress.url}?page={n_page}\"\n",
    "            try:\n",
    "                logger.info(f\"request movies for {url=}\")\n",
    "                resp = requests.get(url, timeout=10)\n",
    "                if resp.status_code % 100 == 5:\n",
    "                    if n_retry < 3:\n",
    "                        time.sleep(3)\n",
    "                        logger.warning(f\"gen_movies retry {url=}, {n_retry=}\")\n",
    "                        n_retry += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        n_retry = 0\n",
    "                        n_page += 1\n",
    "                        continue\n",
    "                elif resp.status_code == 404:\n",
    "                    logger.info(f\"gen_movies exit since for {url=}\")\n",
    "                    break\n",
    "                elif resp.status_code != 200:\n",
    "                    logger.warning(\n",
    "                        f\"gen_movies skip since for {url=} [{resp.status_code}]{resp.text}\"\n",
    "                    )\n",
    "                    n_page += 1\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                cards = soup.select(\".container-fluid .videothumblist .video.panel\")\n",
    "                if not cards:\n",
    "                    logger.info(f\"can not find cards for {url=}\")\n",
    "                    break\n",
    "\n",
    "                for card in cards:\n",
    "                    ele = card.select_one(\".panel-footer a\")\n",
    "                    assert ele, f\"can not find url {url=}\"\n",
    "                    url = f\"https://jav.land{ele.attrs['href']}\"\n",
    "                    description = ele.decode_contents().strip()\n",
    "\n",
    "                    downstream_q.put(\n",
    "                        JavlandMovieInfo(\n",
    "                            actress=actress, url=url, description=description\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                n_page += 1\n",
    "            except:\n",
    "                time.sleep(1)\n",
    "                logger.exception(f\"gen_javland_movies_by_actress got error for {url=}\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def fetch_javland_movie(q: Queue[Optional[JavlandMovieInfo]]):\n",
    "    while True:\n",
    "        movie_info = q.get()\n",
    "        if movie_info is None:\n",
    "            q.put(None)\n",
    "            logger.info(f\"fetch_javland_movie exit\")\n",
    "            return\n",
    "\n",
    "        nretry = 0\n",
    "        while nretry < 3:\n",
    "            try:\n",
    "                logger.debug(f\"fetch movie for {movie_info.url=}\")\n",
    "                resp = requests.get(\n",
    "                    movie_info.url,\n",
    "                    timeout=10,\n",
    "                    headers={\n",
    "                        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\",\n",
    "                        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "                    },\n",
    "                )\n",
    "                if resp.status_code != 200:\n",
    "                    logger.warning(\n",
    "                        f\"fetch_javland_movie error, {movie_info.url=} [{resp.status_code}]{resp.text}\"\n",
    "                    )\n",
    "                    time.sleep(3)\n",
    "                    nretry += 1\n",
    "                    continue\n",
    "\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "                name: str = \"\"\n",
    "                tags: List[str] = []\n",
    "                date: Optional[datetime] = None\n",
    "                rows = soup.select(\".k-right .videotextlist tr\")\n",
    "                for row in rows:\n",
    "                    ele = row.select_one(\"td:nth-child(1)\")\n",
    "                    key: str = \"\"\n",
    "                    value: str = \"\"\n",
    "\n",
    "                    if ele:\n",
    "                        key = ele.decode_contents().strip()\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    ele = row.select_one(\"td:nth-child(2)\")\n",
    "                    if ele:\n",
    "                        value = ele.decode_contents().strip()\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    if \"DVD ID\" in key:\n",
    "                        name = normalize_movie_name(value)\n",
    "                    elif \"ジャンル\" in key:\n",
    "                        vals = row.select(\"td:nth-child(2) .genre a\")\n",
    "                        tags = list(\n",
    "                            set([val.decode_contents().strip() for val in vals])\n",
    "                        )\n",
    "                    elif \"発売日\" in key:\n",
    "                        date = datetime.strptime(value, \"%Y-%m-%d\")\n",
    "\n",
    "                ele = soup.select_one(\".k-right img.img-responsive\")\n",
    "                assert ele, f\"can not find image {movie_info.url=}\"\n",
    "                img_url = ele.attrs[\"src\"]\n",
    "                img_url = replace_image_url(name, img_url)\n",
    "\n",
    "                movie = Movie(\n",
    "                    actresses=[movie_info.actress.id],\n",
    "                    name=name,\n",
    "                    description=movie_info.description,\n",
    "                    img_urls=[img_url] if img_url else [],\n",
    "                    tags=tags,\n",
    "                    published_date=date,\n",
    "                    urls=[movie_info.url],\n",
    "                )\n",
    "                save_movie(movie)\n",
    "                break\n",
    "            except Exception:\n",
    "                nretry += 1\n",
    "                time.sleep(3)\n",
    "                logger.exception(f\"fetch_javland_movie error {movie_info.url=}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "@timer\n",
    "def fetch_javland_by_actress_list():\n",
    "    \"\"\"fetch actresses and movies by actress list and save to db\"\"\"\n",
    "    actress_queue = Queue(maxsize=10)\n",
    "    movie_queue = Queue(maxsize=10)\n",
    "\n",
    "    fs_load_actress = []\n",
    "    fs_load_movies = []\n",
    "    fs_save_movies = []\n",
    "\n",
    "    fs_load_actress.append(executor.submit(gen_javland_actress, actress_queue))\n",
    "    for _ in range(10):\n",
    "        fs_load_movies.append(\n",
    "            executor.submit(gen_javland_movies_by_actress, actress_queue, movie_queue)\n",
    "        )\n",
    "        fs_save_movies.append(executor.submit(fetch_javland_movie, movie_queue))\n",
    "\n",
    "    wait(fs_load_actress, return_when=ALL_COMPLETED)\n",
    "    actress_queue.put(None)\n",
    "    wait(fs_load_movies, return_when=ALL_COMPLETED)\n",
    "    movie_queue.put(None)\n",
    "    wait(fs_save_movies, return_when=ALL_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b05bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = dedent(\n",
    "#     \"\"\"\n",
    "#     You are a tokenization engine. Follow these instructions sequentially, providing the output of each step directly as requested:\n",
    "\n",
    "#     1. **Keyword Extraction:** Identify and extract the most important keywords from the user's input, based on your understanding. Output these keywords separated by commas.\n",
    "\n",
    "#     2. **Expand JSON Array with Translations:** Take the provided JSON array and create an expanded version. The original array should remain intact. For each non-English word in the array, translate it into English and append the translation to the end of the array.\n",
    "\n",
    "#     3. **Output Format:** Present the final expanded JSON array strictly in the format below, without including any additional text, explanations, or characters:\n",
    "\n",
    "#         `result: [\"word-1\", \"word-2\", ...]`\n",
    "#     \"\"\"\n",
    "# )\n",
    "# user_prompt = \"焦らし寸止め絶頂セックス あやみ史上1番エロいです！あやみはまだまだ進化しています！ ACT.03 あやみ旬果\"\n",
    "\n",
    "# segment_word(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99689d34",
   "metadata": {},
   "source": [
    "## `https://javtrailers.com/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ecd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# https://javtrailers.com/search/ipzz-056\n",
    "# =========================================\n",
    "\n",
    "\n",
    "@timer\n",
    "def _list_javtrailers_movies(q: Queue[Optional[str]]):\n",
    "    \"\"\"scan movies by page and put url to queue\"\"\"\n",
    "    n_page = JAVTRAILERS_START_PAGE\n",
    "    n_retry = 0\n",
    "    while True:\n",
    "        url = f\"https://javtrailers.com/ja/videos?page={n_page}\"\n",
    "        try:\n",
    "            logger.info(f\"list_movies get movies for {url=}\")\n",
    "            resp = requests.get(\n",
    "                url,\n",
    "                timeout=10,\n",
    "                headers={\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "                },\n",
    "            )\n",
    "            assert (\n",
    "                resp.status_code == 200\n",
    "            ), f\"get movies got [{resp.status_code}]{resp.text}\"\n",
    "            if \"No videos available, check out these popular videos\" in resp.text:\n",
    "                logger.info(f\"list_movies exit since for find no videos for {url=}\")\n",
    "                return\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            cards = soup.select(\"#home .videos-list .card-container a\")\n",
    "            for card in cards:\n",
    "                url = f\"https://javtrailers.com{card.attrs['href']}\"\n",
    "                logger.debug(f\"yield movie {url=}\")\n",
    "                q.put(url)\n",
    "\n",
    "            n_page += 1\n",
    "        except Exception:\n",
    "            time.sleep(3)\n",
    "            logger.warning(f\"list_movies retry {url=}, {n_retry=}\")\n",
    "            n_retry += 1\n",
    "            if n_retry > 3:\n",
    "                n_retry = 0\n",
    "                n_page += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "REGEX_JAVTRAILER_ROW_VAL = re.compile(r\"</span>([^<]+)</p>\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def _fetch_javtrailers_movie_detail(\n",
    "    ups_q: Queue[Optional[str]], downs_q: Queue[Optional[Movie]]\n",
    "):\n",
    "    while True:\n",
    "        url = ups_q.get()\n",
    "        if url is None:\n",
    "            ups_q.put(None)\n",
    "            logger.info(f\"fetch_movie_detail exit\")\n",
    "            return\n",
    "\n",
    "        n_retry = 0\n",
    "        while n_retry < 3:\n",
    "            try:\n",
    "                logger.info(f\"fetch movie detail for {url=}\")\n",
    "                resp = requests.get(\n",
    "                    url,\n",
    "                    timeout=10,\n",
    "                    headers={\n",
    "                        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\",\n",
    "                        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "                    },\n",
    "                )\n",
    "                assert (\n",
    "                    resp.status_code == 200\n",
    "                ), f\"fetch movie detail got [{resp.status_code}]{resp.text}\"\n",
    "\n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "                ele = soup.select_one(\"#description .lead\")\n",
    "                description = ele.decode_contents().strip() if ele else \"\"\n",
    "\n",
    "                name: str = \"\"\n",
    "                published_date: Optional[datetime] = None\n",
    "                tags: List[str] = []\n",
    "                actresses: List[ObjectId] = []\n",
    "\n",
    "                rows = soup.select(\"#description #info-row p\")\n",
    "                # [<p class=\"mb-1\"><span class=\"font-weight-bold mr-3\">DVD ID:</span> EBWH-090</p>,\n",
    "                # <p class=\"mb-1\"><span class=\"font-weight-bold mr-3\">Content ID:</span> ebwh00090</p>,\n",
    "                # <p class=\"mb-1\"><span class=\"font-weight-bold mr-3\">Release Date:</span> 12 Apr 2024</p>,\n",
    "                # <p class=\"mb-1\"><span class=\"font-weight-bold mr-3\">Duration:</span> 120 mins</p>,\n",
    "                # <p class=\"mb-1\"><span class=\"font-weight-bold mr-3\">Director:</span> Inavar イナバール</p>,\n",
    "                for row in rows:\n",
    "                    ele = row.select_one(\"span\")\n",
    "                    key = ele.decode_contents().strip() if ele else None\n",
    "                    if not key:\n",
    "                        continue\n",
    "\n",
    "                    val: str = \"\"\n",
    "                    matched = REGEX_JAVTRAILER_ROW_VAL.findall(row.decode().strip())\n",
    "                    if matched:\n",
    "                        val = matched[0].strip()\n",
    "                    else:\n",
    "                        logger.debug(f\"can not find value for {key=}, {row.decode()=}\")\n",
    "\n",
    "                    if val and \"DVD ID\" in key:\n",
    "                        name = normalize_movie_name(val)\n",
    "                    elif val and \"Release Date\" in key or \"商品発売日\" in key:\n",
    "                        # 12 Apr 2024\n",
    "                        published_date = datetime.strptime(val, \"%d %b %Y\")\n",
    "                    elif \"Categories\" in key or \"ジャンル\" in key:\n",
    "                        tags = [\n",
    "                            ele.decode_contents().strip().lower()\n",
    "                            for ele in row.select(\"a\")\n",
    "                        ]\n",
    "                    elif \"Cast(s)\" in key or \"出演者\" in key:\n",
    "                        fs = []\n",
    "                        for actress_ele in row.select(\"a\"):\n",
    "                            actress_name = actress_ele.decode_contents().strip()\n",
    "                            # actress = save_actress(\n",
    "                            #     name=actress_name,\n",
    "                            #     url=f\"https://javtrailers.com{actress_ele.attrs['href']}\",\n",
    "                            #     other_names=[actress_name],\n",
    "                            # )\n",
    "                            # actresses.append(actress.id)\n",
    "                            fs.append(\n",
    "                                executor.submit(\n",
    "                                    save_actress,\n",
    "                                    actress_name,\n",
    "                                    f\"https://javtrailers.com{actress_ele.attrs['href']}\",\n",
    "                                    [actress_name],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                        wait(fs, return_when=ALL_COMPLETED)\n",
    "                        for f in fs:\n",
    "                            actress = f.result()\n",
    "                            actresses.append(actress.id)\n",
    "\n",
    "                assert name, f\"can not find movie name for {url=}\"\n",
    "                if not actresses:\n",
    "                    logger.warning(f\"can not find actress for {name=}, {url=}\")\n",
    "\n",
    "                ele = soup.select(\"#description img\")\n",
    "                img_url = ele[0].attrs[\"src\"].replace(\"ps.\", \"pl.\") if ele else \"\"\n",
    "                img_url = replace_image_url(name, img_url)\n",
    "\n",
    "                movie = Movie(\n",
    "                    actresses=actresses,\n",
    "                    name=name,\n",
    "                    description=description,\n",
    "                    img_urls=[img_url] if img_url else [],\n",
    "                    tags=tags,\n",
    "                    published_date=published_date,\n",
    "                    urls=[url],\n",
    "                )\n",
    "                downs_q.put(movie)\n",
    "                break\n",
    "            except Exception:\n",
    "                time.sleep(3)\n",
    "                logger.exception(f\"fetch movie detail got error for {url=}\")\n",
    "                n_retry += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "@timer\n",
    "def _save_javtrailers_movie(q: Queue[Optional[Movie]]):\n",
    "    while True:\n",
    "        movie = q.get()\n",
    "        if movie is None:\n",
    "            q.put(None)\n",
    "            logger.info(f\"save_movie exit\")\n",
    "            return\n",
    "\n",
    "        save_movie(movie)\n",
    "\n",
    "\n",
    "@timer\n",
    "def fetch_javtrailers():\n",
    "    \"\"\"fetch movies from javtrailers and save to db\"\"\"\n",
    "    movie_list_queue: Queue[Optional[str]] = Queue(maxsize=50)\n",
    "    movie_details_queue: Queue[Optional[Movie]] = Queue(maxsize=50)\n",
    "\n",
    "    fs_list_movies = []\n",
    "    fs_fetch_movies = []\n",
    "    fs_save_movies = []\n",
    "\n",
    "    fs_list_movies.append(executor.submit(_list_javtrailers_movies, movie_list_queue))\n",
    "    for _ in range(10):\n",
    "        fs_fetch_movies.append(\n",
    "            executor.submit(\n",
    "                _fetch_javtrailers_movie_detail, movie_list_queue, movie_details_queue\n",
    "            )\n",
    "        )\n",
    "        fs_save_movies.append(\n",
    "            executor.submit(_save_javtrailers_movie, movie_details_queue)\n",
    "        )\n",
    "\n",
    "    wait(fs_list_movies, return_when=ALL_COMPLETED)\n",
    "    movie_list_queue.put(None)\n",
    "    wait(fs_fetch_movies, return_when=ALL_COMPLETED)\n",
    "    movie_details_queue.put(None)\n",
    "    wait(fs_save_movies, return_when=ALL_COMPLETED)\n",
    "\n",
    "\n",
    "# fetch_javtrailers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d511e3ae",
   "metadata": {},
   "source": [
    "## Fulltext Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# generate full text search metadata for each movie\n",
    "# =========================================\n",
    "\n",
    "def llm_oneshot(model: str, system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"get response from LLM\"\"\"\n",
    "    url = f\"{prd.OPENAI_API}/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {prd.OPENAI_TOKEN}\",\n",
    "    }\n",
    "    body = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 3000,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt,\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    resp = requests.post(url, headers=headers, json=body)\n",
    "    assert resp.status_code == 200, f\"[{resp.status_code}]{resp.text}\"\n",
    "\n",
    "    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "GEMMA_API = \"http://100.92.237.35:7861/chat/completions\"\n",
    "\n",
    "\n",
    "def llm_oneshot_gemma(system_prompt: str, user_prompt: str) -> str:\n",
    "    body = {\n",
    "        \"model\": \"gemma-2b-it\",\n",
    "        \"max_tokens\": 500,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            # gemma does not support `system_prompt`\n",
    "            {\"role\": \"user\", \"content\": f\"{system_prompt}\\n>>\\n{user_prompt}\"},\n",
    "        ],\n",
    "    }\n",
    "    resp = requests.post(GEMMA_API, json=body)\n",
    "    assert resp.status_code == 200, f\"[{resp.status_code}]{resp.text}\"\n",
    "    return resp.json()[\"completion\"]\n",
    "\n",
    "\n",
    "REGEX_SEGMENT_RESULT = re.compile(\n",
    "    r\"(?:result|keywords|keyword|words|translated|translations|expanded)\\\"?: (\\[[^\\]\\[]*\\])\",\n",
    "    flags=re.IGNORECASE|re.MULTILINE\n",
    ")\n",
    "\n",
    "\n",
    "def segment_word(sentence: str) -> List[str]:\n",
    "    \"\"\"segment sentence to words\"\"\"\n",
    "    if not ENABLE_FULLTEXT_LLM:\n",
    "        return []\n",
    "\n",
    "    system_prompt = dedent(\n",
    "        \"\"\"\n",
    "        You act as a sophisticated tokenization engine. Follow these instructions step by step, executing each step in order and return every step's output:\n",
    "\n",
    "        1. Analyze the user's input text and extract the most significant keywords based on semantic importance. Output these keywords as a comma-separated list.\n",
    "\n",
    "        2. Take the keyword list from step 1 and create a JSON array. And append every keyword's English translations to the end of the array. Please ensure the original words are retained.\n",
    "\n",
    "        3. Return the final result as a JSON array using this exact format, without any additional text or characters. Here is an example of last step output:\n",
    "\n",
    "            ```json\n",
    "            result: [\"word-1\", \"word-2\", \"english-1\", \"english-2\"]\n",
    "            ```\n",
    "        \"\"\"\n",
    "    )\n",
    "    user_prompt = sentence\n",
    "\n",
    "    # resp = llm_oneshot(\"deepseek-coder\", system_prompt, user_prompt)\n",
    "    resp = llm_oneshot_gemma(system_prompt, user_prompt)\n",
    "    logger.debug(f\"segment_word got llm resp {resp=}\")\n",
    "\n",
    "    matched = REGEX_SEGMENT_RESULT.findall(resp)\n",
    "    matched = [val for val in matched if 'word-1' not in val]\n",
    "    assert matched, f\"invalid format of response {resp=}\"\n",
    "\n",
    "    words: List[str] = []\n",
    "    for match in matched:\n",
    "        if 'word-1' in match:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            vs = json.loads(match)\n",
    "            assert isinstance(vs, list)\n",
    "        except Exception:\n",
    "            logger.warning(f\"cannot parse LLM's response {match=}\")\n",
    "            continue\n",
    "\n",
    "        words.extend(vs)\n",
    "\n",
    "    # additions text split\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.extend(word.split(\",\"))\n",
    "    words = new_words\n",
    "\n",
    "    words = [word.strip() for word in words if isinstance(word, str) and word.strip()]\n",
    "    words = [word for word in words if word not in [\"word-1\", \"word-2\", \"english-1\", \"english-2\"]]\n",
    "    words = [word for word in words if len(word) <= 20]\n",
    "    words = list(set(words))\n",
    "\n",
    "    logger.debug(f\"segment_word result {words=}\")\n",
    "    return words\n",
    "\n",
    "\n",
    "@timer\n",
    "def _list_movies_in_db(q: Queue[Movie]):\n",
    "    for docu in col_movies.find().sort([(\"_id\", pymongo.DESCENDING)]):\n",
    "        movie = Movie(\n",
    "            actresses=docu[\"actresses\"],\n",
    "            name=docu[\"name\"],\n",
    "            description=','.join(docu.get(\"descriptions\", [])),\n",
    "            img_urls=docu[\"img_urls\"],\n",
    "            tags=docu[\"tags\"],\n",
    "            published_date=docu.get(\"published_date\"),\n",
    "            urls=docu.get(\"urls\"),\n",
    "        )\n",
    "        q.put(movie)\n",
    "\n",
    "\n",
    "class FulltextItem(NamedTuple):\n",
    "    movie_id: ObjectId\n",
    "    word: str\n",
    "\n",
    "\n",
    "@timer\n",
    "def _generate_fulltext_for_movie(\n",
    "    upstream_q: Queue[Optional[Movie]], downstream_q: Queue[FulltextItem]\n",
    "):\n",
    "    \"\"\"generate fulltext for each movie, each word points to a movie\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            movie = upstream_q.get()\n",
    "            if movie is None:\n",
    "                upstream_q.put(None)\n",
    "                logger.info(f\"_generate_fulltext_for_movie exit\")\n",
    "                return\n",
    "\n",
    "            keywords: List[str] = [movie.name]\n",
    "            keywords.extend(movie.tags)\n",
    "\n",
    "            # load movie\n",
    "            logger.info(f\"_generate_fulltext_for_movie for {movie.name=}\")\n",
    "            movie_docu = col_movies.find_one({\"name\": movie.name})\n",
    "            assert movie_docu, f\"can not find movie {movie.name}\"\n",
    "\n",
    "            # load actress\n",
    "            for docu in col_actress.find({\"_id\": {\"$in\": movie_docu[\"actresses\"]}}):\n",
    "                keywords.extend(docu.get(\"other_names\", []))\n",
    "\n",
    "            keywords = list(set(keywords))\n",
    "\n",
    "            tobe_segment = f\"{','.join(keywords)}, {','.join(movie_docu.get('descriptions', []))}\"\n",
    "            movie_words = segment_word(tobe_segment)\n",
    "            keywords.extend(movie_words)\n",
    "\n",
    "            keywords = list(set(keywords))\n",
    "            for word in keywords:\n",
    "                downstream_q.put(FulltextItem(movie_id=movie_docu[\"_id\"], word=word))\n",
    "        except Exception:\n",
    "            logger.exception(f\"error in _generate_fulltext_for_movie\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def _save_fulltext_item(q: Queue[Optional[FulltextItem]]):\n",
    "    \"\"\"save fulltext item to db, each word may points to multiple movies\"\"\"\n",
    "    last_movie_id: Optional[ObjectId] = None\n",
    "    while True:\n",
    "        try:\n",
    "            item = q.get()\n",
    "            if item is None:\n",
    "                q.put(None)\n",
    "                logger.info(f\"_save_fulltext_item exit\")\n",
    "                return\n",
    "\n",
    "            if item.movie_id != last_movie_id:\n",
    "                col_movies.update_one(\n",
    "                    {\"_id\": last_movie_id},\n",
    "                    {\"$set\": {\"fulltext_updated_at\": datetime.now()}},\n",
    "                )\n",
    "                last_movie_id = item.movie_id\n",
    "\n",
    "            # each word may points to multiple movies,\n",
    "            # these movies are stored in a set\n",
    "            col_fulltext.update_one(\n",
    "                {\"word\": item.word},\n",
    "                {\"$addToSet\": {\"movies\": item.movie_id}},\n",
    "                upsert=True,\n",
    "            )\n",
    "        except Exception:\n",
    "            logger.exception(f\"error in _save_fulltext_item\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def generate_fulltext():\n",
    "    movie_queue = Queue(maxsize=100)\n",
    "    fulltext_queue = Queue(maxsize=100)\n",
    "\n",
    "    fs_list_movies = []\n",
    "    fs_generate_fulltext = []\n",
    "    fs_save_fulltext = []\n",
    "\n",
    "    fs_list_movies.append(executor.submit(_list_movies_in_db, movie_queue))\n",
    "    for i in range(10):\n",
    "        fs_generate_fulltext.append(\n",
    "            executor.submit(_generate_fulltext_for_movie, movie_queue, fulltext_queue)\n",
    "        )\n",
    "\n",
    "    for i in range(20):\n",
    "        fs_save_fulltext.append(executor.submit(_save_fulltext_item, fulltext_queue))\n",
    "\n",
    "    wait(fs_list_movies, return_when=ALL_COMPLETED)\n",
    "    movie_queue.put(None)\n",
    "    wait(fs_generate_fulltext, return_when=ALL_COMPLETED)\n",
    "    fulltext_queue.put(None)\n",
    "    wait(fs_save_fulltext, return_when=ALL_COMPLETED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bc8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_MOVIE_NAME = re.compile(r\"^([A-Z]+)(\\d+)$\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def convert_movies():\n",
    "    # remove empty images\n",
    "    for docu in col_movies.find({\"img_urls\": \"\"}):\n",
    "        imgs = [v for v in docu[\"img_urls\"] if v.strip()]\n",
    "        col_movies.update_one(\n",
    "            {\"_id\": docu[\"_id\"]},\n",
    "            {\"$set\": {\"img_urls\": imgs}},\n",
    "        )\n",
    "\n",
    "\n",
    "@timer\n",
    "def remove_long_words():\n",
    "    result = col_fulltext.delete_many({\"$expr\": {\"$gt\": [{\"$strLenCP\": \"$word\"}, 20]}})\n",
    "    logger.info(f\"remove_long_words {result.deleted_count=}\")\n",
    "\n",
    "\n",
    "@timer\n",
    "def _scan_movies(q: Queue[Optional[Dict[str, Any]]]):\n",
    "    for docu in col_movies.find({\"description\": {\"$exists\": True}}):\n",
    "        q.put(docu)\n",
    "\n",
    "    q.put(None)\n",
    "\n",
    "\n",
    "@timer\n",
    "def _scan_actress(q: Queue[Optional[Dict[str, Any]]]):\n",
    "    # for docu in col_actress.find({\"name\": {\"$regex\": r\"[（\\(]\"}}):\n",
    "    for docu in col_actress.find({\"name\": \"\"}):\n",
    "        # for docu in col_actress.find():\n",
    "        q.put(docu)\n",
    "\n",
    "    q.put(None)\n",
    "\n",
    "\n",
    "@timer\n",
    "def _do_normalize_actress_name(q: Queue[Optional[Dict[str, Any]]]):\n",
    "    while True:\n",
    "        actress_docu = q.get()\n",
    "        if actress_docu is None:\n",
    "            q.put(None)\n",
    "            logger.info(f\"_do_normalize_actress_name exit\")\n",
    "            return\n",
    "\n",
    "        name = actress_docu[\"name\"]\n",
    "        other_names = actress_docu.get(\"other_names\", [])\n",
    "\n",
    "        names = split_actress_name(name)\n",
    "        other_names.extend(names)\n",
    "        assert names, f\"can not find name for {name=}\"\n",
    "\n",
    "        name = choose_kanji_name(names)\n",
    "        other_names.append(name)\n",
    "        parsed_other_names: List[str] = []\n",
    "        for v in other_names:\n",
    "            parsed_other_names.extend(split_actress_name(v))\n",
    "        parsed_other_names = list(set(parsed_other_names))\n",
    "\n",
    "        col_actress.update_one(\n",
    "            {\"_id\": actress_docu[\"_id\"]},\n",
    "            {\n",
    "                \"$set\": {\"name\": name, \"other_names\": parsed_other_names},\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "@timer\n",
    "def normalize_actress_name():\n",
    "    actress_queue = Queue(maxsize=10)\n",
    "    fs_scan_actress = []\n",
    "    fs_convert_actress = []\n",
    "\n",
    "    fs_scan_actress.append(executor.submit(_scan_actress, actress_queue))\n",
    "    for i in range(40):\n",
    "        fs_convert_actress.append(\n",
    "            executor.submit(_do_normalize_actress_name, actress_queue)\n",
    "        )\n",
    "\n",
    "    wait(fs_scan_actress, return_when=ALL_COMPLETED)\n",
    "    actress_queue.put(None)\n",
    "    wait(fs_convert_actress, return_when=ALL_COMPLETED)\n",
    "\n",
    "\n",
    "@timer\n",
    "def _convert_movie(q: Queue[Optional[Dict[str, Any]]]):\n",
    "    while True:\n",
    "        movie_docu = q.get()\n",
    "        if movie_docu is None:\n",
    "            q.put(None)\n",
    "            logger.info(f\"_convert_movie exit\")\n",
    "            return\n",
    "\n",
    "        col_movies.update_one(\n",
    "            {\"_id\": movie_docu[\"_id\"]},\n",
    "            {\n",
    "                \"$set\": {\"descriptions\": [movie_docu[\"description\"]]},\n",
    "                \"$unset\": {\"description\": \"\"},\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "_duplicate_actress_lock = RLock()\n",
    "_duplicate_actress_docu_lock: Dict[str, RLock] = {}\n",
    "\n",
    "\n",
    "@timer\n",
    "def _do_reset_actress_name(q: Queue[Optional[Dict[str, Any]]]):\n",
    "    while True:\n",
    "        actress_docu = q.get()\n",
    "        if actress_docu is None:\n",
    "            q.put(None)\n",
    "            logger.info(f\"_do_reset_actress_name exit\")\n",
    "            return\n",
    "\n",
    "        other_names = [v for v in actress_docu[\"other_names\"] if v.strip()]\n",
    "        name = choose_kanji_name(other_names)\n",
    "        col_actress.update_one(\n",
    "            {\"_id\": actress_docu[\"_id\"]},\n",
    "            {\n",
    "                \"$set\": {\n",
    "                    \"name\": name,\n",
    "                    \"other_names\": other_names,\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "\n",
    "@timer\n",
    "def reset_actress_name():\n",
    "    actress_queue = Queue(maxsize=40)\n",
    "    fs_scan_actress = []\n",
    "    fs_convert_actress = []\n",
    "\n",
    "    fs_scan_actress.append(executor.submit(_scan_actress, actress_queue))\n",
    "    for i in range(40):\n",
    "        fs_convert_actress.append(\n",
    "            executor.submit(_do_reset_actress_name, actress_queue)\n",
    "        )\n",
    "\n",
    "    wait(fs_scan_actress, return_when=ALL_COMPLETED)\n",
    "    actress_queue.put(None)\n",
    "    wait(fs_convert_actress, return_when=ALL_COMPLETED)\n",
    "\n",
    "\n",
    "@timer\n",
    "def _deal_duplicate_actress(q: Queue[Optional[Dict[str, Any]]]):\n",
    "    while True:\n",
    "        actress_docu = q.get()\n",
    "        if actress_docu is None:\n",
    "            q.put(None)\n",
    "            logger.info(f\"_deal_duplicate_actress exit\")\n",
    "            return\n",
    "\n",
    "        all_docus = [actress_docu]\n",
    "        for name in actress_docu[\"other_names\"]:\n",
    "            all_docus.extend(col_actress.find({\"other_names\": name}))\n",
    "\n",
    "        earliest_docu = sorted(all_docus, key=lambda x: x[\"_id\"])[0]\n",
    "        tobe_delete_docus = [\n",
    "            docu for docu in all_docus if docu[\"_id\"] != earliest_docu[\"_id\"]\n",
    "        ]\n",
    "\n",
    "        if not tobe_delete_docus:\n",
    "            continue\n",
    "\n",
    "        with _duplicate_actress_lock:\n",
    "            lock = _duplicate_actress_docu_lock.get(earliest_docu[\"_id\"], RLock())\n",
    "\n",
    "        with lock:\n",
    "            for docu in all_docus:\n",
    "                earliest_docu.update(\n",
    "                    {\n",
    "                        \"urls\": earliest_docu.get(\"urls\", []) + docu.get(\"urls\", []),\n",
    "                        \"other_names\": earliest_docu[\"other_names\"]\n",
    "                        + docu[\"other_names\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # replace actress id in movies by earliest actress id\n",
    "                if docu[\"_id\"] != earliest_docu[\"_id\"]:\n",
    "                    col_movies.update_many(\n",
    "                        {\"actresses\": docu[\"_id\"]},\n",
    "                        {\n",
    "                            \"$addToSet\": {\"actresses\": earliest_docu[\"_id\"]},\n",
    "                        },\n",
    "                    )\n",
    "\n",
    "            earliest_docu[\"urls\"] = list(set(earliest_docu[\"urls\"]))\n",
    "            earliest_docu[\"other_names\"] = [\n",
    "                v for v in earliest_docu[\"other_names\"] if v.strip()\n",
    "            ]\n",
    "            earliest_docu[\"other_names\"] = list(set(earliest_docu[\"other_names\"]))\n",
    "            earliest_docu[\"name\"] = choose_kanji_name(earliest_docu[\"other_names\"])\n",
    "\n",
    "            # print(f\"{earliest_docu=}\")\n",
    "            # return\n",
    "\n",
    "            # print(f'{earliest_docu[\"_id\"]}')\n",
    "            # print(f\"{tobe_delete_docus=}\")\n",
    "            # return\n",
    "\n",
    "            # update actress\n",
    "            col_actress.update_one(\n",
    "                {\"_id\": earliest_docu[\"_id\"]},\n",
    "                {\n",
    "                    \"$set\": {\n",
    "                        \"name\": earliest_docu[\"name\"],\n",
    "                    },\n",
    "                    \"$addToSet\": {\n",
    "                        \"urls\": {\"$each\": earliest_docu[\"urls\"]},\n",
    "                        \"other_names\": {\"$each\": earliest_docu[\"other_names\"]},\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # delete duplicate actress in movies\n",
    "            col_movies.update_many(\n",
    "                {\"actresses\": {\"$in\": [docu[\"_id\"] for docu in tobe_delete_docus]}},\n",
    "                {\n",
    "                    \"$pull\": {\n",
    "                        \"actresses\": {\n",
    "                            \"$in\": [docu[\"_id\"] for docu in tobe_delete_docus]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # db.movies.update_many(\n",
    "            #     {\"actresses\": {\"$in\": [ObjectId(\"66becdb7c59907294115f007\")]}},\n",
    "            #     {\"$pull\": {\"actresses\": {\"$in\": [ObjectId(\"66becdb7c59907294115f007\")]}}},\n",
    "            # )\n",
    "\n",
    "            # delete duplicate actress\n",
    "            col_actress.delete_many(\n",
    "                {\"_id\": {\"$in\": [docu[\"_id\"] for docu in tobe_delete_docus]}}\n",
    "            )\n",
    "\n",
    "            logger.info(\n",
    "                f\"gather {len(tobe_delete_docus)} duplicate actresses into {earliest_docu['name']=}, {earliest_docu['_id']=}\"\n",
    "            )\n",
    "\n",
    "            with _duplicate_actress_lock:\n",
    "                try:\n",
    "                    del _duplicate_actress_docu_lock[earliest_docu[\"_id\"]]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "\n",
    "@timer\n",
    "def remove_duplicate_actress():\n",
    "    actress_queue = Queue(maxsize=10)\n",
    "    fs_scan_actress = []\n",
    "    fs_convert_actress = []\n",
    "\n",
    "    fs_scan_actress.append(executor.submit(_scan_actress, actress_queue))\n",
    "    for i in range(50):\n",
    "        fs_convert_actress.append(\n",
    "            executor.submit(_deal_duplicate_actress, actress_queue)\n",
    "        )\n",
    "\n",
    "    wait(fs_scan_actress, return_when=ALL_COMPLETED)\n",
    "    actress_queue.put(None)\n",
    "    wait(fs_convert_actress, return_when=ALL_COMPLETED)\n",
    "\n",
    "\n",
    "@timer\n",
    "def convert_movie_description_to_array():\n",
    "    movie_queue = Queue(maxsize=10)\n",
    "    fs_scan_movies = []\n",
    "    fs_convert_movie = []\n",
    "\n",
    "    fs_scan_movies.append(executor.submit(_scan_movies, movie_queue))\n",
    "    for i in range(20):\n",
    "        fs_convert_movie.append(executor.submit(_convert_movie, movie_queue))\n",
    "\n",
    "    wait(fs_scan_movies, return_when=ALL_COMPLETED)\n",
    "    wait(fs_convert_movie, return_when=ALL_COMPLETED)\n",
    "\n",
    "\n",
    "# convert_movie_description_to_array()\n",
    "# reset_actress_name()\n",
    "remove_duplicate_actress()\n",
    "# normalize_actress_name()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
