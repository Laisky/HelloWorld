{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import traceback\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import tweepy\n",
    "from tweepy import API, OAuthHandler\n",
    "\n",
    "\n",
    "# sys.path.append(r'/Users/laisky/repo/laisky/ramjet/ramjet/settings')\n",
    "sys.path.append(r'/opt/configs/ramjet')\n",
    "sys.path\n",
    "import prd\n",
    "\n",
    "mongo = MongoClient(\n",
    "    f\"mongodb://{prd.MONGO_USER}:{prd.MONGO_PASSWD}@{prd.MONGO_HOST}:{prd.MONGO_PORT}/{prd.MONGO_DB}\",\n",
    ")\n",
    "tweets = mongo['twitter']['tweets']\n",
    "\n",
    "auth = OAuthHandler(prd.CONSUMER_KEY, prd.CONSUMER_SECRET)\n",
    "auth.set_access_token(prd.ACCESS_TOKEN, prd.ACCESS_TOKEN_SECRET)\n",
    "api = API(auth, wait_on_rate_limit=True, parser=tweepy.parsers.JSONParser())\n",
    "api.me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api.user_timeline(count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# delete tweets\n",
    "\n",
    "\n",
    "def is_status_tobe_delete(tweet):\n",
    "    if len(tweet.get('entities', {}).get(\"hashtags\", [])) >= 1:\n",
    "        return False\n",
    "    \n",
    "    if tweet['created_at'] < datetime(2017,6,5):\n",
    "        return True\n",
    "\n",
    "def delete():\n",
    "#     last_tweets = api.user_timeline(count=1)\n",
    "#     if not last_tweets:\n",
    "#         return\n",
    "    \n",
    "#     print(last_tweets[0].created_at < datetime(2019,7,1))\n",
    "#     print(last_tweets[0])\n",
    "#     print(dir(last_tweets[0]))\n",
    "#     return\n",
    "\n",
    "#     current_id = last_tweets[0].id\n",
    "    current_id = \"379207474345046017\"\n",
    "    n = n_del = 1\n",
    "#         tweets = api.user_timeline(max_id=current_id, count=1000)\n",
    "    for tweet in tweets.find({'user.screen_name': 'ppcelery'}).sort(\"_id\", -1):\n",
    "        n += 1\n",
    "\n",
    "        if is_status_tobe_delete(tweet):\n",
    "            n_del += 1\n",
    "            print(\"delete\", tweet['id'],tweet['created_at'])\n",
    "            try:\n",
    "                api.destroy_status(tweet['id'])\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            current_id = tweet['id']\n",
    "\n",
    "        if n % 1000 == 0:\n",
    "            print(\">> scan {} statuses\".format(n-1))\n",
    "        if n_del % 100 == 0:\n",
    "            print(\">> delete {} statuses\".format(n_del-1))\n",
    "\n",
    "\n",
    "delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download image\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "dirpath = r'/var/www/uploads/twitter'\n",
    "# dirpath = r'/Users/laisky/Downloads'\n",
    "\n",
    "\n",
    "def download_images_for_tweet(tweet):\n",
    "    for img in tweet['entities']['media']:\n",
    "        with requests.get(img['media_url_https']+\":orig\") as r:\n",
    "            if r.status_code != 200:\n",
    "                print(f\"download error: [{r.status_code}]{r.content}\")\n",
    "                continue\n",
    "                \n",
    "            fpath = Path(dirpath, img['media_url_https'].split(\"/\")[-1])\n",
    "            if fpath.is_file():\n",
    "                continue\n",
    "                \n",
    "            with open(fpath, 'wb') as f:\n",
    "                f.write(r.content) \n",
    "                \n",
    "            print(\"tweet img ok\", tweet['id'], fpath)\n",
    "            \n",
    "\n",
    "def download_images():\n",
    "    for tweet in tweets.find({\"entities.media\": {\"$exists\": 1}}, no_cursor_timeout=True).sort(\"_id\", -1):\n",
    "        download_images_for_tweet(tweet)\n",
    "        \n",
    "        \n",
    "download_images()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download related tweets\n",
    "\n",
    "def twitter_api_parser(tweet):\n",
    "    reg_topic = re.compile(r\"[\\b|\\s]#(\\S+)\")\n",
    "    tweet['topics'] = reg_topic.findall(\n",
    "        tweet['text'].replace('.', '_')\n",
    "    )\n",
    "    tweet['created_at'] = datetime.strptime(\n",
    "        tweet['created_at'], '%a %b %d %H:%M:%S +0000 %Y'\n",
    "    )\n",
    "\n",
    "    # replace url\n",
    "    t = tweet['text']\n",
    "    if 't.co' in t:\n",
    "        # parse entities media\n",
    "        if 'media' in tweet['entities']:\n",
    "            for media in tweet['entities']['media']:\n",
    "                surl = media['url']\n",
    "                eurl = media['media_url']\n",
    "                t = t.replace(surl, eurl)\n",
    "\n",
    "        # parse entities urls\n",
    "        if 'urls' in tweet['entities']:\n",
    "            for d in tweet['entities']['urls']:\n",
    "                surl = d['url']\n",
    "                eurl = d['expanded_url']\n",
    "                t = t.replace(surl, eurl)\n",
    "\n",
    "        tweet['text'] = t\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def save_relate_tweets(status):\n",
    "    related_ids = []\n",
    "    status.get(\"in_reply_to_status_id\") and related_ids.append(status['in_reply_to_status_id'])\n",
    "    status.get(\"retweeted_status\") and related_ids.append(status['retweeted_status']['id'])\n",
    "    status.get(\"quoted_status\") and related_ids.append(status['quoted_status']['id'])\n",
    "    related_ids = filter(lambda id_: not tweets.find_one({\"id\": id_}), related_ids)\n",
    "    \n",
    "    for id_ in related_ids:\n",
    "        try:\n",
    "            docu = api.get_status(id_)\n",
    "        except Exception as err:\n",
    "            print(f\"load tweet {id_} got error: {err}\")\n",
    "        else:\n",
    "            print(f\"save tweet [{docu['user']['screen_name']}]{docu['id']}\")\n",
    "            save_tweet(docu)\n",
    "            save_relate_tweets(docu)\n",
    "            \n",
    "def save_tweet(docu):\n",
    "    docu = twitter_api_parser(docu)\n",
    "    tweets.update_one(\n",
    "        {'id': docu['id']},\n",
    "        {'$set': docu},\n",
    "        upsert=True\n",
    "    )\n",
    "\n",
    "def download_relate_tweets():\n",
    "    for tweet in tweets.find(no_cursor_timeout=True).sort(\"_id\", -1):\n",
    "        save_relate_tweets(tweet)\n",
    "\n",
    "        \n",
    "download_relate_tweets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
