{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537d6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embeddings\"].deployment_id\n",
    "azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "# os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "logger = setup_logger(\"security\")\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 定义文件路径\n",
    "# =============================\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# 通用的函数定义\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(fpath)\n",
    "            for page, data in enumerate(loader.load_and_split()):\n",
    "                splits = text_splitter.split_text(data.page_content)\n",
    "                docs.extend(splits)\n",
    "                for ichunk, _ in enumerate(splits):\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "        except Exception as err:\n",
    "            logger.error(f\"skip file {fpath}: {err}\")\n",
    "            continue\n",
    "\n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fpath):\n",
    "            continue\n",
    "\n",
    "        with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "            docus = markdown_splitter.create_documents([fp.read()])\n",
    "            for ichunk, docu in enumerate(docus):\n",
    "                docs.append(docu.page_content)\n",
    "                title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "                if url:\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "                else:\n",
    "                    metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=azure_embeddings_deploymentid,\n",
    "    )\n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0db0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 生成 Basebit XEGO Doc index\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=200)\n",
    "\n",
    "\n",
    "def scan_xego_docs_in_mongo(index: Index) -> FAISS:\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    dbconn = MongoClient(prd.OPENAI_EMBEDDING_QA[\"bbt-xego\"][\"mongo_source\"])\n",
    "    cursor = dbconn[\"basebit\"][\"docus\"].find().max_time_ms(0).batch_size(50)\n",
    "    i = 0\n",
    "    for doc in cursor:\n",
    "        i += 1\n",
    "        splits = markdown_splitter.split_text(doc[\"text\"])\n",
    "        docs.extend(splits)\n",
    "        metadatas.extend([{\"source\": doc[\"url\"]}] * len(splits))\n",
    "        \n",
    "    dbconn.close()\n",
    "    index.store.add_texts(docs, metadatas=metadatas)\n",
    "    return i\n",
    "\n",
    "\n",
    "def main():\n",
    "    index = new_store()\n",
    "#     index = load_store(\n",
    "#         dirpath=\"/home/laisky/data/langchain/index\",\n",
    "#         name=\"bbt-xego\",\n",
    "#     )\n",
    "    n = scan_xego_docs_in_mongo(index)\n",
    "    save_store(\n",
    "        index=index, \n",
    "        dirpath=index_dirpath, \n",
    "        name=\"bbt-xego\",\n",
    "    )\n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be35dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 生成用于问答的 query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=index_dirpath,\n",
    "    name=\"bbt-xego\",\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name=\"gpt-3.5-turbo\", \n",
    "#     temperature=0, \n",
    "#     max_tokens=1000)  \n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=azure_gpt_deploymentid,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "# chain = VectorDBQAWithSourcesChain.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     vectorstore=index.store,\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs=chain_type_kwargs,\n",
    "#     reduce_k_below_max_tokens=True,\n",
    "# )\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec62c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔️: 如何实现 PKI\n",
      "\n",
      "🤖️: PKI可以实现一个加解密的数据处理层，可以接入一个S3\n",
      "    API作为外部存储介质。PKI系统内会存储一个用户表，保存用户名、uid、角色、TOTP token、recov\n",
      "    erCode，有且仅有一个超管用户，超管可以修改PKI的所有配置。操作PKI，除了要进行SSO，还需要输入TOT\n",
      "    P密钥证明身份。可以为CA设置ca_admin，CA admin可以启用CA的2-step认证。不使用数据库，所\n",
      "    有需要持久化的数据都以文件的形式加密后存储到S3。持久化的数据包括：【加密】masterKey【加密】超管账户信\n",
      "    息（用户名、密码、recoverCode）【加密】服务端完整配置文件【签名】待签CSR信息（包含。PKI以确保i\n",
      "    n-use安全。服务初次启动进行初始化，创建超管、recoverAdmins、RootCA、MK、RK、RKS、\n",
      "    RAK。PKI基于RBAC的设计，会为用户分配角色，角色是一组权限的集合，用户通过角色来获得权限。有如下角色：超\n",
      "    级管理员（superAdmin）证书管理员（certAdmin）恢复管理员（recoverAdmin）用户（us\n",
      "    er）rootCA必须关联到一个certAdmin，这个certAdmin就是超级管理员。恢复管理员可以将自己的\n",
      "    职位禅让给其他人。恢复管理员的K和L。\n",
      "\n",
      "📖: http://xego-dev.basebit.me/doc/xss/2023/02/xss/pki/refs/components/ http://xego-dev.basebit.me/doc/xss/2023/01/xss/pki/refs/flows/ http://xego-dev.basebit.me/doc/xss/2023/01/xss/pki/refs/user-and-perm/\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# chatbot as service\n",
    "# ====================================\n",
    "question = \"如何实现 PKI\"\n",
    "result = chain(\n",
    "    {\n",
    "        \"question\": question,\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "\n",
    "print(f\"🤔️: {question}\\n\")\n",
    "print(f\"🤖️: {pretty_print(result['answer'])}\\n\")\n",
    "print(f\"📖: {result['sources']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
