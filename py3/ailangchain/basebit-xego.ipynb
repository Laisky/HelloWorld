{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537d6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embeddings\"].deployment_id\n",
    "azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "# os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "logger = setup_logger(\"security\")\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
    "# =============================\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# é€šç”¨çš„å‡½æ•°å®šä¹‰\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(fpath)\n",
    "            for page, data in enumerate(loader.load_and_split()):\n",
    "                splits = text_splitter.split_text(data.page_content)\n",
    "                docs.extend(splits)\n",
    "                for ichunk, _ in enumerate(splits):\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "        except Exception as err:\n",
    "            logger.error(f\"skip file {fpath}: {err}\")\n",
    "            continue\n",
    "\n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fpath):\n",
    "            continue\n",
    "\n",
    "        with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "            docus = markdown_splitter.create_documents([fp.read()])\n",
    "            for ichunk, docu in enumerate(docus):\n",
    "                docs.append(docu.page_content)\n",
    "                title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "                if url:\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "                else:\n",
    "                    metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=azure_embeddings_deploymentid,\n",
    "    )\n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0db0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# ç”Ÿæˆ Basebit XEGO Doc index\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=200)\n",
    "\n",
    "\n",
    "def scan_xego_docs_in_mongo(index: Index) -> FAISS:\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    dbconn = MongoClient(prd.OPENAI_EMBEDDING_QA[\"bbt-xego\"][\"mongo_source\"])\n",
    "    cursor = dbconn[\"basebit\"][\"docus\"].find().max_time_ms(0).batch_size(50)\n",
    "    i = 0\n",
    "    for doc in cursor:\n",
    "        i += 1\n",
    "        splits = markdown_splitter.split_text(doc[\"text\"])\n",
    "        docs.extend(splits)\n",
    "        metadatas.extend([{\"source\": doc[\"url\"]}] * len(splits))\n",
    "        \n",
    "    dbconn.close()\n",
    "    index.store.add_texts(docs, metadatas=metadatas)\n",
    "    return i\n",
    "\n",
    "\n",
    "def main():\n",
    "    index = new_store()\n",
    "#     index = load_store(\n",
    "#         dirpath=\"/home/laisky/data/langchain/index\",\n",
    "#         name=\"bbt-xego\",\n",
    "#     )\n",
    "    n = scan_xego_docs_in_mongo(index)\n",
    "    save_store(\n",
    "        index=index, \n",
    "        dirpath=index_dirpath, \n",
    "        name=\"bbt-xego\",\n",
    "    )\n",
    "    \n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be35dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# ç”Ÿæˆç”¨äºé—®ç­”çš„ query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=index_dirpath,\n",
    "    name=\"bbt-xego\",\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name=\"gpt-3.5-turbo\", \n",
    "#     temperature=0, \n",
    "#     max_tokens=1000)  \n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=azure_gpt_deploymentid,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "# chain = VectorDBQAWithSourcesChain.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     vectorstore=index.store,\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs=chain_type_kwargs,\n",
    "#     reduce_k_below_max_tokens=True,\n",
    "# )\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec62c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤”ï¸: å¦‚ä½•å®ç° PKI\n",
      "\n",
      "ğŸ¤–ï¸: PKIå¯ä»¥å®ç°ä¸€ä¸ªåŠ è§£å¯†çš„æ•°æ®å¤„ç†å±‚ï¼Œå¯ä»¥æ¥å…¥ä¸€ä¸ªS3\n",
      "    APIä½œä¸ºå¤–éƒ¨å­˜å‚¨ä»‹è´¨ã€‚PKIç³»ç»Ÿå†…ä¼šå­˜å‚¨ä¸€ä¸ªç”¨æˆ·è¡¨ï¼Œä¿å­˜ç”¨æˆ·åã€uidã€è§’è‰²ã€TOTP tokenã€recov\n",
      "    erCodeï¼Œæœ‰ä¸”ä»…æœ‰ä¸€ä¸ªè¶…ç®¡ç”¨æˆ·ï¼Œè¶…ç®¡å¯ä»¥ä¿®æ”¹PKIçš„æ‰€æœ‰é…ç½®ã€‚æ“ä½œPKIï¼Œé™¤äº†è¦è¿›è¡ŒSSOï¼Œè¿˜éœ€è¦è¾“å…¥TOT\n",
      "    På¯†é’¥è¯æ˜èº«ä»½ã€‚å¯ä»¥ä¸ºCAè®¾ç½®ca_adminï¼ŒCA adminå¯ä»¥å¯ç”¨CAçš„2-stepè®¤è¯ã€‚ä¸ä½¿ç”¨æ•°æ®åº“ï¼Œæ‰€\n",
      "    æœ‰éœ€è¦æŒä¹…åŒ–çš„æ•°æ®éƒ½ä»¥æ–‡ä»¶çš„å½¢å¼åŠ å¯†åå­˜å‚¨åˆ°S3ã€‚æŒä¹…åŒ–çš„æ•°æ®åŒ…æ‹¬ï¼šã€åŠ å¯†ã€‘masterKeyã€åŠ å¯†ã€‘è¶…ç®¡è´¦æˆ·ä¿¡\n",
      "    æ¯ï¼ˆç”¨æˆ·åã€å¯†ç ã€recoverCodeï¼‰ã€åŠ å¯†ã€‘æœåŠ¡ç«¯å®Œæ•´é…ç½®æ–‡ä»¶ã€ç­¾åã€‘å¾…ç­¾CSRä¿¡æ¯ï¼ˆåŒ…å«ã€‚PKIä»¥ç¡®ä¿i\n",
      "    n-useå®‰å…¨ã€‚æœåŠ¡åˆæ¬¡å¯åŠ¨è¿›è¡Œåˆå§‹åŒ–ï¼Œåˆ›å»ºè¶…ç®¡ã€recoverAdminsã€RootCAã€MKã€RKã€RKSã€\n",
      "    RAKã€‚PKIåŸºäºRBACçš„è®¾è®¡ï¼Œä¼šä¸ºç”¨æˆ·åˆ†é…è§’è‰²ï¼Œè§’è‰²æ˜¯ä¸€ç»„æƒé™çš„é›†åˆï¼Œç”¨æˆ·é€šè¿‡è§’è‰²æ¥è·å¾—æƒé™ã€‚æœ‰å¦‚ä¸‹è§’è‰²ï¼šè¶…\n",
      "    çº§ç®¡ç†å‘˜ï¼ˆsuperAdminï¼‰è¯ä¹¦ç®¡ç†å‘˜ï¼ˆcertAdminï¼‰æ¢å¤ç®¡ç†å‘˜ï¼ˆrecoverAdminï¼‰ç”¨æˆ·ï¼ˆus\n",
      "    erï¼‰rootCAå¿…é¡»å…³è”åˆ°ä¸€ä¸ªcertAdminï¼Œè¿™ä¸ªcertAdminå°±æ˜¯è¶…çº§ç®¡ç†å‘˜ã€‚æ¢å¤ç®¡ç†å‘˜å¯ä»¥å°†è‡ªå·±çš„\n",
      "    èŒä½ç¦…è®©ç»™å…¶ä»–äººã€‚æ¢å¤ç®¡ç†å‘˜çš„Kå’ŒLã€‚\n",
      "\n",
      "ğŸ“–: http://xego-dev.basebit.me/doc/xss/2023/02/xss/pki/refs/components/ http://xego-dev.basebit.me/doc/xss/2023/01/xss/pki/refs/flows/ http://xego-dev.basebit.me/doc/xss/2023/01/xss/pki/refs/user-and-perm/\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# chatbot as service\n",
    "# ====================================\n",
    "question = \"å¦‚ä½•å®ç° PKI\"\n",
    "result = chain(\n",
    "    {\n",
    "        \"question\": question,\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¤”ï¸: {question}\\n\")\n",
    "print(f\"ğŸ¤–ï¸: {pretty_print(result['answer'])}\\n\")\n",
    "print(f\"ğŸ“–: {result['sources']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
