{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "537d6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embedding\"].deployment_id\n",
    "azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "# os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "logger = setup_logger(\"security\")\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# é€šç”¨çš„å‡½æ•°å®šä¹‰\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(fpath)\n",
    "            for page, data in enumerate(loader.load_and_split()):\n",
    "                splits = text_splitter.split_text(data.page_content)\n",
    "                docs.extend(splits)\n",
    "                for ichunk, _ in enumerate(splits):\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "        except Exception as err:\n",
    "            logger.error(f\"skip file {fpath}: {err}\")\n",
    "            continue\n",
    "\n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fpath):\n",
    "            continue\n",
    "\n",
    "        with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "            docus = markdown_splitter.create_documents([fp.read()])\n",
    "            for ichunk, docu in enumerate(docus):\n",
    "                docs.append(docu.page_content)\n",
    "                title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "                if url:\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "                else:\n",
    "                    metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=azure_embeddings_deploymentid,\n",
    "    )\n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "addb310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
    "# =============================\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-04 03:45:41,545 - ERROR - /tmp/ipykernel_1424395/578690748.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/RFC2986_Certification Request Syntax Specification.pdf: EOF marker not found\n",
      "scaned /home/laisky/data/langchain/pdf/security/NIST_SP-800-78-4-Cryptographic Algorithms and Key Sizes for Personal Identity Verification.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/NIST_SP-800-73-4_Interfaces for Personal Identity Verification.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/ISOIEC 27001-2022_Information security, cybersecurity and privacy protection - Information security management systems - Requirements.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/Practical Threshold Signatures.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/NIST_SP-800-57-Part 3_Application-Specific Key Management Guidance.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/RFC5280_Internet X.509 Public Key Infrastructure Certificate and Certificate Revocation List (CRL) Profile.pdf\n",
      "save store to /home/laisky/data/langchain/index-azure/security\n",
      "scanned 6 files\n",
      "[2023-06-04 04:21:14,850 - ERROR - /tmp/ipykernel_1424395/578690748.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/RFC2986_Certification Request Syntax Specification.pdf: EOF marker not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0xb57a for key /Ascent\n",
      "Multiple definitions in dictionary at byte 0x303c0 for key /Ascent\n",
      "Multiple definitions in dictionary at byte 0x4556f for key /Ascent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaned /home/laisky/data/langchain/pdf/security/Computer Security Art And Science_Matt Bishop, Elisabeth Sullivan etc.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x79331 for key /Ascent\n",
      "Created a chunk of size 676, which is longer than the specified 500\n",
      "Overwriting cache for 0 2166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaned /home/laisky/data/langchain/pdf/security/ç¥ç»ç½‘ç»œåŒæ­¥çš„åˆ¤å®šåŠåœ¨ç¥ç»å¯†ç ä¸­çš„åº”ç”¨.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/NIST_FIPS-198-1_The Keyed-Hash Message Authentication Code.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/NIST_FIPS-180-4_Secure Hash Standard.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/NIST_SP-800-89_Recommendation for Obtaining Assurances for Digital Signature Applications.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/whitepaper_Security of AWS CloudHSM Backups.pdf\n"
     ]
    }
   ],
   "source": [
    "# incremental scan pdfs\n",
    "\n",
    "def gen_pdfs():\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/pdf/security/**/*.pdf\", recursive=True)\n",
    "\n",
    "def run_scan_pdfs():\n",
    "#     index = new_store()\n",
    "#     save_store(\n",
    "#         index=index, \n",
    "#         dirpath=index_dirpath, \n",
    "#         name=\"security\",\n",
    "#     )\n",
    "    \n",
    "    total = 0\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=index_dirpath,\n",
    "            name=\"security\",\n",
    "        )\n",
    "        n = embedding_pdfs(\n",
    "            index=index,\n",
    "            fpaths=gen_pdfs(),\n",
    "            url=\"https://s3.laisky.com/public/papers/security/\",\n",
    "            replace_by_url=\"/home/laisky/data/langchain/pdf/security/\",\n",
    "        )\n",
    "        total += n\n",
    "        save_store(\n",
    "            index=index, \n",
    "            dirpath=index_dirpath, \n",
    "            name=\"security\",\n",
    "        )\n",
    "        \n",
    "#         return\n",
    "        print(f\"scanned {total} files\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "run_scan_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49778be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# incremental scan markdowns\n",
    "\n",
    "def gen_markdowns():\n",
    "    yield \"/home/laisky/data/langchain/basebit/doc/content/terms.md\"\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/basebit/doc/content/research/**/*.md\", recursive=True)\n",
    "    \n",
    "\n",
    "def run_scan_markdowns():\n",
    "#         index = new_store()\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=\"/home/laisky/data/langchain/index\",\n",
    "            name=\"security\",\n",
    "        )\n",
    "        files = gen_markdowns()\n",
    "        n = embedding_markdowns(\n",
    "            index=index,\n",
    "            fpaths=files,\n",
    "            url=\"https://s3.laisky.com/public/papers/security/\",\n",
    "            replace_by_url=\"/home/laisky/data/langchain/pdf/security/\",\n",
    "        )\n",
    "        save_store(\n",
    "            index=index,\n",
    "            dirpath=\"/home/laisky/data/langchain/index/\", \n",
    "            name=\"security\",\n",
    "        )\n",
    "        \n",
    "        print(f\"{n=}\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "        \n",
    "run_scan_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eae4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# ç”Ÿæˆç”¨äºé—®ç­”çš„ query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=index_dirpath,\n",
    "    name=\"security\",\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name=\"gpt-3.5-turbo\", \n",
    "#     temperature=0, \n",
    "#     max_tokens=1000)  \n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=azure_gpt_deploymentid,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "# chain = VectorDBQAWithSourcesChain.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     vectorstore=index.store,\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs=chain_type_kwargs,\n",
    "#     reduce_k_below_max_tokens=True,\n",
    "# )\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.store.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec62c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤”ï¸: TLS èƒ½å¹²å•¥\n",
      "\n",
      "ğŸ¤–ï¸: TLS æ˜¯ä¸€ç§ä¼ è¾“å±‚å®‰å…¨åè®®ï¼Œä¸»è¦ç”¨äºæä¾›æ•°æ®çš„åŠ å¯†å’Œè®¤è¯ã€‚å®ƒå¯ä»¥åœ¨ç½‘ç»œè¿æ¥ä¸­æä¾›ç«¯åˆ°ç«¯çš„å®‰å…¨æ€§ï¼ŒåŒ…æ‹¬åœ¨å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹\n",
      "    é—´ä¼ è¾“çš„æ•°æ®çš„æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œèº«ä»½éªŒè¯ã€‚TLS\n",
      "    èƒ½å¤Ÿé˜²æ­¢æ•°æ®åœ¨ä¼ è¾“è¿‡ç¨‹ä¸­è¢«çªƒå¬ã€ç¯¡æ”¹å’Œä¼ªé€ ã€‚åœ¨ç½‘ç»œå®‰å…¨æ–¹é¢ï¼ŒTLS æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„åè®®ï¼Œåº”è¯¥å­¦ä¹ å’Œäº†è§£ã€‚\n",
      "\n",
      "ğŸ“–: https://s3.laisky.com/public/papers/security/Brian%20Ward%20-%20How%20Linux%20Works%20%282021%2C%20No%20Starch%20Press%29.pdf#page=316\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ask pdf embeddings\n",
    "# ====================================\n",
    "question = \"TLS èƒ½å¹²å•¥\"\n",
    "result = chain(\n",
    "    {\n",
    "        \"question\": question,\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¤”ï¸: {question}\\n\")\n",
    "print(f\"ğŸ¤–ï¸: {pretty_print(result['answer'])}\\n\")\n",
    "print(f\"ğŸ“–: {result['sources']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
