{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537d6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "# os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "# os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "# os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "# os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "# openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "# openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "# openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embeddings\"].deployment_id\n",
    "# azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
    "# =============================\n",
    "name = \"security\"\n",
    "logger = setup_logger(name)\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure\"\n",
    "pdf_dirpath = f\"/home/laisky/data/langchain/pdf/{name}\"\n",
    "\n",
    "for path in [index_dirpath, pdf_dirpath]:\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# é€šç”¨çš„å‡½æ•°å®šä¹‰\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from ramjet.tasks.gptchat.embedding.embeddings import reset_eof_of_pdf\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            reset_eof_of_pdf(fpath)\n",
    "            loader = PyPDFLoader(fpath)\n",
    "            for page, data in enumerate(loader.load_and_split()):\n",
    "                splits = text_splitter.split_text(data.page_content)\n",
    "                docs.extend(splits)\n",
    "                for ichunk, _ in enumerate(splits):\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "        except Exception:\n",
    "            logger.exception(f\"skip file {fpath}\")\n",
    "            continue\n",
    "\n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        # fix stupid compatability issue in langchain faiss\n",
    "        if not getattr(index.store, \"_normalize_L2\", None):\n",
    "            index.store._normalize_L2 = False\n",
    "            \n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fpath):\n",
    "            continue\n",
    "\n",
    "        with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "            docus = markdown_splitter.create_documents([fp.read()])\n",
    "            for ichunk, docu in enumerate(docus):\n",
    "                docs.append(docu.page_content)\n",
    "                title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "                if url:\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "                else:\n",
    "                    metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"OPENAI_API_TYPE\") == \"azure\":\n",
    "        azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\n",
    "            \"embeddings\"\n",
    "        ].deployment_id\n",
    "        # azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            deployment=azure_embeddings_deploymentid,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "    \n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "        \n",
    "    # compatable with azure/openai embeddings\n",
    "    store.embedding_function = embedding_model.embed_query\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    if os.environ.get(\"OPENAI_API_TYPE\") == \"azure\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            deployment=azure_embeddings_deploymentid,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "        \n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f488aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-24 03:06:36,189 - ERROR - /tmp/ipykernel_3350828/996360124.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/RFC2986_Certification Request Syntax Specification.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3350828/996360124.py\", line 39, in embedding_pdfs\n",
      "    for page, data in enumerate(loader.load_and_split()):\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 43, in load_and_split\n",
      "    docs = self.load()\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 118, in load\n",
      "    return list(self.lazy_load())\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 125, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 95, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/parsers/pdf.py\", line 20, in lazy_parse\n",
      "    pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 322, in __init__\n",
      "    self.read(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1506, in read\n",
      "    self._find_eof_marker(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1570, in _find_eof_marker\n",
      "    raise PdfReadError(\"EOF marker not found\")\n",
      "pypdf.errors.PdfReadError: EOF marker not found\n",
      "scaned /home/laisky/data/langchain/pdf/security/Trojan Source - Invisible Vulnerabilities.pdf\n",
      "[2023-07-24 03:06:36,519 - ERROR - /tmp/ipykernel_3350828/996360124.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/PKI/RFC2986_Certification Request Syntax Specification.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3350828/996360124.py\", line 39, in embedding_pdfs\n",
      "    for page, data in enumerate(loader.load_and_split()):\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 43, in load_and_split\n",
      "    docs = self.load()\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 118, in load\n",
      "    return list(self.lazy_load())\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 125, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 95, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/parsers/pdf.py\", line 20, in lazy_parse\n",
      "    pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 322, in __init__\n",
      "    self.read(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1506, in read\n",
      "    self._find_eof_marker(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1570, in _find_eof_marker\n",
      "    raise PdfReadError(\"EOF marker not found\")\n",
      "pypdf.errors.PdfReadError: EOF marker not found\n",
      "scaned /home/laisky/data/langchain/pdf/security/PKI/rfc6960 - X.509 Internet Public Key Infrastructure Online Certificate Status Protocol - OCSP.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 803, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaned /home/laisky/data/langchain/pdf/security/TEE/Honeycomb- Secure and Efficient GPU Executions via Static Validation.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/Security Protocol and Data Model (SPDM) Specification.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/Secured Messages using SPDM Specification.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/Integrity and Data Encryption (IDE) ECN Deep Dive.pdf\n",
      "save store to /home/laisky/data/langchain/index-azure/security\n",
      "scanned 6 files\n",
      "[2023-07-24 03:18:45,225 - ERROR - /tmp/ipykernel_3350828/996360124.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/RFC2986_Certification Request Syntax Specification.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3350828/996360124.py\", line 39, in embedding_pdfs\n",
      "    for page, data in enumerate(loader.load_and_split()):\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 43, in load_and_split\n",
      "    docs = self.load()\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 118, in load\n",
      "    return list(self.lazy_load())\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 125, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 95, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/parsers/pdf.py\", line 20, in lazy_parse\n",
      "    pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 322, in __init__\n",
      "    self.read(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1506, in read\n",
      "    self._find_eof_marker(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1570, in _find_eof_marker\n",
      "    raise PdfReadError(\"EOF marker not found\")\n",
      "pypdf.errors.PdfReadError: EOF marker not found\n",
      "[2023-07-24 03:18:45,229 - ERROR - /tmp/ipykernel_3350828/996360124.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/PKI/RFC2986_Certification Request Syntax Specification.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3350828/996360124.py\", line 39, in embedding_pdfs\n",
      "    for page, data in enumerate(loader.load_and_split()):\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 43, in load_and_split\n",
      "    docs = self.load()\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 118, in load\n",
      "    return list(self.lazy_load())\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 125, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 95, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/parsers/pdf.py\", line 20, in lazy_parse\n",
      "    pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 322, in __init__\n",
      "    self.read(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1506, in read\n",
      "    self._find_eof_marker(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1570, in _find_eof_marker\n",
      "    raise PdfReadError(\"EOF marker not found\")\n",
      "pypdf.errors.PdfReadError: EOF marker not found\n",
      "[2023-07-24 03:18:45,446 - ERROR - /tmp/ipykernel_3350828/996360124.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/å›½æ ‡/GB_T39204-2022 ä¿¡æ¯å®‰å…¨æŠ€æœ¯ å…³é”®ä¿¡æ¯åŸºç¡€è®¾æ–½å®‰å…¨ä¿æŠ¤è¦æ±‚.pdf\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3350828/996360124.py\", line 39, in embedding_pdfs\n",
      "    for page, data in enumerate(loader.load_and_split()):\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 43, in load_and_split\n",
      "    docs = self.load()\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 118, in load\n",
      "    return list(self.lazy_load())\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/pdf.py\", line 125, in lazy_load\n",
      "    yield from self.parser.parse(blob)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/base.py\", line 95, in parse\n",
      "    return list(self.lazy_parse(blob))\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/document_loaders/parsers/pdf.py\", line 20, in lazy_parse\n",
      "    pdf_reader = pypdf.PdfReader(pdf_file_obj, password=self.password)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 322, in __init__\n",
      "    self.read(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1506, in read\n",
      "    self._find_eof_marker(stream)\n",
      "  File \"/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/pypdf/_reader.py\", line 1570, in _find_eof_marker\n",
      "    raise PdfReadError(\"EOF marker not found\")\n",
      "pypdf.errors.PdfReadError: EOF marker not found\n",
      "save store to /home/laisky/data/langchain/index-azure/security\n",
      "scanned 6 files\n"
     ]
    }
   ],
   "source": [
    "# incremental scan pdfs\n",
    "# /home/laisky/data/langchain/pdf/security\n",
    "\n",
    "def gen_pdfs():\n",
    "    yield from glob.glob(f\"{pdf_dirpath}/**/*.pdf\", recursive=True)\n",
    "\n",
    "def run_scan_pdfs():\n",
    "#     index = new_store()\n",
    "#     save_store(\n",
    "#         index=index, \n",
    "#         dirpath=index_dirpath, \n",
    "#         name=name,\n",
    "#     )\n",
    "    \n",
    "    total = 0\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=index_dirpath,\n",
    "            name=name,\n",
    "        )\n",
    "        n = embedding_pdfs(\n",
    "            index=index,\n",
    "            fpaths=gen_pdfs(),\n",
    "            url=f\"https://s3.laisky.com/public/papers/{name}/\",\n",
    "            replace_by_url=f\"/home/laisky/data/langchain/pdf/{name}/\",\n",
    "        )\n",
    "        total += n\n",
    "        save_store(\n",
    "            index=index, \n",
    "            dirpath=index_dirpath, \n",
    "            name=name,\n",
    "        )\n",
    "        \n",
    "#         return\n",
    "        print(f\"scanned {total} files\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "run_scan_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83eae4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# ç”Ÿæˆç”¨äºŽé—®ç­”çš„ query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=index_dirpath,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    client=None,\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0, \n",
    "    max_tokens=2000,\n",
    "    streaming=False,\n",
    ")  \n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     deployment_name=azure_gpt_deploymentid,\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     max_tokens=2000,\n",
    "# )\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec62c4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤”ï¸: list tpm's features\n",
      "\n",
      "ðŸ“–: Some of the features of TPM (Trusted Platform Module) include:\n",
      "\n",
      "1. Validation of acceptable integrity metrics: By isolating processes, the set of acceptable platform configurations can be reduced to one operating system and one application only.\n",
      "\n",
      "2. TPM Components: The TPM can be implemented as an IC (Integrated Circuit) or in software. Most commercially available implementations are hardware-based.\n",
      "\n",
      "3. Enhanced functionality: TPM 2.0 has been enhanced from TPM 1.2 to support more platforms. This includes adding encryption algorithms, enhancing availability for applications, enhancing authentication features, simplifying TPM management, and adding features that enhance the security of platform services.\n",
      "\n",
      "4. Monotonic counters: TPM provides secure mechanisms, such as monotonic counters, to prevent replay attacks.\n",
      "\n",
      "5. Time-stamping: TPM provides the ability to measure time intervals, although absolute measurement of time is not possible.\n",
      "\n",
      "6. Audit trail management: TPM provides mechanisms to create and manage audit trails.\n",
      "\n",
      "7. Roots of Trust: A trusted platform has a root of trust, which serves as the foundation for other security services such as authenticated boot, secure storage, and attestation.\n",
      "\n",
      "8. Automotive Thin Profile: TCG (Trusted Computing Group) has formulated \"TPM 2.0 Automotive Thin Profile\" specifically for automotive applications. It includes features such as testing ECU (Electronic Control Unit) firmware and software integrity, management of encryption keys used by ECU, authentication and assurance of ECU integrity, secure update of ECU firmware, and protecting memory from write-back of information in ECU.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ask pdf embeddings\n",
    "# ====================================\n",
    "query = \"list tpm's features\"\n",
    "\n",
    "\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "response = chain.run(\n",
    "#     context=';'.join([d.page_content for d in related_docs]), \n",
    "    input_documents=related_docs,\n",
    "    question=query,\n",
    ")\n",
    "\n",
    "print(f\"ðŸ¤”ï¸: {query}\\n\")\n",
    "print(f\"ðŸ“–: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dbb792e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To measure the host OS in a virtual machine (VM) using a virtual Trusted Platform Module (vTPM), you can follow these steps:\n",
      "\n",
      "1. Ensure that your VM has a vTPM enabled. This can usually be done through the VM settings or configuration options provided by your virtualization software.\n",
      "\n",
      "2. Install the necessary software and drivers for the vTPM in both the host OS and the guest OS. This may involve installing specific TPM device drivers or modules.\n",
      "\n",
      "3. Once the vTPM is successfully emulated in the VM, you can access the TPM device in the guest OS. In the provided context, it mentions that a device named \"tpm0\" will be created under \"/sys/class/misc/\" in the guest OS.\n",
      "\n",
      "4. Use the appropriate TPM management tools or APIs in the guest OS to measure the host OS. These tools will typically provide functions to read and write to the Platform Configuration Registers (PCR) in the TPM.\n",
      "\n",
      "5. In the host OS, maintain a log that indicates what has been measured into each PCR register. This log can be used for trust-based decisions using the Intel TXT measurements.\n",
      "\n",
      "It's important to note that the specific steps and tools may vary depending on the virtualization software and TPM implementation you are using. It's recommended to consult the documentation or support resources provided by your virtualization software and TPM manufacturer for detailed instructions.\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# manually qa based on embedidngs step by step\n",
    "# ====================================\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "If you don't know the answer, or you think more information is needed to provide a better answer, \n",
    "just say in this strict format: \"I need more informations about: [list keywords that will be used to search more informations]\" to ask more informations, \n",
    "don't try to make up an answer.\n",
    "----------------\n",
    "context: {summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "def query_for_more_info(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    return \"; \".join([d.page_content for d in related_docs]) \n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "query = \"how to measure host os in vm by vtpm\"\n",
    "\n",
    "\n",
    "n = 0\n",
    "last_sub_query = \"\"\n",
    "regexp = re.compile(r'I need more information about \"([^\"]+)\"')\n",
    "while n<3: \n",
    "    n += 1\n",
    "    resp = chain.run({\n",
    "        \"summaries\": query_for_more_info(query),\n",
    "        \"question\": query,\n",
    "    })\n",
    "    matched = regexp.findall(resp)\n",
    "    if len(matched) == 0:\n",
    "        break\n",
    "        \n",
    "    sub_query = matched[0]\n",
    "    if sub_query == last_sub_query:\n",
    "        break\n",
    "    last_sub_query = sub_query\n",
    "    \n",
    "    print(f\"require more informations about: {sub_query}\")\n",
    "    query += f\"; {query_for_more_info(sub_query)}\"\n",
    "    \n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cabf2025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amber GPU-CC']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 'I need more information about \"Amber GPU-CC\" to provide an accurate answer. Could you please provide more context or clarify your question?'\n",
    "\n",
    "regexp = re.compile(r'I need more information about \"([^\\)]+)\"')\n",
    "\n",
    "regexp.findall(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "936b2204",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'input'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is tee-io\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     50\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     tools, llm, agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mOPENAI_FUNCTIONS, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m )\n\u001b[0;32m---> 55\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelated_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:445\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    441\u001b[0m         _output_key\n\u001b[1;32m    442\u001b[0m     ]\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    446\u001b[0m         _output_key\n\u001b[1;32m    447\u001b[0m     ]\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:220\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    187\u001b[0m     inputs: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    193\u001b[0m     include_run_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    194\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    222\u001b[0m         callbacks,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    229\u001b[0m     )\n\u001b[1;32m    230\u001b[0m     new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:374\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    372\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    373\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexternal_context)\n\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:132\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    130\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'input'}"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# use vectore store in functions(agents)\n",
    "# ====================================\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "\n",
    "\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "\n",
    "def query_for_agent(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join([d.page_content for d in related_docs])\n",
    "\n",
    "def context_for_agent(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "        \n",
    "    response = chain.run(\n",
    "        input_documents=related_docs,\n",
    "        question=query,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "    \n",
    "    \n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=query_for_agent,\n",
    "        description=\"useful for when you need to answer questions, this function takes a string as input and returns a string. This function is capable of vectorizing the input string and searching for similar information in a vector database. Your AI can call this function to retrieve the data it needs based on its requirements.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "query = \"what is tee-io\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "#     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True,\n",
    ")\n",
    "\n",
    "agent.run(\n",
    "    input_documents=related_docs,\n",
    "    question=query,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "81eab418",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
