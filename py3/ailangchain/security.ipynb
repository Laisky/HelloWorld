{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "537d6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN_ME\n",
    "\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# 通用的函数定义\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "\n",
    "        loader = PyPDFLoader(fpath)\n",
    "        for page, data in enumerate(loader.load_and_split()):\n",
    "            splits = text_splitter.split_text(data.page_content)\n",
    "            docs.extend(splits)\n",
    "            for ichunk, _ in enumerate(splits):\n",
    "                fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                furl = url + fnameurl\n",
    "                metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "\n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fpath):\n",
    "            continue\n",
    "\n",
    "        with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "            docus = markdown_splitter.create_documents([fp.read()])\n",
    "            for ichunk, docu in enumerate(docus):\n",
    "                docs.append(docu.page_content)\n",
    "                title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "                if url:\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "                else:\n",
    "                    metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    store = FAISS.from_texts([\"world\"], OpenAIEmbeddings(), metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f488aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaned /home/laisky/data/langchain/pdf/security/TEE/comparison_between_SEV_and_SGX.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/Confidential_High-Performance_Computing_in_the_Public_Cloud.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/cocoTPM.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 857, which is longer than the specified 500\n",
      "Created a chunk of size 986, which is longer than the specified 500\n",
      "Created a chunk of size 546, which is longer than the specified 500\n",
      "Created a chunk of size 549, which is longer than the specified 500\n",
      "Created a chunk of size 555, which is longer than the specified 500\n",
      "Created a chunk of size 538, which is longer than the specified 500\n",
      "Created a chunk of size 510, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaned /home/laisky/data/langchain/pdf/security/TEE/A survey of Intel SGX and its applications.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/Towards_A_Secure_Joint_Cloud_With_Confidential_Computing.pdf\n",
      "save store to /home/laisky/data/langchain/index/security\n",
      "scanned 5 files\n",
      "save store to /home/laisky/data/langchain/index/security\n",
      "scanned 5 files\n"
     ]
    }
   ],
   "source": [
    "# incremental scan pdfs\n",
    "\n",
    "def gen_pdfs():\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/pdf/security/**/*.pdf\", recursive=True)\n",
    "\n",
    "def run_scan_pdfs():\n",
    "#     index = new_store()\n",
    "    total = 0\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=\"/home/laisky/data/langchain/index\",\n",
    "            name=\"security\",\n",
    "        )\n",
    "        n = embedding_pdfs(\n",
    "            index=index,\n",
    "            fpaths=gen_pdfs(),\n",
    "            url=\"https://s3.laisky.com/public/papers/security/\",\n",
    "            replace_by_url=\"/home/laisky/data/langchain/pdf/security/\",\n",
    "        )\n",
    "        total += n\n",
    "        save_store(\n",
    "            index=index, \n",
    "            dirpath=\"/home/laisky/data/langchain/index\", \n",
    "            name=\"security\",\n",
    "        )\n",
    "        \n",
    "        print(f\"scanned {total} files\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "run_scan_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49778be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# incremental scan markdowns\n",
    "\n",
    "def gen_markdowns():\n",
    "    yield \"/home/laisky/data/langchain/basebit/doc/content/terms.md\"\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/basebit/doc/content/research/**/*.md\", recursive=True)\n",
    "    \n",
    "\n",
    "def run_scan_markdowns():\n",
    "#         index = new_store()\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=\"/home/laisky/data/langchain/index\",\n",
    "            name=\"security\",\n",
    "        )\n",
    "        files = gen_markdowns()\n",
    "        n = embedding_markdowns(\n",
    "            index=index,\n",
    "            fpaths=files,\n",
    "            url=\"https://s3.laisky.com/public/papers/security/\",\n",
    "            replace_by_url=\"/home/laisky/data/langchain/pdf/security/\",\n",
    "        )\n",
    "        save_store(\n",
    "            index=index,\n",
    "            dirpath=\"/home/laisky/data/langchain/index/\", \n",
    "            name=\"security\",\n",
    "        )\n",
    "        \n",
    "        print(f\"{n=}\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "        \n",
    "run_scan_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9c77ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 生成用于问答的 query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chains import VectorDBQAWithSourcesChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=\"/home/laisky/data/langchain/index\",\n",
    "    name=\"security\",\n",
    ")\n",
    "chain = VectorDBQAWithSourcesChain.from_llm(\n",
    "    llm=OpenAI(\n",
    "        temperature=0, \n",
    "        max_tokens=1000,\n",
    "        model_name=\"text-davinci-003\",\n",
    "        streaming=False,\n",
    "    ), \n",
    "#     retriever=VectorStoreRetriever(vectorstore=index.store, search_kwargs={\"filter\":{\"type\":\"filter\"},\"k\":3},),\n",
    "    vectorstore=index.store,\n",
    "    reduce_k_below_max_tokens=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ec62c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔️: 使用 HKDF 算法派生密钥时，作为派生根的 master key 的长度该设置为多少\n",
      "\n",
      "🤖️: The length of the master key used for deriving keys with the\n",
      "    HKDF algorithm should be at least 256 bits.\n",
      "\n",
      "📖: \n",
      "https://s3.laisky.com/public/papers/security/RFC8446_The%20Transport%20Layer%20Security%20%28TLS%29%20Protocol%20Version%201.3.pdf#page=145\n",
      "https://s3.laisky.com/public/papers/security/Computer%20Security%20Art%20And%20Science_Matt%20Bishop%2C%20Elisabeth%20Sullivan%20etc.pdf#page=448\n",
      "https://s3.laisky.com/public/papers/security/NIST_SP-800-131Ar2_Transitioning%20the%20Use%20of%20Cryptographic%20Algorithms%20and%20Key%20Lengths.pdf#page=22\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ask pdf embeddings\n",
    "# ====================================\n",
    "question = \"使用 HKDF 算法派生密钥时，作为派生根的 master key 的长度该设置为多少\"\n",
    "result = chain(\n",
    "    {\n",
    "        \"question\": question,\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "\n",
    "print(f\"🤔️: {question}\\n\")\n",
    "print(f\"🤖️: {pretty_print(result['answer'])}\\n\")\n",
    "print(f\"📖: {result['sources']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
