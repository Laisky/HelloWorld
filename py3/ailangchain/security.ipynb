{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "537d6951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embeddings\"].deployment_id\n",
    "azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "# os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "logger = setup_logger(\"security\")\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 定义文件路径\n",
    "# =============================\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# 通用的函数定义\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            loader = PyPDFLoader(fpath)\n",
    "            for page, data in enumerate(loader.load_and_split()):\n",
    "                splits = text_splitter.split_text(data.page_content)\n",
    "                docs.extend(splits)\n",
    "                for ichunk, _ in enumerate(splits):\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "        except Exception as err:\n",
    "            logger.error(f\"skip file {fpath}: {err}\")\n",
    "            continue\n",
    "\n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "    i = 0\n",
    "    docs = []\n",
    "    metadatas = []\n",
    "    for fpath in fpaths:\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fpath):\n",
    "            continue\n",
    "\n",
    "        with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "            docus = markdown_splitter.create_documents([fp.read()])\n",
    "            for ichunk, docu in enumerate(docus):\n",
    "                docs.append(docu.page_content)\n",
    "                title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "                if url:\n",
    "                    fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "                    furl = url + fnameurl\n",
    "                    metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "                else:\n",
    "                    metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "        index.scaned_files.add(fname)\n",
    "        print(f\"scaned {fpath}\")\n",
    "        i += 1\n",
    "        if i > N_BACTCH_FILES:\n",
    "            break\n",
    "\n",
    "    if i != 0:\n",
    "        index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "    return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    embedding_model = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        deployment=azure_embeddings_deploymentid,\n",
    "    )\n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f488aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-06-15 03:00:14,366 - ERROR - /tmp/ipykernel_567492/578690748.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/RFC2986_Certification Request Syntax Specification.pdf: EOF marker not found\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/Intel® TDX Connect TEE-IO Device Guide.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/PCIe Security Webinar_Aug 2020_PDF.pdf\n",
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/AMD SEV-TIO- Trusted I:O for Secure Encrypted Virtualization.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1415, which is longer than the specified 500\n",
      "Created a chunk of size 759, which is longer than the specified 500\n",
      "Created a chunk of size 921, which is longer than the specified 500\n",
      "Created a chunk of size 526, which is longer than the specified 500\n",
      "Created a chunk of size 1661, which is longer than the specified 500\n",
      "Created a chunk of size 1349, which is longer than the specified 500\n",
      "Created a chunk of size 960, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaned /home/laisky/data/langchain/pdf/security/TEE/TIO/Software Enabling for Intel® TDX in Support of TEE-I:O.pdf\n",
      "[2023-06-15 03:00:19,088 - ERROR - /tmp/ipykernel_567492/578690748.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/国标/GB_T39204-2022 信息安全技术 关键信息基础设施安全保护要求.pdf: EOF marker not found\n",
      "save store to /home/laisky/data/langchain/index-azure/security\n",
      "scanned 4 files\n",
      "[2023-06-15 03:07:22,269 - ERROR - /tmp/ipykernel_567492/578690748.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/RFC2986_Certification Request Syntax Specification.pdf: EOF marker not found\n",
      "[2023-06-15 03:07:22,485 - ERROR - /tmp/ipykernel_567492/578690748.py:47 - security] - skip file /home/laisky/data/langchain/pdf/security/国标/GB_T39204-2022 信息安全技术 关键信息基础设施安全保护要求.pdf: EOF marker not found\n",
      "save store to /home/laisky/data/langchain/index-azure/security\n",
      "scanned 4 files\n"
     ]
    }
   ],
   "source": [
    "# incremental scan pdfs\n",
    "\n",
    "def gen_pdfs():\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/pdf/security/**/*.pdf\", recursive=True)\n",
    "\n",
    "def run_scan_pdfs():\n",
    "#     index = new_store()\n",
    "#     save_store(\n",
    "#         index=index, \n",
    "#         dirpath=index_dirpath, \n",
    "#         name=\"security\",\n",
    "#     )\n",
    "    \n",
    "    total = 0\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=index_dirpath,\n",
    "            name=\"security\",\n",
    "        )\n",
    "        n = embedding_pdfs(\n",
    "            index=index,\n",
    "            fpaths=gen_pdfs(),\n",
    "            url=\"https://s3.laisky.com/public/papers/security/\",\n",
    "            replace_by_url=\"/home/laisky/data/langchain/pdf/security/\",\n",
    "        )\n",
    "        total += n\n",
    "        save_store(\n",
    "            index=index, \n",
    "            dirpath=index_dirpath, \n",
    "            name=\"security\",\n",
    "        )\n",
    "        \n",
    "#         return\n",
    "        print(f\"scanned {total} files\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "run_scan_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49778be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# incremental scan markdowns\n",
    "\n",
    "def gen_markdowns():\n",
    "    yield \"/home/laisky/data/langchain/basebit/doc/content/terms.md\"\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/basebit/doc/content/research/**/*.md\", recursive=True)\n",
    "    \n",
    "\n",
    "def run_scan_markdowns():\n",
    "#         index = new_store()\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=\"/home/laisky/data/langchain/index\",\n",
    "            name=\"security\",\n",
    "        )\n",
    "        files = gen_markdowns()\n",
    "        n = embedding_markdowns(\n",
    "            index=index,\n",
    "            fpaths=files,\n",
    "            url=\"https://s3.laisky.com/public/papers/security/\",\n",
    "            replace_by_url=\"/home/laisky/data/langchain/pdf/security/\",\n",
    "        )\n",
    "        save_store(\n",
    "            index=index,\n",
    "            dirpath=\"/home/laisky/data/langchain/index/\", \n",
    "            name=\"security\",\n",
    "        )\n",
    "        \n",
    "        print(f\"{n=}\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "        \n",
    "run_scan_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b9ee193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# merge FAISS index\n",
    "# ====================================\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def merge_index(indexs: List[Index]) -> Index:\n",
    "    new_index = new_store()\n",
    "    \n",
    "    for coming_idx in indexs:\n",
    "        new_ids = faiss.IDSelectorRange(0, new_index.store.ntotal)\n",
    "        coming_ids = faiss.IDSelectorRange(0, coming_idx.ntotal)\n",
    "        _, dups = faiss.bitset_remove_pairwise_and_reorder(new_ids, coming_ids)\n",
    "        new_index = faiss.reconstruct_from_subset(new_index, dups)\n",
    "\n",
    "        new_index.store.merge_from(coming_idx.store)\n",
    "        new_index.scaned_files.add(coming_idx.scaned_files)\n",
    "        \n",
    "    return new_index\n",
    "        \n",
    "\n",
    "new_index = new_store()\n",
    "old_index = load_store(\n",
    "    dirpath=\"/home/laisky/data/langchain/index\",\n",
    "    name=\"security\",\n",
    ")\n",
    "      \n",
    "    \n",
    "# merge_index([new_index, old_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb1b7923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170563\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "new_index.store.merge_from(old_index.store)\n",
    "print(new_index.store.index.ntotal)\n",
    "print(old_index.store.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83eae4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TPM (Trusted Platform Module) provides several security features, including:\n",
      "- Support for bulk (symmetric) encryption in the platform\n",
      "- High-quality random numbers\n",
      "- Cryptographic services\n",
      "- A protected persistent store for small amounts of data, sticky-bits, monotonic counters and extendable registers\n",
      "- A protected pseudo-persistent store for unlimited amounts of keys and data\n",
      "- An extensive choice of authorization methods\n",
      "- Provision of monotonic counters that can provide a secure mechanism to prevent replay attacks\n",
      "- Provision of time-stamping\n",
      "- Algorithm agility, the ability to implement new cryptographic algorithms as needed\n",
      "- Support for multiple TPM instances\n",
      "- Feature API to make the most-used facilities of the TPM 2.0 easily available to programmers\n",
      "- User-defined indexes that can hold unstructured data with various read and write locks\n",
      "- Attestation, which allows the TPM to provide a fundamental set of security features that have been defined by the TCG, including protected capabilities, integrity measurement, and integrity reporting.\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# 生成用于问答的 query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "# index = load_store(\n",
    "#     dirpath=index_dirpath,\n",
    "#     name=\"security\",\n",
    "# )\n",
    "index = new_index\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name=\"gpt-3.5-turbo\", \n",
    "#     temperature=0, \n",
    "#     max_tokens=1000)  \n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=azure_gpt_deploymentid,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "# chain = VectorDBQAWithSourcesChain.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     vectorstore=index.store,\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs=chain_type_kwargs,\n",
    "#     reduce_k_below_max_tokens=True,\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     chain_type=\"stuff\",\n",
    "#     retriever=index.store.as_retriever(),\n",
    "#     return_source_documents=True,\n",
    "#     chain_type_kwargs=chain_type_kwargs,\n",
    "#     reduce_k_below_max_tokens=True,\n",
    "# )\n",
    "\n",
    "\n",
    "query = \"list tpm's features\"\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=query,\n",
    "    k=10,\n",
    ")\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "response = chain.run(\n",
    "    input_documents=related_docs, \n",
    "    question=query,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec62c4ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'input_documents'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ====================================\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# ask pdf embeddings\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ====================================\u001b[39;00m\n\u001b[1;32m      4\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow virtual tpm help measured guest system boot, explain in step by step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤔️: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖️: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretty_print(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:123\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     inputs: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[1;32m    109\u001b[0m     return_only_outputs: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    110\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124;03m\"\"\"Run the logic of this chain and add to output if desired.\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    125\u001b[0m         callbacks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    127\u001b[0m     new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:216\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    215\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexternal_context)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/lib/python3.9/site-packages/langchain/chains/base.py:83\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     81\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[0;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'input_documents'}"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ask pdf embeddings\n",
    "# ====================================\n",
    "question = \"how virtual tpm help measured guest system boot, explain in step by step\"\n",
    "result = chain(\n",
    "    {\n",
    "        \"question\": question,\n",
    "    },\n",
    "    return_only_outputs=True,\n",
    ")\n",
    "\n",
    "print(f\"🤔️: {question}\\n\")\n",
    "print(f\"🤖️: {pretty_print(result['answer'])}\\n\")\n",
    "print(f\"📖: {result['sources']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
