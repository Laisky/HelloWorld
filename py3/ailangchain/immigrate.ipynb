{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537d6951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.25) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "# os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "# os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "# os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "# os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "# openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "# openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "# openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embeddings\"].deployment_id\n",
    "# azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "os.environ[\"OPENAI_API_BASE\"] = prd.OPENAI_API\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 定义文件路径\n",
    "# =============================\n",
    "name = \"immigrate\"\n",
    "logger = setup_logger(name)\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure\"\n",
    "pdf_dirpath = f\"/home/laisky/data/langchain/pdf/{name}\"\n",
    "\n",
    "for path in [index_dirpath, pdf_dirpath]:\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 process workers and 20 thread workers\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# 通用的函数定义\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from ramjet.tasks.gptchat.llm.embeddings import reset_eof_of_pdf\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "# def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "#     i = 0\n",
    "#     docs = []\n",
    "#     metadatas = []\n",
    "#     for fpath in fpaths:\n",
    "#         fname = os.path.split(fpath)[1]\n",
    "#         if is_file_scaned(index, fname):\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             reset_eof_of_pdf(fpath)\n",
    "#             loader = PyPDFLoader(fpath)\n",
    "#             for page, data in enumerate(loader.load_and_split()):\n",
    "#                 splits = text_splitter.split_text(data.page_content)\n",
    "#                 docs.extend(splits)\n",
    "#                 for ichunk, _ in enumerate(splits):\n",
    "#                     fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "#                     furl = url + fnameurl\n",
    "#                     metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "#         except Exception:\n",
    "#             logger.exception(f\"skip file {fpath}\")\n",
    "#             continue\n",
    "\n",
    "#         index.scaned_files.add(fname)\n",
    "#         print(f\"scaned {fpath}\")\n",
    "#         i += 1\n",
    "#         if i > N_BACTCH_FILES:\n",
    "#             break\n",
    "\n",
    "#     if i != 0:\n",
    "#         # fix stupid compatability issue in langchain faiss\n",
    "#         if not getattr(index.store, \"_normalize_L2\", None):\n",
    "#             index.store._normalize_L2 = False\n",
    "            \n",
    "#         index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "#     return i\n",
    "\n",
    "\n",
    "# def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "#     i = 0\n",
    "#     docs = []\n",
    "#     metadatas = []\n",
    "#     for fpath in fpaths:\n",
    "#         fname = os.path.split(fpath)[1]\n",
    "#         if is_file_scaned(index, fpath):\n",
    "#             continue\n",
    "\n",
    "#         with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "#             docus = markdown_splitter.create_documents([fp.read()])\n",
    "#             for ichunk, docu in enumerate(docus):\n",
    "#                 docs.append(docu.page_content)\n",
    "#                 title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "#                 if url:\n",
    "#                     fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "#                     furl = url + fnameurl\n",
    "#                     metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "#                 else:\n",
    "#                     metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "#         index.scaned_files.add(fname)\n",
    "#         print(f\"scaned {fpath}\")\n",
    "#         i += 1\n",
    "#         if i > N_BACTCH_FILES:\n",
    "#             break\n",
    "\n",
    "#     if i != 0:\n",
    "#         index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "#     return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"OPENAI_API_TYPE\") == \"azure\":\n",
    "        azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\n",
    "            \"embeddings\"\n",
    "        ].deployment_id\n",
    "        # azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            deployment=azure_embeddings_deploymentid,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "    \n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "        \n",
    "    # compatable with azure/openai embeddings\n",
    "    store.embedding_function = embedding_model.embed_query\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    if os.environ.get(\"OPENAI_API_TYPE\") == \"azure\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            deployment=azure_embeddings_deploymentid,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "        \n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f488aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save store to /home/laisky/data/langchain/index-azure/immigrate\n",
      "[2023-09-18 06:27:25,451 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Welcome to Canada What you should know.pdf'\n",
      "[2023-09-18 06:27:36,704 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Documents for Express Entry.pdf'\n",
      "[2023-09-18 06:27:39,485 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Eligibility to apply for the Canadian Experience Class (Express Entry).pdf'\n",
      "[2023-09-18 06:27:43,573 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/immigration_and_refugee_law.pdf'\n",
      "[2023-09-18 06:27:52,255 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Fees for Express Entry.pdf'\n",
      "[2023-09-18 06:27:54,436 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Submit your Express Entry application.pdf'\n",
      "[2023-09-18 06:27:57,145 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Express Entry- What prospective candidates need to know.pdf'\n",
      "[2023-09-18 06:28:00,407 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/ICCRC-Exam-Preparation-Manual.pdf'\n",
      "[2023-09-18 06:28:05,961 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/阿尔伯塔省机遇指南.pdf'\n",
      "[2023-09-18 06:28:10,985 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Consolidated Practice Guidelines for Citizenship, Immigration, and Refugee Protection Proceedings.pdf'\n",
      "[2023-09-18 06:28:15,459 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Immigration and Refugee Protection Act.pdf'\n",
      "[2023-09-18 06:28:25,721 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Express Entry.pdf'\n",
      "[2023-09-18 06:28:27,913 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Recruiting for success - Challenges for Canada’s Labour Migration System.pdf'\n",
      "[2023-09-18 06:28:32,807 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Eligibility to apply as a Federal Skilled Worker (Express Entry).pdf'\n",
      "[2023-09-18 06:28:36,878 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Benefits, credits, and taxes for newcomers.pdf'\n",
      "[2023-09-18 06:28:40,866 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/An Ultimate Guide About A Regulated Canadian Immigration Consultant (RCIC) -.pdf'\n",
      "[2023-09-18 06:28:48,072 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Information for Skilled Foreign Workers.pdf'\n",
      "[2023-09-18 06:28:52,134 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Express Entry- Check your application status.pdf'\n",
      "[2023-09-18 06:28:55,127 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/the comprehensive ICCRC entry-to-practice exam preparation manual.pdf'\n",
      "[2023-09-18 06:29:00,464 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Eligibility to apply for the Federal Skilled Trades Program (Express Entry).pdf'\n",
      "[2023-09-18 06:29:04,629 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- After you apply through Express Entry.pdf'\n",
      "[2023-09-18 06:29:08,744 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/stragegy to expand transitions to permanent residency.pdf'\n",
      "[2023-09-18 06:29:15,812 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/canadian_immigration_handbook.pdf'\n",
      "save store to /home/laisky/data/langchain/index-azure/immigrate\n",
      "scanned 23 files\n"
     ]
    }
   ],
   "source": [
    "# incremental scan pdfs\n",
    "# /home/laisky/data/langchain/pdf/security\n",
    "\n",
    "from ramjet.tasks.gptchat.llm.embeddings import _embedding_pdf\n",
    "\n",
    "def gen_pdfs():\n",
    "    yield from glob.glob(f\"{pdf_dirpath}/**/*.pdf\", recursive=True)\n",
    "\n",
    "def run_scan_pdfs():\n",
    "#     index = new_store()\n",
    "#     save_store(\n",
    "#         index=index, \n",
    "#         dirpath=index_dirpath, \n",
    "#         name=name,\n",
    "#     )\n",
    "\n",
    "    index = load_store(\n",
    "        dirpath=index_dirpath,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    total = 0\n",
    "    for fpath in gen_pdfs():\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "            \n",
    "        metadata_name = f\"https://s3.laisky.com/public/papers/{fpath.removeprefix('/home/laisky/data/langchain/pdf/')}\"\n",
    "        logger.info(f\"scan pdf {metadata_name=}\")\n",
    "#         continue\n",
    "    \n",
    "        try:\n",
    "            file_index = _embedding_pdf(\n",
    "                fpath=fpath,\n",
    "                metadata_name=metadata_name,\n",
    "                apikey=os.environ['OPENAI_API_KEY'],\n",
    "                max_chunks=1000,\n",
    "            )\n",
    "            index.store.merge_from(file_index.store)\n",
    "            total += 1\n",
    "        except Exception:\n",
    "            logger.exception(f\"failed parse file {fpath=}\")\n",
    "\n",
    "    save_store(\n",
    "        index=index, \n",
    "        dirpath=index_dirpath, \n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    print(f\"scanned {total} files\")\n",
    "        \n",
    "run_scan_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49778be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# incremental scan markdowns\n",
    "\n",
    "def gen_markdowns():\n",
    "    yield \"/home/laisky/data/langchain/basebit/doc/content/terms.md\"\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/basebit/doc/content/research/**/*.md\", recursive=True)\n",
    "    \n",
    "\n",
    "def run_scan_markdowns():\n",
    "#         index = new_store()\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=\"/home/laisky/data/langchain/index\",\n",
    "            name=name,\n",
    "        )\n",
    "        files = gen_markdowns()\n",
    "        n = embedding_markdowns(\n",
    "            index=index,\n",
    "            fpaths=files,\n",
    "            url=f\"https://s3.laisky.com/public/papers/{name}/\",\n",
    "            replace_by_url=f\"/home/laisky/data/langchain/pdf/{name}/\",\n",
    "        )\n",
    "        save_store(\n",
    "            index=index,\n",
    "            dirpath=\"/home/laisky/data/langchain/index/\", \n",
    "            name=name,\n",
    "        )\n",
    "        \n",
    "        print(f\"{n=}\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "        \n",
    "run_scan_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83eae4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# 生成用于问答的 query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=index_dirpath,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    client=None,\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0, \n",
    "    max_tokens=2000,\n",
    "    streaming=False,\n",
    ")  \n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     deployment_name=azure_gpt_deploymentid,\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     max_tokens=2000,\n",
    "# )\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec62c4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤔️: 提交 FSW 前需要准备什么考试\n",
      "\n",
      "📖: 根据提供的上下文，没有明确提到提交FSW（Federal Skilled Worker）申请前需要参加任何考试。然而，根据加拿大移民规定，申请FSW需要满足一些基本要求，包括语言能力测试（如IELTS或CELPIP）和教育背景评估（如学历认证）。此外，根据个人情况，可能还需要通过其他专业资格考试或技能评估。建议您查阅加拿大移民部官方网站或咨询专业移民顾问以获取最准确和最新的信息。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ask pdf embeddings\n",
    "# ====================================\n",
    "query = \"提交 FSW 前需要准备什么考试\"\n",
    "\n",
    "\n",
    "translated = llm.predict(f\"\"\"I want you to act as an English translator, spelling corrector and improver. \n",
    "I will speak to you in any language and you will detect the language, \n",
    "translate it and answer in the corrected and improved version of my text, in English. \n",
    "I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, \n",
    "upper level English words and sentences. Keep the meaning same, but make them more literary. \n",
    "I want you to only reply the correction, the improvements and nothing else, do not write explanations. \n",
    "following is the sentence that need to be translate:\n",
    "---\n",
    "{query}\"\"\")\n",
    "\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=translated,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "response = chain.run(\n",
    "#     context=';'.join([d.page_content for d in related_docs]), \n",
    "    input_documents=related_docs,\n",
    "    question=query,\n",
    ")\n",
    "\n",
    "print(f\"🤔️: {query}\\n\")\n",
    "print(f\"📖: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# manually qa based on embedidngs step by step\n",
    "# ====================================\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "If you don't know the answer, or you think more information is needed to provide a better answer, \n",
    "just say in this strict format: \"I need more informations about: [list keywords that will be used to search more informations]\" to ask more informations, \n",
    "don't try to make up an answer.\n",
    "----------------\n",
    "context: {summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "def query_for_more_info(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    return \"; \".join([d.page_content for d in related_docs]) \n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "query = \"how to measure host os in vm by vtpm\"\n",
    "\n",
    "\n",
    "n = 0\n",
    "last_sub_query = \"\"\n",
    "regexp = re.compile(r'I need more information about \"([^\"]+)\"')\n",
    "while n<3: \n",
    "    n += 1\n",
    "    resp = chain.run({\n",
    "        \"summaries\": query_for_more_info(query),\n",
    "        \"question\": query,\n",
    "    })\n",
    "    matched = regexp.findall(resp)\n",
    "    if len(matched) == 0:\n",
    "        break\n",
    "        \n",
    "    sub_query = matched[0]\n",
    "    if sub_query == last_sub_query:\n",
    "        break\n",
    "    last_sub_query = sub_query\n",
    "    \n",
    "    print(f\"require more informations about: {sub_query}\")\n",
    "    query += f\"; {query_for_more_info(sub_query)}\"\n",
    "    \n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 'I need more information about \"Amber GPU-CC\" to provide an accurate answer. Could you please provide more context or clarify your question?'\n",
    "\n",
    "regexp = re.compile(r'I need more information about \"([^\\)]+)\"')\n",
    "\n",
    "regexp.findall(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# use vectore store in functions(agents)\n",
    "# ====================================\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "\n",
    "\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "\n",
    "def query_for_agent(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join([d.page_content for d in related_docs])\n",
    "\n",
    "def context_for_agent(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "        \n",
    "    response = chain.run(\n",
    "        input_documents=related_docs,\n",
    "        question=query,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "    \n",
    "    \n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=query_for_agent,\n",
    "        description=\"useful for when you need to answer questions, this function takes a string as input and returns a string. This function is capable of vectorizing the input string and searching for similar information in a vector database. Your AI can call this function to retrieve the data it needs based on its requirements.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "query = \"what is tee-io\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "#     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True,\n",
    ")\n",
    "\n",
    "agent.run(\n",
    "    input_documents=related_docs,\n",
    "    question=query,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eab418",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
