{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537d6951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laisky/.pyenv/versions/3.9.7/lib/python3.9/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.6.25) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import openai\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from kipp.utils import setup_logger\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Azure\n",
    "# ----------------------------------------------\n",
    "# os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "# os.environ['OPENAI_API_VERSION'] = prd.OPENAI_AZURE_VERSION\n",
    "# os.environ['OPENAI_API_BASE'] = prd.OPENAI_AZURE_API\n",
    "# os.environ['OPENAI_API_KEY'] = prd.OPENAI_AZURE_TOKEN\n",
    "\n",
    "# openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "# openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "# openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "# openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "# azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"embeddings\"].deployment_id\n",
    "# azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "# ----------------------------------------------\n",
    "\n",
    "# ----------------------------------------------\n",
    "# OpenAI\n",
    "# ----------------------------------------------\n",
    "os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN\n",
    "os.environ[\"OPENAI_API_BASE\"] = prd.OPENAI_API\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"]\n",
    "# ----------------------------------------------\n",
    "\n",
    "Index = namedtuple(\"index\", [\"store\", \"scaned_files\"])\n",
    "\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# å®šä¹‰æ–‡ä»¶è·¯å¾„\n",
    "# =============================\n",
    "name = \"immigrate\"\n",
    "logger = setup_logger(name)\n",
    "\n",
    "index_dirpath = \"/home/laisky/data/langchain/index-azure\"\n",
    "pdf_dirpath = f\"/home/laisky/data/langchain/pdf/{name}\"\n",
    "\n",
    "for path in [index_dirpath, pdf_dirpath]:\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e68c21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 process workers and 20 thread workers\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# prepare pdf documents docs.index & docs.store\n",
    "#\n",
    "# https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#retain-elements\n",
    "#\n",
    "# é€šç”¨çš„å‡½æ•°å®šä¹‰\n",
    "# ==============================================================\n",
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from ramjet.tasks.gptchat.llm.embeddings import reset_eof_of_pdf\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, separator=\"\\n\")\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "N_BACTCH_FILES = 5\n",
    "\n",
    "\n",
    "def is_file_scaned(index: Index, fpath):\n",
    "    return os.path.split(fpath)[1] in index.scaned_files\n",
    "\n",
    "\n",
    "# def embedding_pdfs(index: Index, fpaths, url, replace_by_url):\n",
    "#     i = 0\n",
    "#     docs = []\n",
    "#     metadatas = []\n",
    "#     for fpath in fpaths:\n",
    "#         fname = os.path.split(fpath)[1]\n",
    "#         if is_file_scaned(index, fname):\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             reset_eof_of_pdf(fpath)\n",
    "#             loader = PyPDFLoader(fpath)\n",
    "#             for page, data in enumerate(loader.load_and_split()):\n",
    "#                 splits = text_splitter.split_text(data.page_content)\n",
    "#                 docs.extend(splits)\n",
    "#                 for ichunk, _ in enumerate(splits):\n",
    "#                     fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "#                     furl = url + fnameurl\n",
    "#                     metadatas.append({\"source\": f\"{furl}#page={page+1}\"})\n",
    "#         except Exception:\n",
    "#             logger.exception(f\"skip file {fpath}\")\n",
    "#             continue\n",
    "\n",
    "#         index.scaned_files.add(fname)\n",
    "#         print(f\"scaned {fpath}\")\n",
    "#         i += 1\n",
    "#         if i > N_BACTCH_FILES:\n",
    "#             break\n",
    "\n",
    "#     if i != 0:\n",
    "#         # fix stupid compatability issue in langchain faiss\n",
    "#         if not getattr(index.store, \"_normalize_L2\", None):\n",
    "#             index.store._normalize_L2 = False\n",
    "            \n",
    "#         index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "#     return i\n",
    "\n",
    "\n",
    "# def embedding_markdowns(index: Index, fpaths, url, replace_by_url):\n",
    "#     i = 0\n",
    "#     docs = []\n",
    "#     metadatas = []\n",
    "#     for fpath in fpaths:\n",
    "#         fname = os.path.split(fpath)[1]\n",
    "#         if is_file_scaned(index, fpath):\n",
    "#             continue\n",
    "\n",
    "#         with codecs.open(fpath, \"rb\", \"utf8\") as fp:\n",
    "#             docus = markdown_splitter.create_documents([fp.read()])\n",
    "#             for ichunk, docu in enumerate(docus):\n",
    "#                 docs.append(docu.page_content)\n",
    "#                 title = quote(docu.page_content.strip().split(\"\\n\", maxsplit=1)[0])\n",
    "#                 if url:\n",
    "#                     fnameurl = quote(fpath.removeprefix(replace_by_url), safe=\"\")\n",
    "#                     furl = url + fnameurl\n",
    "#                     metadatas.append({\"source\": f\"{furl}#{title}\"})\n",
    "#                 else:\n",
    "#                     metadatas.append({\"source\": f\"{fname}#{title}\"})\n",
    "                    \n",
    "#         index.scaned_files.add(fname)\n",
    "#         print(f\"scaned {fpath}\")\n",
    "#         i += 1\n",
    "#         if i > N_BACTCH_FILES:\n",
    "#             break\n",
    "\n",
    "#     if i != 0:\n",
    "#         index.store.add_texts(docs, metadatas=metadatas)\n",
    "\n",
    "#     return i\n",
    "\n",
    "\n",
    "def load_store(dirpath, name) -> Index:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dirpath: dirpath to store index files\n",
    "        name: project/file name\n",
    "    \"\"\"\n",
    "    if os.environ.get(\"OPENAI_API_TYPE\") == \"azure\":\n",
    "        azure_embeddings_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\n",
    "            \"embeddings\"\n",
    "        ].deployment_id\n",
    "        # azure_gpt_deploymentid = prd.OPENAI_AZURE_DEPLOYMENTS[\"chat\"].deployment_id\n",
    "\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            deployment=azure_embeddings_deploymentid,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "    \n",
    "    index = faiss.read_index(f\"{os.path.join(dirpath, name)}.index\")\n",
    "    with open(f\"{os.path.join(dirpath, name)}.store\", \"rb\") as f:\n",
    "        store = pickle.load(f)\n",
    "    store.index = index\n",
    "\n",
    "    with open(f\"{os.path.join(dirpath, name)}.scanedfile\", \"rb\") as f:\n",
    "        scaned_files = pickle.load(f)\n",
    "        \n",
    "    # compatable with azure/openai embeddings\n",
    "    store.embedding_function = embedding_model.embed_query\n",
    "\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=scaned_files,\n",
    "    )\n",
    "\n",
    "\n",
    "def new_store() -> Index:\n",
    "    if os.environ.get(\"OPENAI_API_TYPE\") == \"azure\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            deployment=azure_embeddings_deploymentid,\n",
    "        )\n",
    "    else:\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            client=None,\n",
    "            model=\"text-embedding-ada-002\",\n",
    "        )\n",
    "        \n",
    "    store = FAISS.from_texts([\"world\"], embedding_model, metadatas=[{\"source\": \"hello\"}])\n",
    "    return Index(\n",
    "        store=store,\n",
    "        scaned_files=set([]),\n",
    "    )\n",
    "\n",
    "\n",
    "def save_store(index: Index, dirpath, name):\n",
    "    store_index = index.store.index\n",
    "    fpath_prefix = os.path.join(dirpath, name)\n",
    "    print(f\"save store to {fpath_prefix}\")\n",
    "    faiss.write_index(store_index, f\"{fpath_prefix}.index\")\n",
    "    index.store.index = None\n",
    "    with open(f\"{fpath_prefix}.store\", \"wb\") as f:\n",
    "        pickle.dump(index.store, f)\n",
    "    index.store.index = store_index\n",
    "\n",
    "    with open(f\"{fpath_prefix}.scanedfile\", \"wb\") as f:\n",
    "        pickle.dump(index.scaned_files, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f488aba2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save store to /home/laisky/data/langchain/index-azure/immigrate\n",
      "[2023-09-18 06:27:25,451 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Welcome to Canada What you should know.pdf'\n",
      "[2023-09-18 06:27:36,704 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Documents for Express Entry.pdf'\n",
      "[2023-09-18 06:27:39,485 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Eligibility to apply for the Canadian Experience Class (Express Entry).pdf'\n",
      "[2023-09-18 06:27:43,573 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/immigration_and_refugee_law.pdf'\n",
      "[2023-09-18 06:27:52,255 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Fees for Express Entry.pdf'\n",
      "[2023-09-18 06:27:54,436 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Submit your Express Entry application.pdf'\n",
      "[2023-09-18 06:27:57,145 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Express Entry- What prospective candidates need to know.pdf'\n",
      "[2023-09-18 06:28:00,407 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/ICCRC-Exam-Preparation-Manual.pdf'\n",
      "[2023-09-18 06:28:05,961 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/é˜¿å°”ä¼¯å¡”çœæœºé‡æŒ‡å—.pdf'\n",
      "[2023-09-18 06:28:10,985 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Consolidated Practice Guidelines for Citizenship, Immigration, and Refugee Protection Proceedings.pdf'\n",
      "[2023-09-18 06:28:15,459 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Immigration and Refugee Protection Act.pdf'\n",
      "[2023-09-18 06:28:25,721 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- Express Entry.pdf'\n",
      "[2023-09-18 06:28:27,913 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Recruiting for success - Challenges for Canadaâ€™s Labour Migration System.pdf'\n",
      "[2023-09-18 06:28:32,807 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Eligibility to apply as a Federal Skilled Worker (Express Entry).pdf'\n",
      "[2023-09-18 06:28:36,878 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Benefits, credits, and taxes for newcomers.pdf'\n",
      "[2023-09-18 06:28:40,866 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/An Ultimate Guide About A Regulated Canadian Immigration Consultant (RCIC) -.pdf'\n",
      "[2023-09-18 06:28:48,072 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Information for Skilled Foreign Workers.pdf'\n",
      "[2023-09-18 06:28:52,134 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Express Entry- Check your application status.pdf'\n",
      "[2023-09-18 06:28:55,127 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/the comprehensive ICCRC entry-to-practice exam preparation manual.pdf'\n",
      "[2023-09-18 06:29:00,464 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Eligibility to apply for the Federal Skilled Trades Program (Express Entry).pdf'\n",
      "[2023-09-18 06:29:04,629 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/Apply for permanent residence- After you apply through Express Entry.pdf'\n",
      "[2023-09-18 06:29:08,744 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/stragegy to expand transitions to permanent residency.pdf'\n",
      "[2023-09-18 06:29:15,812 - INFO - /tmp/ipykernel_2399029/2716387378.py:29 - immigrate] - scan pdf metadata_name='https://s3.laisky.com/public/papers/immigrate/canada/canadian_immigration_handbook.pdf'\n",
      "save store to /home/laisky/data/langchain/index-azure/immigrate\n",
      "scanned 23 files\n"
     ]
    }
   ],
   "source": [
    "# incremental scan pdfs\n",
    "# /home/laisky/data/langchain/pdf/security\n",
    "\n",
    "from ramjet.tasks.gptchat.llm.embeddings import _embedding_pdf\n",
    "\n",
    "def gen_pdfs():\n",
    "    yield from glob.glob(f\"{pdf_dirpath}/**/*.pdf\", recursive=True)\n",
    "\n",
    "def run_scan_pdfs():\n",
    "#     index = new_store()\n",
    "#     save_store(\n",
    "#         index=index, \n",
    "#         dirpath=index_dirpath, \n",
    "#         name=name,\n",
    "#     )\n",
    "\n",
    "    index = load_store(\n",
    "        dirpath=index_dirpath,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    total = 0\n",
    "    for fpath in gen_pdfs():\n",
    "        fname = os.path.split(fpath)[1]\n",
    "        if is_file_scaned(index, fname):\n",
    "            continue\n",
    "            \n",
    "        metadata_name = f\"https://s3.laisky.com/public/papers/{fpath.removeprefix('/home/laisky/data/langchain/pdf/')}\"\n",
    "        logger.info(f\"scan pdf {metadata_name=}\")\n",
    "#         continue\n",
    "    \n",
    "        try:\n",
    "            file_index = _embedding_pdf(\n",
    "                fpath=fpath,\n",
    "                metadata_name=metadata_name,\n",
    "                apikey=os.environ['OPENAI_API_KEY'],\n",
    "                max_chunks=1000,\n",
    "            )\n",
    "            index.store.merge_from(file_index.store)\n",
    "            total += 1\n",
    "        except Exception:\n",
    "            logger.exception(f\"failed parse file {fpath=}\")\n",
    "\n",
    "    save_store(\n",
    "        index=index, \n",
    "        dirpath=index_dirpath, \n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    print(f\"scanned {total} files\")\n",
    "        \n",
    "run_scan_pdfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49778be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# incremental scan markdowns\n",
    "\n",
    "def gen_markdowns():\n",
    "    yield \"/home/laisky/data/langchain/basebit/doc/content/terms.md\"\n",
    "    yield from glob.glob(\"/home/laisky/data/langchain/basebit/doc/content/research/**/*.md\", recursive=True)\n",
    "    \n",
    "\n",
    "def run_scan_markdowns():\n",
    "#         index = new_store()\n",
    "    while True:\n",
    "        index = load_store(\n",
    "            dirpath=\"/home/laisky/data/langchain/index\",\n",
    "            name=name,\n",
    "        )\n",
    "        files = gen_markdowns()\n",
    "        n = embedding_markdowns(\n",
    "            index=index,\n",
    "            fpaths=files,\n",
    "            url=f\"https://s3.laisky.com/public/papers/{name}/\",\n",
    "            replace_by_url=f\"/home/laisky/data/langchain/pdf/{name}/\",\n",
    "        )\n",
    "        save_store(\n",
    "            index=index,\n",
    "            dirpath=\"/home/laisky/data/langchain/index/\", \n",
    "            name=name,\n",
    "        )\n",
    "        \n",
    "        print(f\"{n=}\")\n",
    "        if n == 0:\n",
    "            return\n",
    "        \n",
    "        \n",
    "run_scan_markdowns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83eae4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# ç”Ÿæˆç”¨äºŽé—®ç­”çš„ query chain\n",
    "# ====================================\n",
    "\n",
    "from langchain.chains import VectorDBQAWithSourcesChain, RetrievalQAWithSourcesChain\n",
    "from langchain.chat_models import ChatOpenAI, AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "Take note of the sources and include them in the answer in the format: \"SOURCES: source1 source2\", use \"SOURCES\" in capital letters regardless of the number of sources.\n",
    "If you don't know the answer, just say that \"I don't know\", don't try to make up an answer.\n",
    "----------------\n",
    "{summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "index = load_store(\n",
    "    dirpath=index_dirpath,\n",
    "    name=name,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    client=None,\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=0, \n",
    "    max_tokens=2000,\n",
    "    streaming=False,\n",
    ")  \n",
    "\n",
    "# llm = AzureChatOpenAI(\n",
    "#     deployment_name=azure_gpt_deploymentid,\n",
    "#     model_name=\"gpt-3.5-turbo\",\n",
    "#     max_tokens=2000,\n",
    "# )\n",
    "\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "# chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec62c4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤”ï¸: æäº¤ FSW å‰éœ€è¦å‡†å¤‡ä»€ä¹ˆè€ƒè¯•\n",
      "\n",
      "ðŸ“–: æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰æ˜Žç¡®æåˆ°æäº¤FSWï¼ˆFederal Skilled Workerï¼‰ç”³è¯·å‰éœ€è¦å‚åŠ ä»»ä½•è€ƒè¯•ã€‚ç„¶è€Œï¼Œæ ¹æ®åŠ æ‹¿å¤§ç§»æ°‘è§„å®šï¼Œç”³è¯·FSWéœ€è¦æ»¡è¶³ä¸€äº›åŸºæœ¬è¦æ±‚ï¼ŒåŒ…æ‹¬è¯­è¨€èƒ½åŠ›æµ‹è¯•ï¼ˆå¦‚IELTSæˆ–CELPIPï¼‰å’Œæ•™è‚²èƒŒæ™¯è¯„ä¼°ï¼ˆå¦‚å­¦åŽ†è®¤è¯ï¼‰ã€‚æ­¤å¤–ï¼Œæ ¹æ®ä¸ªäººæƒ…å†µï¼Œå¯èƒ½è¿˜éœ€è¦é€šè¿‡å…¶ä»–ä¸“ä¸šèµ„æ ¼è€ƒè¯•æˆ–æŠ€èƒ½è¯„ä¼°ã€‚å»ºè®®æ‚¨æŸ¥é˜…åŠ æ‹¿å¤§ç§»æ°‘éƒ¨å®˜æ–¹ç½‘ç«™æˆ–å’¨è¯¢ä¸“ä¸šç§»æ°‘é¡¾é—®ä»¥èŽ·å–æœ€å‡†ç¡®å’Œæœ€æ–°çš„ä¿¡æ¯ã€‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# ask pdf embeddings\n",
    "# ====================================\n",
    "query = \"æäº¤ FSW å‰éœ€è¦å‡†å¤‡ä»€ä¹ˆè€ƒè¯•\"\n",
    "\n",
    "\n",
    "translated = llm.predict(f\"\"\"I want you to act as an English translator, spelling corrector and improver. \n",
    "I will speak to you in any language and you will detect the language, \n",
    "translate it and answer in the corrected and improved version of my text, in English. \n",
    "I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, \n",
    "upper level English words and sentences. Keep the meaning same, but make them more literary. \n",
    "I want you to only reply the correction, the improvements and nothing else, do not write explanations. \n",
    "following is the sentence that need to be translate:\n",
    "---\n",
    "{query}\"\"\")\n",
    "\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=translated,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "response = chain.run(\n",
    "#     context=';'.join([d.page_content for d in related_docs]), \n",
    "    input_documents=related_docs,\n",
    "    question=query,\n",
    ")\n",
    "\n",
    "print(f\"ðŸ¤”ï¸: {query}\\n\")\n",
    "print(f\"ðŸ“–: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb792e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# manually qa based on embedidngs step by step\n",
    "# ====================================\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "system_template=\"\"\"Use the following pieces of context to answer the users question.\n",
    "If you don't know the answer, or you think more information is needed to provide a better answer, \n",
    "just say in this strict format: \"I need more informations about: [list keywords that will be used to search more informations]\" to ask more informations, \n",
    "don't try to make up an answer.\n",
    "----------------\n",
    "context: {summaries}\"\"\"\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "def query_for_more_info(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    return \"; \".join([d.page_content for d in related_docs]) \n",
    "\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "\n",
    "query = \"how to measure host os in vm by vtpm\"\n",
    "\n",
    "\n",
    "n = 0\n",
    "last_sub_query = \"\"\n",
    "regexp = re.compile(r'I need more information about \"([^\"]+)\"')\n",
    "while n<3: \n",
    "    n += 1\n",
    "    resp = chain.run({\n",
    "        \"summaries\": query_for_more_info(query),\n",
    "        \"question\": query,\n",
    "    })\n",
    "    matched = regexp.findall(resp)\n",
    "    if len(matched) == 0:\n",
    "        break\n",
    "        \n",
    "    sub_query = matched[0]\n",
    "    if sub_query == last_sub_query:\n",
    "        break\n",
    "    last_sub_query = sub_query\n",
    "    \n",
    "    print(f\"require more informations about: {sub_query}\")\n",
    "    query += f\"; {query_for_more_info(sub_query)}\"\n",
    "    \n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 'I need more information about \"Amber GPU-CC\" to provide an accurate answer. Could you please provide more context or clarify your question?'\n",
    "\n",
    "regexp = re.compile(r'I need more information about \"([^\\)]+)\"')\n",
    "\n",
    "regexp.findall(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# use vectore store in functions(agents)\n",
    "# ====================================\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import LLMMathChain, SerpAPIWrapper\n",
    "\n",
    "\n",
    "related_docs = index.store.similarity_search(\n",
    "    query=query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "\n",
    "def query_for_agent(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "\n",
    "    return \"\\n\".join([d.page_content for d in related_docs])\n",
    "\n",
    "def context_for_agent(query: str) -> str:\n",
    "    related_docs = index.store.similarity_search(\n",
    "        query=query,\n",
    "        k=5,\n",
    "    )\n",
    "        \n",
    "    response = chain.run(\n",
    "        input_documents=related_docs,\n",
    "        question=query,\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "    \n",
    "    \n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=query_for_agent,\n",
    "        description=\"useful for when you need to answer questions, this function takes a string as input and returns a string. This function is capable of vectorizing the input string and searching for similar information in a vector database. Your AI can call this function to retrieve the data it needs based on its requirements.\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "query = \"what is tee-io\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "#     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    "    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True,\n",
    ")\n",
    "\n",
    "agent.run(\n",
    "    input_documents=related_docs,\n",
    "    question=query,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eab418",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.run??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
