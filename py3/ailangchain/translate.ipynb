{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b80976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# openai tokens\n",
    "# ====================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import re\n",
    "import textwrap\n",
    "from collections import namedtuple\n",
    "\n",
    "import faiss\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, MarkdownTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from pymongo import MongoClient\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from sys import path\n",
    "\n",
    "path.append(\"/opt/configs/ramjet\")\n",
    "import prd\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = prd.OPENAI_TOKEN_ME\n",
    "\n",
    "\n",
    "def pretty_print(text: str) -> str:\n",
    "    text = text.strip()\n",
    "    return textwrap.fill(text, width=60, subsequent_indent=\"    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04f51ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "You are a translation bot. Please translate the content under <<raw>> into English, preserving all formatting and symbols. Only provide the translated text, and do not provide irrelevant instructions, not or explanations. If you encounter code wrapped in markdown or HTML tags, only translate the Chinese characters and maintain the original formatting. If you encounter any content that cannot be translated, please return it unchanged.\n",
    "\n",
    "<<raw>>\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"], \n",
    "    template=template\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\", \n",
    "    temperature=0, \n",
    "    max_tokens=2000) \n",
    "conversation = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt, \n",
    "    verbose=False, \n",
    "#     memory=ConversationBufferWindowMemory(k=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8524c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc[51186bb90000000000000000] is ok\n",
      "chinesehistory[51b5cacb0000000000000000] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: HTTP code 502 from API (<html>\r\n",
      "<head><title>502 Bad Gateway</title></head>\r\n",
      "<body>\r\n",
      "<center><h1>502 Bad Gateway</h1></center>\r\n",
      "<hr><center>cloudflare</center>\r\n",
      "</body>\r\n",
      "</html>\r\n",
      ").\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modern-history[52233e5f0000000000000000] is ok\n",
      "roc-corner[523d9ad10000000000000000] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 38daad4ed5ef584429a5648ecbef8c39 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unittest-mock[53f049fb0000000000000000] is ok\n",
      "clustering[53f9fe330000000000000000] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 477369f40ab926256a0026ae9458e2c7 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django-rest[55ad0b6f825d9ea2793ef5c7] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID dade456ff4e46a20706823c53148b619 in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID abe8b194c42ebd12453ea76807dacd2e in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literature-1[55afb5d52d8d75943fcabe50] is ok\n",
      "osx-tools-normal[55b0ec7b2d8d75943fcabe56] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4e2656e413f593feebd08504dc2fcd37 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jasmine[55b1e9f12d8d75943fcabe60] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4e06e526f361674691afc6376bb3f9fb in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3e9337218ab973b91f1f86c671abaefd in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e76433b6fd808bf48c6f226ab9b8c93d in your message.).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d354fdeb1e85f07253cda3db78d2bbd9 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angular-learn[55b8338b2d8d75943fcabe75] is ok\n",
      "flask-restful[55c180d42d8d75943fcabea2] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a7f95aab24a5ca7f5b246e20fb7fdfd9 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldap[55d3fa07a874218549123c79] is ok\n",
      "handlebars[55d68cbcc2cdd34bfec4b029] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a94a306d1ba430c4a5f4cc32912e4ab2 in your message.).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gou-jian-zhi-fa[55d9c043c2cdd34bfec4b02f] is ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=60).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snmp[55de613069e9690ddf878d7c] is ok\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# 切块\n",
    "# =====================================\n",
    "\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from langchain.text_splitter import MarkdownTextSplitter, CharacterTextSplitter\n",
    "\n",
    "\n",
    "DB_HOST = prd.MONGO_HOST\n",
    "# DB_HOST = \"100.97.108.34\"  # ubuntu\n",
    "\n",
    "mongohost = f\"mongodb://{prd.MONGO_ADMIN_USER}:{prd.MONGO_ADMIN_PASSWD}@{prd.MONGO_HOST}:{prd.MONGO_PORT}\"\n",
    "dbconn = MongoClient(host=mongohost)\n",
    "posts_col = dbconn[\"blog\"]['posts']\n",
    "\n",
    "\n",
    "# splitter = MarkdownTextSplitter(chunk_size=200,chunk_overlap=0)\n",
    "splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "\n",
    "def scan_posts_in_mongo():\n",
    "    cursor = posts_col.find().max_time_ms(0).batch_size(50)\n",
    "    i = 0\n",
    "    for docu in cursor: \n",
    "        i += 1\n",
    "        content = docu.get(\"post_markdown\")\n",
    "        translated_content = \"\"\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        #print(docu[\"_id\"])\n",
    "        #return\n",
    "            \n",
    "        for chunk in content.split(\"\\n\"):\n",
    "            if not chunk.strip():\n",
    "                translated_content += chunk + \"\\n\"\n",
    "                continue\n",
    "            \n",
    "            if len(chunk) < 400:\n",
    "                cnt = conversation.predict(input=chunk) + \"\\n\"\n",
    "            else:\n",
    "                cnt = \"\"\n",
    "                for ichunk in splitter.split_text(chunk): \n",
    "                    cnt += conversation.predict(input=ichunk)\n",
    "\n",
    "            translated_content += cnt\n",
    "            \n",
    "        # print(translated_content)\n",
    "\n",
    "        # save to mongodb\n",
    "        posts_col.update_one(\n",
    "            filter={\"_id\": docu['_id']},\n",
    "            update={\"$set\": {\"i18n\": {\n",
    "                \"updated_at\": datetime.now(),\n",
    "                \"en-us\": {\n",
    "                    \"post_markdown\": translated_content,\n",
    "                }\n",
    "            }}}\n",
    "        )\n",
    "        \n",
    "        print(f\"{docu.get('post_name')}[{docu.get('_id')}] is ok\")\n",
    "        # return\n",
    "\n",
    "scan_posts_in_mongo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342bc3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"你好\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
